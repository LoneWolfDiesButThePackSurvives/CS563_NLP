{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf300bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import Span\n",
    "from stanza.models.common.constant import is_right_to_left\n",
    "import stanza\n",
    "import spacy\n",
    "\n",
    "\n",
    "def visualize_ner_doc(doc, pipeline, select=None, colors=None, rtl_clr_adjusted=False):\n",
    "    \"\"\"\n",
    "    Takes a stanza doc object and language pipeline and visualizes the named entities within it.\n",
    "    Stanza currently supports a limited amount of languages for NER, which you can view here:\n",
    "    https://stanfordnlp.github.io/stanza/ner_models.html\n",
    "    To view only a specific type(s) of named entities, set the optional 'select' argument to\n",
    "    a list of the named entity types. Ex: select=[\"PER\", \"ORG\", \"GPE\"] to only see entities tagged as Person(s),\n",
    "    Organizations, and Geo-political entities. A full list of the available types can be found here:\n",
    "    https://stanfordnlp.github.io/stanza/available_models.html (ctrl + F \"The following table\").\n",
    "    The colors argument is formatted as a dictionary of NER tags with their corresponding colors, which can be \n",
    "    represented as a string (ex: \"blue\"), a color hex value (ex: #aa9cfc), or as a linear gradient of color\n",
    "    values (ex: \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\").\n",
    "    Do not change the 'rtl_clr_adjusted' argument; it is used for ensuring that the visualize_strings function \n",
    "    works properly on rtl languages.\n",
    "    \"\"\"\n",
    "    model, documents = spacy.blank('en'), []  # blank model, spacy is only used for visualization purposes\n",
    "    sentences, rtl = doc.sentences, is_right_to_left(pipeline)\n",
    "    if rtl:  # need to flip order of all the sentences in rendered display\n",
    "        sentences = reversed(doc.sentences)\n",
    "        # adjust colors to be in LTR flipped format due to the RLO unicode char flipping words\n",
    "        if colors and not rtl_clr_adjusted:\n",
    "            for color in colors:\n",
    "                clr_val = colors[color]\n",
    "                colors.pop(color)\n",
    "                colors[\"‮\" + color[::-1]] = clr_val\n",
    "    for sentence in sentences:\n",
    "        words, display_ents, already_found = [], [], False\n",
    "        # initialize doc object with words first\n",
    "        for i, word in enumerate(sentence.words):\n",
    "            if rtl and word.text.isascii() and not already_found:\n",
    "                to_append = [word.text[::-1]]\n",
    "                next_word_index = i + 1\n",
    "                # account for flipping non Arabic words back to original form and order. two flips -> original order\n",
    "                while next_word_index <= len(sentence.words) - 1 and sentence.words[next_word_index].text.isascii():\n",
    "                    to_append.append(sentence.words[next_word_index].text[::-1])\n",
    "                    next_word_index += 1\n",
    "                to_append = reversed(to_append)\n",
    "                for token in to_append:\n",
    "                    words.append(token)\n",
    "                already_found = True\n",
    "            elif rtl and word.text.isascii() and already_found:  # skip over already collected words\n",
    "                continue\n",
    "            else:  # arabic chars\n",
    "                words.append(word.text)\n",
    "                already_found = False\n",
    "\n",
    "        document = Doc(model.vocab, words=words)\n",
    "\n",
    "        # tag all NER tokens found\n",
    "        for ent in sentence.ents:\n",
    "            if select and ent.type not in select:\n",
    "                continue\n",
    "            found_indexes = []\n",
    "            for token in ent.tokens:\n",
    "                found_indexes.append(token.id[0] - 1)\n",
    "            if not rtl:\n",
    "                to_add = Span(document, found_indexes[0], found_indexes[-1] + 1, ent.type)\n",
    "            else:  # RTL languages need the override char to flip order\n",
    "                to_add = Span(document, found_indexes[0], found_indexes[-1] + 1, \"‮\" + ent.type[::-1])\n",
    "            display_ents.append(to_add)\n",
    "        document.set_ents(display_ents)\n",
    "        documents.append(document)\n",
    "\n",
    "    # Visualize doc objects\n",
    "    visualization_options = {\"ents\": select}\n",
    "    if colors:\n",
    "        visualization_options[\"colors\"] = colors\n",
    "    for document in documents:\n",
    "        displacy.render(document, style='ent', options=visualization_options)\n",
    "\n",
    "\n",
    "def visualize_ner_str(text, pipeline_code, pipe, select=None, colors=None, rtl_clr_adjusted=False):\n",
    "    \"\"\"\n",
    "    Takes in a text string and visualizes the named entities within the text. Required args also include a \n",
    "    pipeline code, the two-letter code for a language defined by Universal Dependencies (ex: \"en\" for English).\n",
    "    Lastly, the user must provide an NLP pipeline - we recommend Stanza (ex: pipe = stanza.Pipeline('en')).\n",
    "    Optionally, the 'select' argument allows for specific NER tags to be highlighted; the 'color' argument allows\n",
    "    for specific NER tags to have certain color(s).\n",
    "    \"\"\"\n",
    "    doc = pipe(text)\n",
    "    visualize_ner_doc(doc, pipeline_code, select, colors, rtl_clr_adjusted)\n",
    "    \n",
    "    \n",
    "def visualize_strings(texts, language_code, select=None, colors=None):\n",
    "    \"\"\"\n",
    "    Takes in a list of strings and a language code (Stanza defines these, ex: 'en' for English) to visualize all \n",
    "    of the strings' named entities. The strings are processed by the Stanza pipeline and the named entities are displayed. \n",
    "    Each text is separated by a delimiting line. \n",
    "    Optionally, the 'select' argument may be configured to only visualize given named entities (ex: select=['ORG', 'PERSON']).\n",
    "    The optional colors argument is formatted as a dictionary of NER tags with their corresponding colors, which can be \n",
    "    represented as a string (ex: \"blue\"), a color hex value (ex: #aa9cfc), or as a linear gradient of color\n",
    "    values (ex: \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\").\n",
    "    \"\"\"\n",
    "    lang_pipe = stanza.Pipeline(language_code)\n",
    "    if is_right_to_left(language_code) and len(texts) > 1:\n",
    "        for text in texts:\n",
    "            if text is texts[0]:\n",
    "                visualize_ner_str(text, language_code, lang_pipe, select=select, colors=colors)\n",
    "            else:\n",
    "                visualize_ner_str(text, language_code, lang_pipe, select=select, colors=colors, rtl_clr_adjusted=True)\n",
    "            print(\"\\n\\n\")\n",
    "    else:\n",
    "        for text in texts:\n",
    "            visualize_ner_str(text, language_code, lang_pipe, select=select, colors=colors)\n",
    "            print(\"\\n\\n\")\n",
    "        \n",
    "    \n",
    "def visualize_docs(docs, language_code, select=None, colors=None):\n",
    "    \"\"\"\n",
    "    Takes in a list of doc and a language code (Stanza defines these, ex: 'en' for English) to visualize all \n",
    "    of the strings' named entities. Each text is separated by a delimiting line. \n",
    "    Optionally, the 'select' argument may be configured to only visualize given named entities (ex: select=['ORG', 'PERSON']).\n",
    "    The optional colors argument is formatted as a dictionary of NER tags with their corresponding colors, which can be \n",
    "    represented as a string (ex: \"blue\"), a color hex value (ex: #aa9cfc), or as a linear gradient of color\n",
    "    values (ex: \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\").\n",
    "    \"\"\"\n",
    "    lang_pipe = stanza.Pipeline(language_code)\n",
    "    \n",
    "    if is_right_to_left(language_code) and len(docs) > 1:\n",
    "        for doc in docs:\n",
    "            if docs is docs[0]:\n",
    "                visualize_ner_doc(doc, language_code, lang_pipe, select=select, colors=colors)\n",
    "            else:\n",
    "                visualize_ner_doc(doc, language_code, lang_pipe, select=select, colors=colors, rtl_clr_adjusted=True)\n",
    "            print(\"\\n\\n\")\n",
    "    else:\n",
    "        for text in texts:\n",
    "            visualize_ner_doc(text, language_code, lang_pipe, select=select, colors=colors)\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "def main():\n",
    "    en_strings = ['''Samuel Jackson, a Christian man from Utah, went to the JFK Airport for a flight to New York.\n",
    "                               He was thinking of attending the US Open, his favorite tennis tournament besides Wimbledon.\n",
    "                               That would be a dream trip, certainly not possible since it is $5000 attendance and 5000 miles away.\n",
    "                               On the way there, he watched the Super Bowl for 2 hours and read War and Piece by Tolstoy for 1 hour.\n",
    "                               In New York, he crossed the Brooklyn Bridge and listened to the 5th symphony of Beethoven as well as\n",
    "                               \"All I want for Christmas is You\" by Mariah Carey.''', \n",
    "                       \"Barack Obama was born in Hawaii. He was elected President of the United States in 2008\"]\n",
    "    zh_strings = ['''来自犹他州的基督徒塞缪尔杰克逊前往肯尼迪机场搭乘航班飞往纽约。\n",
    "                             他正在考虑参加美国公开赛，这是除了温布尔登之外他最喜欢的网球赛事。\n",
    "                             那将是一次梦想之旅，当然不可能，因为它的出勤费为 5000 美元，距离 5000 英里。\n",
    "                             在去的路上，他看了 2 个小时的超级碗比赛，看了 1 个小时的托尔斯泰的《战争与碎片》。\n",
    "                               在纽约，他穿过布鲁克林大桥，聆听了贝多芬的第五交响曲以及 玛丽亚凯莉的“圣诞节我想要的就是你”。''',\n",
    "                 \"我觉得罗家费德勒住在加州, 在美国里面。\"]\n",
    "    ar_strings = [\".أعيش في سان فرانسيسكو ، كاليفورنيا. اسمي أليكس وأنا ألتحق بجامعة ستانفورد. أنا أدرس علوم الكمبيوتر وأستاذي هو كريس مانينغ\"\n",
    "                  , \"اسمي أليكس ، أنا من الولايات المتحدة.\",  \n",
    "                 '''صامويل جاكسون ، رجل مسيحي من ولاية يوتا ، ذهب إلى مطار جون كنيدي في رحلة إلى نيويورك. كان يفكر في حضور بطولة الولايات المتحدة المفتوحة للتنس ، بطولة التنس المفضلة لديه إلى جانب بطولة ويمبلدون. ستكون هذه رحلة الأحلام ، وبالتأكيد ليست ممكنة لأنها تبلغ 5000 دولار للحضور و 5000 ميل. في الطريق إلى هناك ، شاهد Super Bowl لمدة ساعتين وقرأ War and Piece by Tolstoy لمدة ساعة واحدة. في نيويورك ، عبر جسر بروكلين واستمع إلى السيمفونية الخامسة لبيتهوفن وكذلك \"كل ما أريده في عيد الميلاد هو أنت\" لماريا كاري.''']\n",
    "    \n",
    "    \n",
    "    visualize_strings(en_strings, \"en\")\n",
    "    visualize_strings(zh_strings, \"zh\", colors={\"PERSON\": \"yellow\", \"DATE\": \"red\", \"GPE\": \"blue\"})\n",
    "    visualize_strings(zh_strings, \"zh\", select=['PERSON', 'DATE'])\n",
    "    visualize_strings(ar_strings, \"ar\", colors={\"PER\": \"pink\", \"LOC\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\", \"ORG\": \"yellow\"})\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670921a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

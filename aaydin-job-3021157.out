slurm submission log: 2022-08-03 14:40:22.173287
created following sbatch script: 

###############################

#!/bin/bash

#SBATCH --cpus-per-task=2
#SBATCH --gres=gpu:1
#SBATCH --job-name=aaydin-job-3021157
#SBATCH --mem=16G
#SBATCH --open-mode=append
#SBATCH --output=aaydin-job-3021157.out
#SBATCH --partition=jag-hi
#SBATCH --time=14-0

# activate your desired anaconda environment


# cd to working directory
cd .

# launch commands
srun --unbuffered run_as_child_processes 'python -m stanza.utils.training.run_ner kk_kazNERD'

###############################

submission to slurm complete!


###############################
slurm submission output

Submitted batch job 3892142



###############################

###############################
start time: 2022-08-03 14:40:23.022780
machine: jagupard25.stanford.edu
conda env: base
###############################
running following processes

	python -m stanza.utils.training.run_ner kk_kazNERD


###############################
command outputs: 


2022-08-03 14:40:32 INFO: Training program called with:
/juice/scr/aaydin/stanza/stanza/utils/training/run_ner.py kk_kazNERD
2022-08-03 14:40:32 DEBUG: kk_kazNERD: kk_kazNERD
2022-08-03 14:40:32 INFO: Save file for kk_kazNERD model: kk_kazNERD_nertagger.pt
2022-08-03 14:40:32 INFO: kk_kazNERD: saved_models/ner/kk_kazNERD_nertagger.pt does not exist, training new model
2022-08-03 14:40:32 INFO: Default pretrain should be /nlp/scr/aaydin/stanza_resources/kk/pretrain/ktb.pt  Attempting to download
2022-08-03 14:40:32 DEBUG: Downloading resource file...
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 174kB [00:00, 68.3MB/s]                    
2022-08-03 14:40:33 INFO: Using pretrain found in /nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt  To use a different pretrain, specify --wordvec_pretrain_file
2022-08-03 14:40:33 INFO: Running train step with args: ['--train_file', '/nlp/scr/aaydin/kazNERD/data/ner/kk_kazNERD.train.json', '--eval_file', '/nlp/scr/aaydin/kazNERD/data/ner/kk_kazNERD.dev.json', '--lang', 'kk', '--shorthand', 'kk_kazNERD', '--mode', 'train', '--wordvec_pretrain_file', '/nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt', '--save_name', 'kk_kazNERD_nertagger.pt', '--save_dir', 'saved_models/ner']
2022-08-03 14:40:33 INFO: Running NER tagger in train mode
2022-08-03 14:40:34 DEBUG: Loaded pretrain from /nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt
2022-08-03 14:40:34 WARNING: Embedding file has a dimension of 300.  Model will be built with that size instead of 100
2022-08-03 14:40:34 INFO: Loading data with batch size 32...
2022-08-03 14:40:48 INFO: Loaded 90228 sentences of training data
2022-08-03 14:40:50 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
2022-08-03 14:40:53 DEBUG: Creating delta vocab of size 65580
2022-08-03 14:40:58 DEBUG: 2820 batches created.
2022-08-03 14:40:59 INFO: Loaded 11167 sentences of dev data
2022-08-03 14:40:59 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
2022-08-03 14:41:00 DEBUG: 349 batches created.
2022-08-03 14:41:00 INFO: Tags present in training set:
  Tags without BIES markers: O
  Tags with B-, I-, E-, or S-: ADAGE ART CARDINAL CONTACT DATE DISEASE EVENT FACILITY GPE LANGUAGE LAW LOCATION MISCELLANEOUS MONEY NON_HUMAN NORP ORDINAL ORGANISATION PERCENTAGE PERSON POSITION PRODUCT PROJECT QUANTITY TIME
2022-08-03 14:41:00 INFO: Training tagger...
2022-08-03 14:41:07 INFO: NERTagger(
  (word_emb): Embedding(150000, 300, padding_idx=0)
  (delta_emb): Embedding(65580, 300, padding_idx=0)
  (charmodel): CharacterModel(
    (char_emb): Embedding(174, 100, padding_idx=0)
    (charlstm): PackedLSTM(
      (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)
    )
    (dropout): Dropout(p=0.5, inplace=False)
  )
  (input_transform): Linear(in_features=500, out_features=500, bias=True)
  (taggerlstm): PackedLSTM(
    (lstm): LSTM(500, 256, batch_first=True, bidirectional=True)
  )
  (tag_clf): Linear(in_features=512, out_features=102, bias=True)
  (crit): CRFLoss()
  (drop): Dropout(p=0.5, inplace=False)
  (worddrop): WordDropout(p=0.01)
  (lockeddrop): LockedDropout(p=0.0)
)
2022-08-03 14:41:08 INFO: 2022-08-03 14:41:08: step 20/200000, loss = 14.665400 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:41:09 INFO: 2022-08-03 14:41:09: step 40/200000, loss = 15.170139 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:41:10 INFO: 2022-08-03 14:41:10: step 60/200000, loss = 12.178808 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:41:11 INFO: 2022-08-03 14:41:11: step 80/200000, loss = 17.704506 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:41:11 INFO: 2022-08-03 14:41:11: step 100/200000, loss = 10.094112 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:41:12 INFO: 2022-08-03 14:41:12: step 120/200000, loss = 9.779055 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:41:13 INFO: 2022-08-03 14:41:13: step 140/200000, loss = 11.066172 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:41:14 INFO: 2022-08-03 14:41:14: step 160/200000, loss = 10.079378 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:41:15 INFO: 2022-08-03 14:41:15: step 180/200000, loss = 13.082584 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:41:16 INFO: 2022-08-03 14:41:16: step 200/200000, loss = 9.335278 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:41:17 INFO: 2022-08-03 14:41:17: step 220/200000, loss = 6.037622 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:41:18 INFO: 2022-08-03 14:41:18: step 240/200000, loss = 8.368092 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:41:19 INFO: 2022-08-03 14:41:19: step 260/200000, loss = 7.765886 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:41:19 INFO: 2022-08-03 14:41:19: step 280/200000, loss = 8.253831 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:41:20 INFO: 2022-08-03 14:41:20: step 300/200000, loss = 9.474228 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:41:21 INFO: 2022-08-03 14:41:21: step 320/200000, loss = 4.362227 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:41:22 INFO: 2022-08-03 14:41:22: step 340/200000, loss = 8.905494 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:41:23 INFO: 2022-08-03 14:41:23: step 360/200000, loss = 9.883215 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:41:24 INFO: 2022-08-03 14:41:24: step 380/200000, loss = 7.866916 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:41:25 INFO: 2022-08-03 14:41:25: step 400/200000, loss = 4.204906 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:41:26 INFO: 2022-08-03 14:41:26: step 420/200000, loss = 5.025517 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:41:27 INFO: 2022-08-03 14:41:27: step 440/200000, loss = 8.603817 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:41:27 INFO: 2022-08-03 14:41:27: step 460/200000, loss = 7.606321 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:41:28 INFO: 2022-08-03 14:41:28: step 480/200000, loss = 4.505303 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:41:29 INFO: 2022-08-03 14:41:29: step 500/200000, loss = 6.007037 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:41:29 INFO: Evaluating on dev set...
2022-08-03 14:41:54 INFO: Score by entity:
Prec.	Rec.	F1
52.02	39.75	45.07
2022-08-03 14:41:54 INFO: step 500: train_loss = 9.657624, dev_score = 0.4507
2022-08-03 14:41:55 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:41:55 INFO: New best model saved.
2022-08-03 14:41:55 INFO: 
2022-08-03 14:41:56 INFO: 2022-08-03 14:41:56: step 520/200000, loss = 3.607562 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:41:57 INFO: 2022-08-03 14:41:57: step 540/200000, loss = 7.916049 (0.050 sec/batch), lr: 0.100000
2022-08-03 14:41:58 INFO: 2022-08-03 14:41:58: step 560/200000, loss = 10.842843 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:41:59 INFO: 2022-08-03 14:41:59: step 580/200000, loss = 7.230587 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:00 INFO: 2022-08-03 14:42:00: step 600/200000, loss = 4.315906 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:00 INFO: 2022-08-03 14:42:00: step 620/200000, loss = 6.142842 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:42:01 INFO: 2022-08-03 14:42:01: step 640/200000, loss = 7.475223 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:42:02 INFO: 2022-08-03 14:42:02: step 660/200000, loss = 4.300269 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:42:03 INFO: 2022-08-03 14:42:03: step 680/200000, loss = 3.813404 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:42:04 INFO: 2022-08-03 14:42:04: step 700/200000, loss = 4.922737 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:42:05 INFO: 2022-08-03 14:42:05: step 720/200000, loss = 4.433532 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:42:06 INFO: 2022-08-03 14:42:06: step 740/200000, loss = 6.458642 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:42:07 INFO: 2022-08-03 14:42:07: step 760/200000, loss = 5.971744 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:42:08 INFO: 2022-08-03 14:42:08: step 780/200000, loss = 2.991156 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:42:08 INFO: 2022-08-03 14:42:08: step 800/200000, loss = 6.071617 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:42:09 INFO: 2022-08-03 14:42:09: step 820/200000, loss = 3.692614 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:42:10 INFO: 2022-08-03 14:42:10: step 840/200000, loss = 4.566433 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:42:11 INFO: 2022-08-03 14:42:11: step 860/200000, loss = 5.092655 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:42:12 INFO: 2022-08-03 14:42:12: step 880/200000, loss = 3.597813 (0.052 sec/batch), lr: 0.100000
2022-08-03 14:42:13 INFO: 2022-08-03 14:42:13: step 900/200000, loss = 3.613365 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:42:14 INFO: 2022-08-03 14:42:14: step 920/200000, loss = 4.584998 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:15 INFO: 2022-08-03 14:42:15: step 940/200000, loss = 4.371263 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:42:16 INFO: 2022-08-03 14:42:16: step 960/200000, loss = 3.654846 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:16 INFO: 2022-08-03 14:42:16: step 980/200000, loss = 2.012434 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:42:17 INFO: 2022-08-03 14:42:17: step 1000/200000, loss = 4.555130 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:42:17 INFO: Evaluating on dev set...
2022-08-03 14:42:42 INFO: Score by entity:
Prec.	Rec.	F1
69.07	68.32	68.69
2022-08-03 14:42:42 INFO: step 1000: train_loss = 4.551683, dev_score = 0.6869
2022-08-03 14:42:44 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:42:44 INFO: New best model saved.
2022-08-03 14:42:44 INFO: 
2022-08-03 14:42:45 INFO: 2022-08-03 14:42:45: step 1020/200000, loss = 2.983184 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:42:46 INFO: 2022-08-03 14:42:46: step 1040/200000, loss = 6.569362 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:42:47 INFO: 2022-08-03 14:42:47: step 1060/200000, loss = 2.516892 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:42:47 INFO: 2022-08-03 14:42:47: step 1080/200000, loss = 2.753169 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:42:48 INFO: 2022-08-03 14:42:48: step 1100/200000, loss = 4.502608 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:42:49 INFO: 2022-08-03 14:42:49: step 1120/200000, loss = 3.235716 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:42:50 INFO: 2022-08-03 14:42:50: step 1140/200000, loss = 2.726194 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:42:51 INFO: 2022-08-03 14:42:51: step 1160/200000, loss = 4.073076 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:42:52 INFO: 2022-08-03 14:42:52: step 1180/200000, loss = 3.636790 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:42:53 INFO: 2022-08-03 14:42:53: step 1200/200000, loss = 4.230822 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:42:54 INFO: 2022-08-03 14:42:54: step 1220/200000, loss = 4.645215 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:55 INFO: 2022-08-03 14:42:55: step 1240/200000, loss = 2.907336 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:42:56 INFO: 2022-08-03 14:42:56: step 1260/200000, loss = 2.946484 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:42:56 INFO: 2022-08-03 14:42:56: step 1280/200000, loss = 4.167583 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:42:57 INFO: 2022-08-03 14:42:57: step 1300/200000, loss = 2.227393 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:42:58 INFO: 2022-08-03 14:42:58: step 1320/200000, loss = 3.529547 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:42:59 INFO: 2022-08-03 14:42:59: step 1340/200000, loss = 3.235492 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:00 INFO: 2022-08-03 14:43:00: step 1360/200000, loss = 1.969310 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:43:01 INFO: 2022-08-03 14:43:01: step 1380/200000, loss = 2.734552 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:43:02 INFO: 2022-08-03 14:43:02: step 1400/200000, loss = 5.350926 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:43:03 INFO: 2022-08-03 14:43:03: step 1420/200000, loss = 2.375207 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:04 INFO: 2022-08-03 14:43:04: step 1440/200000, loss = 4.352025 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:43:04 INFO: 2022-08-03 14:43:04: step 1460/200000, loss = 2.397721 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:43:05 INFO: 2022-08-03 14:43:05: step 1480/200000, loss = 1.546721 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:43:06 INFO: 2022-08-03 14:43:06: step 1500/200000, loss = 2.164580 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:43:06 INFO: Evaluating on dev set...
2022-08-03 14:43:31 INFO: Score by entity:
Prec.	Rec.	F1
75.06	73.31	74.18
2022-08-03 14:43:31 INFO: step 1500: train_loss = 3.069148, dev_score = 0.7418
2022-08-03 14:43:32 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:43:32 INFO: New best model saved.
2022-08-03 14:43:32 INFO: 
2022-08-03 14:43:33 INFO: 2022-08-03 14:43:33: step 1520/200000, loss = 1.878148 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:34 INFO: 2022-08-03 14:43:34: step 1540/200000, loss = 2.221220 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:43:35 INFO: 2022-08-03 14:43:35: step 1560/200000, loss = 2.016914 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:43:36 INFO: 2022-08-03 14:43:36: step 1580/200000, loss = 1.663601 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:43:37 INFO: 2022-08-03 14:43:37: step 1600/200000, loss = 3.379499 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:43:38 INFO: 2022-08-03 14:43:38: step 1620/200000, loss = 3.084126 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:43:39 INFO: 2022-08-03 14:43:39: step 1640/200000, loss = 2.378381 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:39 INFO: 2022-08-03 14:43:39: step 1660/200000, loss = 1.296419 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:43:40 INFO: 2022-08-03 14:43:40: step 1680/200000, loss = 1.319185 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:43:41 INFO: 2022-08-03 14:43:41: step 1700/200000, loss = 3.448748 (0.065 sec/batch), lr: 0.100000
2022-08-03 14:43:42 INFO: 2022-08-03 14:43:42: step 1720/200000, loss = 2.585040 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:43 INFO: 2022-08-03 14:43:43: step 1740/200000, loss = 2.602292 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:44 INFO: 2022-08-03 14:43:44: step 1760/200000, loss = 1.653078 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:43:45 INFO: 2022-08-03 14:43:45: step 1780/200000, loss = 1.432596 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:43:46 INFO: 2022-08-03 14:43:46: step 1800/200000, loss = 1.108941 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:43:47 INFO: 2022-08-03 14:43:47: step 1820/200000, loss = 2.541905 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:43:48 INFO: 2022-08-03 14:43:48: step 1840/200000, loss = 2.286376 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:43:48 INFO: 2022-08-03 14:43:48: step 1860/200000, loss = 2.613994 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:43:49 INFO: 2022-08-03 14:43:49: step 1880/200000, loss = 1.757915 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:43:50 INFO: 2022-08-03 14:43:50: step 1900/200000, loss = 3.664530 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:43:51 INFO: 2022-08-03 14:43:51: step 1920/200000, loss = 3.317127 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:43:52 INFO: 2022-08-03 14:43:52: step 1940/200000, loss = 4.474901 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:43:53 INFO: 2022-08-03 14:43:53: step 1960/200000, loss = 1.301457 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:43:54 INFO: 2022-08-03 14:43:54: step 1980/200000, loss = 1.352166 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:43:55 INFO: 2022-08-03 14:43:55: step 2000/200000, loss = 1.627630 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:43:55 INFO: Evaluating on dev set...
2022-08-03 14:44:19 INFO: Score by entity:
Prec.	Rec.	F1
79.72	78.16	78.93
2022-08-03 14:44:19 INFO: step 2000: train_loss = 2.409864, dev_score = 0.7893
2022-08-03 14:44:21 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:44:21 INFO: New best model saved.
2022-08-03 14:44:21 INFO: 
2022-08-03 14:44:21 INFO: 2022-08-03 14:44:21: step 2020/200000, loss = 2.997336 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:44:22 INFO: 2022-08-03 14:44:22: step 2040/200000, loss = 1.977920 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:44:23 INFO: 2022-08-03 14:44:23: step 2060/200000, loss = 0.904595 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:44:24 INFO: 2022-08-03 14:44:24: step 2080/200000, loss = 2.705589 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:44:25 INFO: 2022-08-03 14:44:25: step 2100/200000, loss = 2.197346 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:44:26 INFO: 2022-08-03 14:44:26: step 2120/200000, loss = 3.500460 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:44:27 INFO: 2022-08-03 14:44:27: step 2140/200000, loss = 1.423775 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:44:28 INFO: 2022-08-03 14:44:28: step 2160/200000, loss = 2.598155 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:44:29 INFO: 2022-08-03 14:44:29: step 2180/200000, loss = 2.056989 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:44:29 INFO: 2022-08-03 14:44:29: step 2200/200000, loss = 2.129166 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:44:30 INFO: 2022-08-03 14:44:30: step 2220/200000, loss = 2.268582 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:44:31 INFO: 2022-08-03 14:44:31: step 2240/200000, loss = 1.118708 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:44:32 INFO: 2022-08-03 14:44:32: step 2260/200000, loss = 2.274887 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:44:33 INFO: 2022-08-03 14:44:33: step 2280/200000, loss = 1.257235 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:44:34 INFO: 2022-08-03 14:44:34: step 2300/200000, loss = 0.883565 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:44:35 INFO: 2022-08-03 14:44:35: step 2320/200000, loss = 2.282345 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:44:36 INFO: 2022-08-03 14:44:36: step 2340/200000, loss = 2.103367 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:44:36 INFO: 2022-08-03 14:44:36: step 2360/200000, loss = 1.747187 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:44:37 INFO: 2022-08-03 14:44:37: step 2380/200000, loss = 1.945730 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:44:38 INFO: 2022-08-03 14:44:38: step 2400/200000, loss = 2.134352 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:44:39 INFO: 2022-08-03 14:44:39: step 2420/200000, loss = 1.028159 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:44:40 INFO: 2022-08-03 14:44:40: step 2440/200000, loss = 2.365473 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:44:41 INFO: 2022-08-03 14:44:41: step 2460/200000, loss = 1.859243 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:44:42 INFO: 2022-08-03 14:44:42: step 2480/200000, loss = 1.713751 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:44:43 INFO: 2022-08-03 14:44:43: step 2500/200000, loss = 2.076471 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:44:43 INFO: Evaluating on dev set...
2022-08-03 14:45:07 INFO: Score by entity:
Prec.	Rec.	F1
83.63	80.62	82.10
2022-08-03 14:45:07 INFO: step 2500: train_loss = 1.976982, dev_score = 0.8210
2022-08-03 14:45:08 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:45:08 INFO: New best model saved.
2022-08-03 14:45:08 INFO: 
2022-08-03 14:45:09 INFO: 2022-08-03 14:45:09: step 2520/200000, loss = 0.765156 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:45:10 INFO: 2022-08-03 14:45:10: step 2540/200000, loss = 2.507481 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:45:11 INFO: 2022-08-03 14:45:11: step 2560/200000, loss = 1.819451 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:12 INFO: 2022-08-03 14:45:12: step 2580/200000, loss = 1.669570 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:45:13 INFO: 2022-08-03 14:45:13: step 2600/200000, loss = 1.616400 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:45:14 INFO: 2022-08-03 14:45:14: step 2620/200000, loss = 1.889611 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:15 INFO: 2022-08-03 14:45:15: step 2640/200000, loss = 2.426637 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:45:15 INFO: 2022-08-03 14:45:15: step 2660/200000, loss = 2.364988 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:16 INFO: 2022-08-03 14:45:16: step 2680/200000, loss = 1.837948 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:45:17 INFO: 2022-08-03 14:45:17: step 2700/200000, loss = 1.939852 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:45:18 INFO: 2022-08-03 14:45:18: step 2720/200000, loss = 2.180314 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:45:19 INFO: 2022-08-03 14:45:19: step 2740/200000, loss = 1.363165 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:45:20 INFO: 2022-08-03 14:45:20: step 2760/200000, loss = 2.105793 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:45:21 INFO: 2022-08-03 14:45:21: step 2780/200000, loss = 2.062932 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:45:22 INFO: 2022-08-03 14:45:22: step 2800/200000, loss = 1.740823 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:23 INFO: 2022-08-03 14:45:23: step 2820/200000, loss = 2.191025 (0.079 sec/batch), lr: 0.100000
2022-08-03 14:45:24 INFO: 2022-08-03 14:45:24: step 2840/200000, loss = 1.206708 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:45:25 INFO: 2022-08-03 14:45:25: step 2860/200000, loss = 0.653581 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:45:25 INFO: 2022-08-03 14:45:25: step 2880/200000, loss = 2.334543 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:26 INFO: 2022-08-03 14:45:26: step 2900/200000, loss = 1.921046 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:45:27 INFO: 2022-08-03 14:45:27: step 2920/200000, loss = 1.769216 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:45:28 INFO: 2022-08-03 14:45:28: step 2940/200000, loss = 0.842797 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:45:29 INFO: 2022-08-03 14:45:29: step 2960/200000, loss = 1.482137 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:45:30 INFO: 2022-08-03 14:45:30: step 2980/200000, loss = 1.758814 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:45:31 INFO: 2022-08-03 14:45:31: step 3000/200000, loss = 2.434921 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:45:31 INFO: Evaluating on dev set...
2022-08-03 14:45:55 INFO: Score by entity:
Prec.	Rec.	F1
84.56	83.28	83.91
2022-08-03 14:45:55 INFO: step 3000: train_loss = 1.702976, dev_score = 0.8391
2022-08-03 14:45:57 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:45:57 INFO: New best model saved.
2022-08-03 14:45:57 INFO: 
2022-08-03 14:45:58 INFO: 2022-08-03 14:45:58: step 3020/200000, loss = 2.476550 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:45:59 INFO: 2022-08-03 14:45:59: step 3040/200000, loss = 0.705688 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:45:59 INFO: 2022-08-03 14:45:59: step 3060/200000, loss = 1.406644 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:46:00 INFO: 2022-08-03 14:46:00: step 3080/200000, loss = 1.533536 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:01 INFO: 2022-08-03 14:46:01: step 3100/200000, loss = 1.491565 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:02 INFO: 2022-08-03 14:46:02: step 3120/200000, loss = 1.174788 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:46:03 INFO: 2022-08-03 14:46:03: step 3140/200000, loss = 1.023481 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:46:04 INFO: 2022-08-03 14:46:04: step 3160/200000, loss = 1.260620 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:46:05 INFO: 2022-08-03 14:46:05: step 3180/200000, loss = 0.823552 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:46:06 INFO: 2022-08-03 14:46:06: step 3200/200000, loss = 1.103654 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:46:07 INFO: 2022-08-03 14:46:07: step 3220/200000, loss = 2.559460 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:46:08 INFO: 2022-08-03 14:46:08: step 3240/200000, loss = 1.096993 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:46:08 INFO: 2022-08-03 14:46:08: step 3260/200000, loss = 1.319297 (0.031 sec/batch), lr: 0.100000
2022-08-03 14:46:09 INFO: 2022-08-03 14:46:09: step 3280/200000, loss = 2.059043 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:46:10 INFO: 2022-08-03 14:46:10: step 3300/200000, loss = 0.929713 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:46:11 INFO: 2022-08-03 14:46:11: step 3320/200000, loss = 1.765625 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:46:12 INFO: 2022-08-03 14:46:12: step 3340/200000, loss = 1.303932 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:13 INFO: 2022-08-03 14:46:13: step 3360/200000, loss = 1.866814 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:14 INFO: 2022-08-03 14:46:14: step 3380/200000, loss = 1.203904 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:46:15 INFO: 2022-08-03 14:46:15: step 3400/200000, loss = 0.934528 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:46:16 INFO: 2022-08-03 14:46:16: step 3420/200000, loss = 1.163384 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:46:16 INFO: 2022-08-03 14:46:16: step 3440/200000, loss = 1.596691 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:46:17 INFO: 2022-08-03 14:46:17: step 3460/200000, loss = 1.789916 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:46:18 INFO: 2022-08-03 14:46:18: step 3480/200000, loss = 1.614575 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:46:19 INFO: 2022-08-03 14:46:19: step 3500/200000, loss = 1.777974 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:46:19 INFO: Evaluating on dev set...
2022-08-03 14:46:43 INFO: Score by entity:
Prec.	Rec.	F1
86.22	84.63	85.42
2022-08-03 14:46:43 INFO: step 3500: train_loss = 1.447005, dev_score = 0.8542
2022-08-03 14:46:45 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:46:45 INFO: New best model saved.
2022-08-03 14:46:45 INFO: 
2022-08-03 14:46:46 INFO: 2022-08-03 14:46:46: step 3520/200000, loss = 1.221864 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:47 INFO: 2022-08-03 14:46:47: step 3540/200000, loss = 0.891556 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:46:48 INFO: 2022-08-03 14:46:48: step 3560/200000, loss = 1.242926 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:46:49 INFO: 2022-08-03 14:46:49: step 3580/200000, loss = 1.554854 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:46:49 INFO: 2022-08-03 14:46:49: step 3600/200000, loss = 1.417146 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:46:50 INFO: 2022-08-03 14:46:50: step 3620/200000, loss = 1.847715 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:46:51 INFO: 2022-08-03 14:46:51: step 3640/200000, loss = 1.315665 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:46:52 INFO: 2022-08-03 14:46:52: step 3660/200000, loss = 1.142413 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:46:53 INFO: 2022-08-03 14:46:53: step 3680/200000, loss = 0.920799 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:46:54 INFO: 2022-08-03 14:46:54: step 3700/200000, loss = 1.801042 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:46:55 INFO: 2022-08-03 14:46:55: step 3720/200000, loss = 2.119566 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:46:56 INFO: 2022-08-03 14:46:56: step 3740/200000, loss = 1.535033 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:46:57 INFO: 2022-08-03 14:46:57: step 3760/200000, loss = 2.163944 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:46:57 INFO: 2022-08-03 14:46:57: step 3780/200000, loss = 1.459682 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:46:58 INFO: 2022-08-03 14:46:58: step 3800/200000, loss = 0.968958 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:46:59 INFO: 2022-08-03 14:46:59: step 3820/200000, loss = 1.760490 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:00 INFO: 2022-08-03 14:47:00: step 3840/200000, loss = 2.550194 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:47:01 INFO: 2022-08-03 14:47:01: step 3860/200000, loss = 1.149984 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:02 INFO: 2022-08-03 14:47:02: step 3880/200000, loss = 1.061686 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:47:03 INFO: 2022-08-03 14:47:03: step 3900/200000, loss = 0.675822 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:47:04 INFO: 2022-08-03 14:47:04: step 3920/200000, loss = 0.884949 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:47:05 INFO: 2022-08-03 14:47:05: step 3940/200000, loss = 1.841483 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:05 INFO: 2022-08-03 14:47:05: step 3960/200000, loss = 1.009364 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:47:06 INFO: 2022-08-03 14:47:06: step 3980/200000, loss = 2.096190 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:47:07 INFO: 2022-08-03 14:47:07: step 4000/200000, loss = 1.420011 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:07 INFO: Evaluating on dev set...
2022-08-03 14:47:32 INFO: Score by entity:
Prec.	Rec.	F1
86.91	86.00	86.45
2022-08-03 14:47:32 INFO: step 4000: train_loss = 1.342563, dev_score = 0.8645
2022-08-03 14:47:33 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:47:33 INFO: New best model saved.
2022-08-03 14:47:33 INFO: 
2022-08-03 14:47:34 INFO: 2022-08-03 14:47:34: step 4020/200000, loss = 1.207394 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:47:35 INFO: 2022-08-03 14:47:35: step 4040/200000, loss = 1.193455 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:47:36 INFO: 2022-08-03 14:47:36: step 4060/200000, loss = 0.918308 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:37 INFO: 2022-08-03 14:47:37: step 4080/200000, loss = 0.505684 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:47:37 INFO: 2022-08-03 14:47:37: step 4100/200000, loss = 2.336349 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:38 INFO: 2022-08-03 14:47:38: step 4120/200000, loss = 1.328800 (0.053 sec/batch), lr: 0.100000
2022-08-03 14:47:39 INFO: 2022-08-03 14:47:39: step 4140/200000, loss = 0.993597 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:40 INFO: 2022-08-03 14:47:40: step 4160/200000, loss = 1.162113 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:41 INFO: 2022-08-03 14:47:41: step 4180/200000, loss = 1.025981 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:47:42 INFO: 2022-08-03 14:47:42: step 4200/200000, loss = 2.228162 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:47:43 INFO: 2022-08-03 14:47:43: step 4220/200000, loss = 1.208083 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:47:44 INFO: 2022-08-03 14:47:44: step 4240/200000, loss = 1.869982 (0.053 sec/batch), lr: 0.100000
2022-08-03 14:47:45 INFO: 2022-08-03 14:47:45: step 4260/200000, loss = 0.808319 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:46 INFO: 2022-08-03 14:47:46: step 4280/200000, loss = 1.259437 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:47:47 INFO: 2022-08-03 14:47:47: step 4300/200000, loss = 2.201404 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:47:47 INFO: 2022-08-03 14:47:47: step 4320/200000, loss = 1.547777 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:47:48 INFO: 2022-08-03 14:47:48: step 4340/200000, loss = 0.809826 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:47:49 INFO: 2022-08-03 14:47:49: step 4360/200000, loss = 1.040588 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:47:50 INFO: 2022-08-03 14:47:50: step 4380/200000, loss = 1.095885 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:51 INFO: 2022-08-03 14:47:51: step 4400/200000, loss = 0.598720 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:47:52 INFO: 2022-08-03 14:47:52: step 4420/200000, loss = 1.148775 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:47:53 INFO: 2022-08-03 14:47:53: step 4440/200000, loss = 1.304138 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:47:54 INFO: 2022-08-03 14:47:54: step 4460/200000, loss = 1.855282 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:47:55 INFO: 2022-08-03 14:47:55: step 4480/200000, loss = 0.822129 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:47:56 INFO: 2022-08-03 14:47:56: step 4500/200000, loss = 1.319900 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:47:56 INFO: Evaluating on dev set...
2022-08-03 14:48:20 INFO: Score by entity:
Prec.	Rec.	F1
88.60	87.61	88.10
2022-08-03 14:48:20 INFO: step 4500: train_loss = 1.220395, dev_score = 0.8810
2022-08-03 14:48:21 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:48:21 INFO: New best model saved.
2022-08-03 14:48:21 INFO: 
2022-08-03 14:48:22 INFO: 2022-08-03 14:48:22: step 4520/200000, loss = 0.667525 (0.049 sec/batch), lr: 0.100000
2022-08-03 14:48:23 INFO: 2022-08-03 14:48:23: step 4540/200000, loss = 0.980478 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:48:24 INFO: 2022-08-03 14:48:24: step 4560/200000, loss = 0.619099 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:48:25 INFO: 2022-08-03 14:48:25: step 4580/200000, loss = 0.956160 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:48:26 INFO: 2022-08-03 14:48:26: step 4600/200000, loss = 0.912898 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:48:27 INFO: 2022-08-03 14:48:27: step 4620/200000, loss = 0.929500 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:48:28 INFO: 2022-08-03 14:48:28: step 4640/200000, loss = 0.604642 (0.031 sec/batch), lr: 0.100000
2022-08-03 14:48:29 INFO: 2022-08-03 14:48:29: step 4660/200000, loss = 0.939159 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:48:29 INFO: 2022-08-03 14:48:29: step 4680/200000, loss = 1.743716 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:48:30 INFO: 2022-08-03 14:48:30: step 4700/200000, loss = 0.486097 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:48:31 INFO: 2022-08-03 14:48:31: step 4720/200000, loss = 1.713591 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:48:32 INFO: 2022-08-03 14:48:32: step 4740/200000, loss = 1.727109 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:48:33 INFO: 2022-08-03 14:48:33: step 4760/200000, loss = 1.533554 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:48:34 INFO: 2022-08-03 14:48:34: step 4780/200000, loss = 1.451773 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:48:35 INFO: 2022-08-03 14:48:35: step 4800/200000, loss = 2.139425 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:48:36 INFO: 2022-08-03 14:48:36: step 4820/200000, loss = 0.515769 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:48:36 INFO: 2022-08-03 14:48:36: step 4840/200000, loss = 1.070116 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:48:37 INFO: 2022-08-03 14:48:37: step 4860/200000, loss = 1.581391 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:48:38 INFO: 2022-08-03 14:48:38: step 4880/200000, loss = 1.016930 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:48:39 INFO: 2022-08-03 14:48:39: step 4900/200000, loss = 1.341309 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:48:40 INFO: 2022-08-03 14:48:40: step 4920/200000, loss = 1.033979 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:48:41 INFO: 2022-08-03 14:48:41: step 4940/200000, loss = 0.846331 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:48:42 INFO: 2022-08-03 14:48:42: step 4960/200000, loss = 1.148612 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:48:43 INFO: 2022-08-03 14:48:43: step 4980/200000, loss = 0.816962 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:48:44 INFO: 2022-08-03 14:48:44: step 5000/200000, loss = 1.697498 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:48:44 INFO: Evaluating on dev set...
2022-08-03 14:49:08 INFO: Score by entity:
Prec.	Rec.	F1
87.93	88.25	88.09
2022-08-03 14:49:08 INFO: step 5000: train_loss = 1.146003, dev_score = 0.8809
2022-08-03 14:49:08 INFO: 
2022-08-03 14:49:09 INFO: 2022-08-03 14:49:09: step 5020/200000, loss = 1.004951 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:49:10 INFO: 2022-08-03 14:49:10: step 5040/200000, loss = 2.012631 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:49:11 INFO: 2022-08-03 14:49:11: step 5060/200000, loss = 1.294348 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:49:12 INFO: 2022-08-03 14:49:12: step 5080/200000, loss = 1.044077 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:49:13 INFO: 2022-08-03 14:49:13: step 5100/200000, loss = 1.327070 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:13 INFO: 2022-08-03 14:49:13: step 5120/200000, loss = 1.944347 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:49:14 INFO: 2022-08-03 14:49:14: step 5140/200000, loss = 1.005656 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:49:15 INFO: 2022-08-03 14:49:15: step 5160/200000, loss = 0.796300 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:16 INFO: 2022-08-03 14:49:16: step 5180/200000, loss = 1.197189 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:49:17 INFO: 2022-08-03 14:49:17: step 5200/200000, loss = 0.503560 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:18 INFO: 2022-08-03 14:49:18: step 5220/200000, loss = 0.895334 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:49:19 INFO: 2022-08-03 14:49:19: step 5240/200000, loss = 2.479998 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:49:20 INFO: 2022-08-03 14:49:20: step 5260/200000, loss = 0.883534 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:21 INFO: 2022-08-03 14:49:21: step 5280/200000, loss = 1.216687 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:21 INFO: 2022-08-03 14:49:21: step 5300/200000, loss = 1.397976 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:49:22 INFO: 2022-08-03 14:49:22: step 5320/200000, loss = 1.292330 (0.050 sec/batch), lr: 0.100000
2022-08-03 14:49:23 INFO: 2022-08-03 14:49:23: step 5340/200000, loss = 0.809420 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:49:24 INFO: 2022-08-03 14:49:24: step 5360/200000, loss = 0.555058 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:49:25 INFO: 2022-08-03 14:49:25: step 5380/200000, loss = 0.619668 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:49:26 INFO: 2022-08-03 14:49:26: step 5400/200000, loss = 0.598542 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:49:27 INFO: 2022-08-03 14:49:27: step 5420/200000, loss = 1.068083 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:49:28 INFO: 2022-08-03 14:49:28: step 5440/200000, loss = 0.302219 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:49:28 INFO: 2022-08-03 14:49:28: step 5460/200000, loss = 0.510412 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:49:29 INFO: 2022-08-03 14:49:29: step 5480/200000, loss = 1.086893 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:49:30 INFO: 2022-08-03 14:49:30: step 5500/200000, loss = 1.102731 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:49:30 INFO: Evaluating on dev set...
2022-08-03 14:49:55 INFO: Score by entity:
Prec.	Rec.	F1
89.96	87.61	88.77
2022-08-03 14:49:55 INFO: step 5500: train_loss = 1.053921, dev_score = 0.8877
2022-08-03 14:49:56 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:49:56 INFO: New best model saved.
2022-08-03 14:49:56 INFO: 
2022-08-03 14:49:57 INFO: 2022-08-03 14:49:57: step 5520/200000, loss = 0.506528 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:49:58 INFO: 2022-08-03 14:49:58: step 5540/200000, loss = 0.513745 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:49:59 INFO: 2022-08-03 14:49:59: step 5560/200000, loss = 0.560877 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:50:00 INFO: 2022-08-03 14:50:00: step 5580/200000, loss = 1.445604 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:50:00 INFO: 2022-08-03 14:50:00: step 5600/200000, loss = 0.889880 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:50:01 INFO: 2022-08-03 14:50:01: step 5620/200000, loss = 1.499376 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:50:02 INFO: 2022-08-03 14:50:02: step 5640/200000, loss = 1.397264 (0.062 sec/batch), lr: 0.100000
2022-08-03 14:50:03 INFO: 2022-08-03 14:50:03: step 5660/200000, loss = 0.351950 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:50:04 INFO: 2022-08-03 14:50:04: step 5680/200000, loss = 0.932632 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:50:05 INFO: 2022-08-03 14:50:05: step 5700/200000, loss = 0.406313 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:50:06 INFO: 2022-08-03 14:50:06: step 5720/200000, loss = 0.427269 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:50:07 INFO: 2022-08-03 14:50:07: step 5740/200000, loss = 1.043348 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:50:08 INFO: 2022-08-03 14:50:08: step 5760/200000, loss = 1.527440 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:50:09 INFO: 2022-08-03 14:50:09: step 5780/200000, loss = 0.879617 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:50:09 INFO: 2022-08-03 14:50:09: step 5800/200000, loss = 1.082369 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:10 INFO: 2022-08-03 14:50:10: step 5820/200000, loss = 0.719822 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:11 INFO: 2022-08-03 14:50:11: step 5840/200000, loss = 0.509582 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:50:12 INFO: 2022-08-03 14:50:12: step 5860/200000, loss = 1.091293 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:13 INFO: 2022-08-03 14:50:13: step 5880/200000, loss = 1.621243 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:14 INFO: 2022-08-03 14:50:14: step 5900/200000, loss = 0.771060 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:15 INFO: 2022-08-03 14:50:15: step 5920/200000, loss = 0.511119 (0.030 sec/batch), lr: 0.100000
2022-08-03 14:50:16 INFO: 2022-08-03 14:50:16: step 5940/200000, loss = 0.813889 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:50:17 INFO: 2022-08-03 14:50:17: step 5960/200000, loss = 1.567022 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:50:18 INFO: 2022-08-03 14:50:18: step 5980/200000, loss = 0.324634 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:19 INFO: 2022-08-03 14:50:19: step 6000/200000, loss = 2.087351 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:50:19 INFO: Evaluating on dev set...
2022-08-03 14:50:43 INFO: Score by entity:
Prec.	Rec.	F1
89.73	88.40	89.06
2022-08-03 14:50:43 INFO: step 6000: train_loss = 0.908522, dev_score = 0.8906
2022-08-03 14:50:44 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:50:44 INFO: New best model saved.
2022-08-03 14:50:44 INFO: 
2022-08-03 14:50:45 INFO: 2022-08-03 14:50:45: step 6020/200000, loss = 0.696952 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:50:46 INFO: 2022-08-03 14:50:46: step 6040/200000, loss = 1.108011 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:50:47 INFO: 2022-08-03 14:50:47: step 6060/200000, loss = 1.400914 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:50:48 INFO: 2022-08-03 14:50:48: step 6080/200000, loss = 0.785498 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:50:49 INFO: 2022-08-03 14:50:49: step 6100/200000, loss = 1.039564 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:50:50 INFO: 2022-08-03 14:50:50: step 6120/200000, loss = 0.575641 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:50:51 INFO: 2022-08-03 14:50:51: step 6140/200000, loss = 0.748407 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:51 INFO: 2022-08-03 14:50:51: step 6160/200000, loss = 0.619803 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:50:52 INFO: 2022-08-03 14:50:52: step 6180/200000, loss = 0.498179 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:50:53 INFO: 2022-08-03 14:50:53: step 6200/200000, loss = 0.731496 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:50:54 INFO: 2022-08-03 14:50:54: step 6220/200000, loss = 1.844373 (0.049 sec/batch), lr: 0.100000
2022-08-03 14:50:55 INFO: 2022-08-03 14:50:55: step 6240/200000, loss = 1.054141 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:50:56 INFO: 2022-08-03 14:50:56: step 6260/200000, loss = 0.971634 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:50:57 INFO: 2022-08-03 14:50:57: step 6280/200000, loss = 2.220423 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:50:58 INFO: 2022-08-03 14:50:58: step 6300/200000, loss = 0.505984 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:50:59 INFO: 2022-08-03 14:50:59: step 6320/200000, loss = 0.709151 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:51:00 INFO: 2022-08-03 14:51:00: step 6340/200000, loss = 1.178999 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:51:00 INFO: 2022-08-03 14:51:00: step 6360/200000, loss = 0.603805 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:51:01 INFO: 2022-08-03 14:51:01: step 6380/200000, loss = 1.165889 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:51:02 INFO: 2022-08-03 14:51:02: step 6400/200000, loss = 0.817985 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:51:03 INFO: 2022-08-03 14:51:03: step 6420/200000, loss = 0.552004 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:51:04 INFO: 2022-08-03 14:51:04: step 6440/200000, loss = 0.638794 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:05 INFO: 2022-08-03 14:51:05: step 6460/200000, loss = 0.894803 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:51:06 INFO: 2022-08-03 14:51:06: step 6480/200000, loss = 1.068313 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:51:07 INFO: 2022-08-03 14:51:07: step 6500/200000, loss = 0.782825 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:51:07 INFO: Evaluating on dev set...
2022-08-03 14:51:31 INFO: Score by entity:
Prec.	Rec.	F1
91.07	89.62	90.34
2022-08-03 14:51:31 INFO: step 6500: train_loss = 0.850133, dev_score = 0.9034
2022-08-03 14:51:32 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:51:32 INFO: New best model saved.
2022-08-03 14:51:32 INFO: 
2022-08-03 14:51:33 INFO: 2022-08-03 14:51:33: step 6520/200000, loss = 0.587972 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:51:34 INFO: 2022-08-03 14:51:34: step 6540/200000, loss = 1.337036 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:51:35 INFO: 2022-08-03 14:51:35: step 6560/200000, loss = 0.479970 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:51:36 INFO: 2022-08-03 14:51:36: step 6580/200000, loss = 1.634330 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:37 INFO: 2022-08-03 14:51:37: step 6600/200000, loss = 1.039830 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:51:38 INFO: 2022-08-03 14:51:38: step 6620/200000, loss = 1.126062 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:51:39 INFO: 2022-08-03 14:51:39: step 6640/200000, loss = 1.542552 (0.050 sec/batch), lr: 0.100000
2022-08-03 14:51:39 INFO: 2022-08-03 14:51:39: step 6660/200000, loss = 0.541177 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:51:40 INFO: 2022-08-03 14:51:40: step 6680/200000, loss = 0.672780 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:41 INFO: 2022-08-03 14:51:41: step 6700/200000, loss = 0.328521 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:51:42 INFO: 2022-08-03 14:51:42: step 6720/200000, loss = 0.989026 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:43 INFO: 2022-08-03 14:51:43: step 6740/200000, loss = 0.283903 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:51:44 INFO: 2022-08-03 14:51:44: step 6760/200000, loss = 1.079888 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:51:45 INFO: 2022-08-03 14:51:45: step 6780/200000, loss = 0.850837 (0.053 sec/batch), lr: 0.100000
2022-08-03 14:51:46 INFO: 2022-08-03 14:51:46: step 6800/200000, loss = 0.934002 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:51:46 INFO: 2022-08-03 14:51:46: step 6820/200000, loss = 1.080475 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:47 INFO: 2022-08-03 14:51:47: step 6840/200000, loss = 0.164892 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:51:48 INFO: 2022-08-03 14:51:48: step 6860/200000, loss = 0.830531 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:51:49 INFO: 2022-08-03 14:51:49: step 6880/200000, loss = 1.193928 (0.029 sec/batch), lr: 0.100000
2022-08-03 14:51:50 INFO: 2022-08-03 14:51:50: step 6900/200000, loss = 1.226343 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:51:51 INFO: 2022-08-03 14:51:51: step 6920/200000, loss = 0.742592 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:51:52 INFO: 2022-08-03 14:51:52: step 6940/200000, loss = 0.820584 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:51:53 INFO: 2022-08-03 14:51:53: step 6960/200000, loss = 0.464137 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:51:54 INFO: 2022-08-03 14:51:54: step 6980/200000, loss = 1.672280 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:55 INFO: 2022-08-03 14:51:55: step 7000/200000, loss = 0.148078 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:51:55 INFO: Evaluating on dev set...
2022-08-03 14:52:21 INFO: Score by entity:
Prec.	Rec.	F1
91.40	89.92	90.65
2022-08-03 14:52:21 INFO: step 7000: train_loss = 0.811991, dev_score = 0.9065
2022-08-03 14:52:22 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:52:22 INFO: New best model saved.
2022-08-03 14:52:22 INFO: 
2022-08-03 14:52:23 INFO: 2022-08-03 14:52:23: step 7020/200000, loss = 0.422624 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:52:24 INFO: 2022-08-03 14:52:24: step 7040/200000, loss = 0.530039 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:52:25 INFO: 2022-08-03 14:52:25: step 7060/200000, loss = 0.299724 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:52:26 INFO: 2022-08-03 14:52:26: step 7080/200000, loss = 0.481745 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:52:27 INFO: 2022-08-03 14:52:27: step 7100/200000, loss = 0.686312 (0.051 sec/batch), lr: 0.100000
2022-08-03 14:52:28 INFO: 2022-08-03 14:52:28: step 7120/200000, loss = 1.108654 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:52:29 INFO: 2022-08-03 14:52:29: step 7140/200000, loss = 0.692871 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:52:29 INFO: 2022-08-03 14:52:29: step 7160/200000, loss = 0.400267 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:52:30 INFO: 2022-08-03 14:52:30: step 7180/200000, loss = 1.009868 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:52:31 INFO: 2022-08-03 14:52:31: step 7200/200000, loss = 0.753280 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:52:32 INFO: 2022-08-03 14:52:32: step 7220/200000, loss = 0.624866 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:52:33 INFO: 2022-08-03 14:52:33: step 7240/200000, loss = 0.614010 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:52:34 INFO: 2022-08-03 14:52:34: step 7260/200000, loss = 0.741134 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:52:35 INFO: 2022-08-03 14:52:35: step 7280/200000, loss = 0.457204 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:52:36 INFO: 2022-08-03 14:52:36: step 7300/200000, loss = 0.561982 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:52:37 INFO: 2022-08-03 14:52:37: step 7320/200000, loss = 1.257054 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:52:37 INFO: 2022-08-03 14:52:37: step 7340/200000, loss = 1.132069 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:52:38 INFO: 2022-08-03 14:52:38: step 7360/200000, loss = 0.651260 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:52:39 INFO: 2022-08-03 14:52:39: step 7380/200000, loss = 1.019761 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:52:40 INFO: 2022-08-03 14:52:40: step 7400/200000, loss = 0.648206 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:52:41 INFO: 2022-08-03 14:52:41: step 7420/200000, loss = 0.570724 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:52:42 INFO: 2022-08-03 14:52:42: step 7440/200000, loss = 0.711494 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:52:43 INFO: 2022-08-03 14:52:43: step 7460/200000, loss = 0.573921 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:52:44 INFO: 2022-08-03 14:52:44: step 7480/200000, loss = 1.334684 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:52:45 INFO: 2022-08-03 14:52:45: step 7500/200000, loss = 0.332397 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:52:45 INFO: Evaluating on dev set...
2022-08-03 14:53:09 INFO: Score by entity:
Prec.	Rec.	F1
92.08	89.97	91.01
2022-08-03 14:53:09 INFO: step 7500: train_loss = 0.765243, dev_score = 0.9101
2022-08-03 14:53:10 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:53:10 INFO: New best model saved.
2022-08-03 14:53:10 INFO: 
2022-08-03 14:53:11 INFO: 2022-08-03 14:53:11: step 7520/200000, loss = 1.012474 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:53:12 INFO: 2022-08-03 14:53:12: step 7540/200000, loss = 2.938251 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:53:13 INFO: 2022-08-03 14:53:13: step 7560/200000, loss = 0.742660 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:14 INFO: 2022-08-03 14:53:14: step 7580/200000, loss = 1.458011 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:53:15 INFO: 2022-08-03 14:53:15: step 7600/200000, loss = 0.510023 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:16 INFO: 2022-08-03 14:53:16: step 7620/200000, loss = 0.647328 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:53:16 INFO: 2022-08-03 14:53:16: step 7640/200000, loss = 0.393148 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:17 INFO: 2022-08-03 14:53:17: step 7660/200000, loss = 2.207486 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:53:18 INFO: 2022-08-03 14:53:18: step 7680/200000, loss = 0.411030 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:53:19 INFO: 2022-08-03 14:53:19: step 7700/200000, loss = 1.018270 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:53:20 INFO: 2022-08-03 14:53:20: step 7720/200000, loss = 1.155524 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:53:21 INFO: 2022-08-03 14:53:21: step 7740/200000, loss = 1.018921 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:53:22 INFO: 2022-08-03 14:53:22: step 7760/200000, loss = 1.791674 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:53:23 INFO: 2022-08-03 14:53:23: step 7780/200000, loss = 0.912378 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:53:23 INFO: 2022-08-03 14:53:23: step 7800/200000, loss = 0.342839 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:24 INFO: 2022-08-03 14:53:24: step 7820/200000, loss = 0.288852 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:53:25 INFO: 2022-08-03 14:53:25: step 7840/200000, loss = 0.720657 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:26 INFO: 2022-08-03 14:53:26: step 7860/200000, loss = 0.804484 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:53:27 INFO: 2022-08-03 14:53:27: step 7880/200000, loss = 0.722764 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:53:28 INFO: 2022-08-03 14:53:28: step 7900/200000, loss = 0.298197 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:53:29 INFO: 2022-08-03 14:53:29: step 7920/200000, loss = 0.671999 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:53:30 INFO: 2022-08-03 14:53:30: step 7940/200000, loss = 0.979026 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:53:31 INFO: 2022-08-03 14:53:31: step 7960/200000, loss = 0.427109 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:53:32 INFO: 2022-08-03 14:53:32: step 7980/200000, loss = 0.877466 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:53:32 INFO: 2022-08-03 14:53:32: step 8000/200000, loss = 0.392715 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:53:32 INFO: Evaluating on dev set...
2022-08-03 14:53:57 INFO: Score by entity:
Prec.	Rec.	F1
92.62	90.19	91.39
2022-08-03 14:53:57 INFO: step 8000: train_loss = 0.764703, dev_score = 0.9139
2022-08-03 14:53:58 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:53:58 INFO: New best model saved.
2022-08-03 14:53:58 INFO: 
2022-08-03 14:53:59 INFO: 2022-08-03 14:53:59: step 8020/200000, loss = 1.069194 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:00 INFO: 2022-08-03 14:54:00: step 8040/200000, loss = 0.850612 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:54:01 INFO: 2022-08-03 14:54:01: step 8060/200000, loss = 0.491426 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:02 INFO: 2022-08-03 14:54:01: step 8080/200000, loss = 0.553074 (0.031 sec/batch), lr: 0.100000
2022-08-03 14:54:02 INFO: 2022-08-03 14:54:02: step 8100/200000, loss = 0.656968 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:54:03 INFO: 2022-08-03 14:54:03: step 8120/200000, loss = 1.499245 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:54:04 INFO: 2022-08-03 14:54:04: step 8140/200000, loss = 0.913901 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:54:05 INFO: 2022-08-03 14:54:05: step 8160/200000, loss = 0.524593 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:06 INFO: 2022-08-03 14:54:06: step 8180/200000, loss = 0.288589 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:54:07 INFO: 2022-08-03 14:54:07: step 8200/200000, loss = 0.512120 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:08 INFO: 2022-08-03 14:54:08: step 8220/200000, loss = 0.498468 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:54:09 INFO: 2022-08-03 14:54:09: step 8240/200000, loss = 0.670776 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:54:10 INFO: 2022-08-03 14:54:10: step 8260/200000, loss = 1.335725 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:10 INFO: 2022-08-03 14:54:10: step 8280/200000, loss = 0.768886 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:11 INFO: 2022-08-03 14:54:11: step 8300/200000, loss = 0.797859 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:12 INFO: 2022-08-03 14:54:12: step 8320/200000, loss = 0.713292 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:54:13 INFO: 2022-08-03 14:54:13: step 8340/200000, loss = 0.337276 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:54:14 INFO: 2022-08-03 14:54:14: step 8360/200000, loss = 1.029587 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:54:15 INFO: 2022-08-03 14:54:15: step 8380/200000, loss = 0.673921 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:54:16 INFO: 2022-08-03 14:54:16: step 8400/200000, loss = 0.189042 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:54:17 INFO: 2022-08-03 14:54:17: step 8420/200000, loss = 0.295888 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:54:18 INFO: 2022-08-03 14:54:18: step 8440/200000, loss = 0.847542 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:54:18 INFO: 2022-08-03 14:54:18: step 8460/200000, loss = 0.854194 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:54:19 INFO: 2022-08-03 14:54:19: step 8480/200000, loss = 1.158722 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:54:20 INFO: 2022-08-03 14:54:20: step 8500/200000, loss = 0.478337 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:54:20 INFO: Evaluating on dev set...
2022-08-03 14:54:45 INFO: Score by entity:
Prec.	Rec.	F1
91.91	90.80	91.35
2022-08-03 14:54:45 INFO: step 8500: train_loss = 0.710668, dev_score = 0.9135
2022-08-03 14:54:45 INFO: 
2022-08-03 14:54:46 INFO: 2022-08-03 14:54:46: step 8520/200000, loss = 0.316068 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:46 INFO: 2022-08-03 14:54:46: step 8540/200000, loss = 0.989193 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:47 INFO: 2022-08-03 14:54:47: step 8560/200000, loss = 0.277278 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:54:48 INFO: 2022-08-03 14:54:48: step 8580/200000, loss = 0.755437 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:49 INFO: 2022-08-03 14:54:49: step 8600/200000, loss = 0.634966 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:54:50 INFO: 2022-08-03 14:54:50: step 8620/200000, loss = 0.401501 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:54:51 INFO: 2022-08-03 14:54:51: step 8640/200000, loss = 0.274288 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:52 INFO: 2022-08-03 14:54:52: step 8660/200000, loss = 0.490197 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:53 INFO: 2022-08-03 14:54:53: step 8680/200000, loss = 1.573573 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:54:54 INFO: 2022-08-03 14:54:54: step 8700/200000, loss = 0.798181 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:54:54 INFO: 2022-08-03 14:54:54: step 8720/200000, loss = 0.626366 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:54:55 INFO: 2022-08-03 14:54:55: step 8740/200000, loss = 0.848319 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:54:56 INFO: 2022-08-03 14:54:56: step 8760/200000, loss = 0.445045 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:54:57 INFO: 2022-08-03 14:54:57: step 8780/200000, loss = 0.470954 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:54:58 INFO: 2022-08-03 14:54:58: step 8800/200000, loss = 0.779555 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:54:59 INFO: 2022-08-03 14:54:59: step 8820/200000, loss = 0.250676 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:55:00 INFO: 2022-08-03 14:55:00: step 8840/200000, loss = 0.571375 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:55:01 INFO: 2022-08-03 14:55:01: step 8860/200000, loss = 0.463613 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:02 INFO: 2022-08-03 14:55:02: step 8880/200000, loss = 1.215668 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:55:02 INFO: 2022-08-03 14:55:02: step 8900/200000, loss = 0.916121 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:03 INFO: 2022-08-03 14:55:03: step 8920/200000, loss = 0.290363 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:55:04 INFO: 2022-08-03 14:55:04: step 8940/200000, loss = 0.484683 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:55:05 INFO: 2022-08-03 14:55:05: step 8960/200000, loss = 0.395232 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:55:06 INFO: 2022-08-03 14:55:06: step 8980/200000, loss = 0.732677 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:07 INFO: 2022-08-03 14:55:07: step 9000/200000, loss = 0.400641 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:55:07 INFO: Evaluating on dev set...
2022-08-03 14:55:31 INFO: Score by entity:
Prec.	Rec.	F1
92.31	90.57	91.43
2022-08-03 14:55:31 INFO: step 9000: train_loss = 0.609924, dev_score = 0.9143
2022-08-03 14:55:33 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:55:33 INFO: New best model saved.
2022-08-03 14:55:33 INFO: 
2022-08-03 14:55:33 INFO: 2022-08-03 14:55:33: step 9020/200000, loss = 0.244028 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:34 INFO: 2022-08-03 14:55:34: step 9040/200000, loss = 0.237342 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:55:35 INFO: 2022-08-03 14:55:35: step 9060/200000, loss = 1.520704 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:36 INFO: 2022-08-03 14:55:36: step 9080/200000, loss = 0.485436 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:37 INFO: 2022-08-03 14:55:37: step 9100/200000, loss = 0.784061 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:38 INFO: 2022-08-03 14:55:38: step 9120/200000, loss = 0.151949 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:39 INFO: 2022-08-03 14:55:39: step 9140/200000, loss = 0.617252 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:55:40 INFO: 2022-08-03 14:55:40: step 9160/200000, loss = 0.390955 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:55:40 INFO: 2022-08-03 14:55:40: step 9180/200000, loss = 0.455048 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:55:41 INFO: 2022-08-03 14:55:41: step 9200/200000, loss = 0.905355 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:42 INFO: 2022-08-03 14:55:42: step 9220/200000, loss = 0.507107 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:55:43 INFO: 2022-08-03 14:55:43: step 9240/200000, loss = 0.364896 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:55:44 INFO: 2022-08-03 14:55:44: step 9260/200000, loss = 1.227915 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:55:45 INFO: 2022-08-03 14:55:45: step 9280/200000, loss = 0.651103 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:46 INFO: 2022-08-03 14:55:46: step 9300/200000, loss = 0.205373 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:55:47 INFO: 2022-08-03 14:55:47: step 9320/200000, loss = 0.243986 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:55:48 INFO: 2022-08-03 14:55:48: step 9340/200000, loss = 0.663579 (0.048 sec/batch), lr: 0.100000
2022-08-03 14:55:49 INFO: 2022-08-03 14:55:49: step 9360/200000, loss = 0.662605 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:55:49 INFO: 2022-08-03 14:55:49: step 9380/200000, loss = 0.698650 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:55:50 INFO: 2022-08-03 14:55:50: step 9400/200000, loss = 0.583886 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:55:51 INFO: 2022-08-03 14:55:51: step 9420/200000, loss = 0.374367 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:55:52 INFO: 2022-08-03 14:55:52: step 9440/200000, loss = 0.663140 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:55:53 INFO: 2022-08-03 14:55:53: step 9460/200000, loss = 0.281039 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:55:54 INFO: 2022-08-03 14:55:54: step 9480/200000, loss = 1.101212 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:55:55 INFO: 2022-08-03 14:55:55: step 9500/200000, loss = 0.479558 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:55:55 INFO: Evaluating on dev set...
2022-08-03 14:56:19 INFO: Score by entity:
Prec.	Rec.	F1
92.51	91.17	91.83
2022-08-03 14:56:19 INFO: step 9500: train_loss = 0.615455, dev_score = 0.9183
2022-08-03 14:56:20 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:56:20 INFO: New best model saved.
2022-08-03 14:56:20 INFO: 
2022-08-03 14:56:21 INFO: 2022-08-03 14:56:21: step 9520/200000, loss = 0.983000 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:56:22 INFO: 2022-08-03 14:56:22: step 9540/200000, loss = 1.098067 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:56:23 INFO: 2022-08-03 14:56:23: step 9560/200000, loss = 0.527803 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:56:24 INFO: 2022-08-03 14:56:24: step 9580/200000, loss = 1.218022 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:56:25 INFO: 2022-08-03 14:56:25: step 9600/200000, loss = 0.754605 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:56:26 INFO: 2022-08-03 14:56:26: step 9620/200000, loss = 0.461164 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:56:27 INFO: 2022-08-03 14:56:27: step 9640/200000, loss = 0.356839 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:56:28 INFO: 2022-08-03 14:56:28: step 9660/200000, loss = 0.748129 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:56:29 INFO: 2022-08-03 14:56:29: step 9680/200000, loss = 0.208916 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:56:29 INFO: 2022-08-03 14:56:29: step 9700/200000, loss = 0.327370 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:56:30 INFO: 2022-08-03 14:56:30: step 9720/200000, loss = 1.077112 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:56:31 INFO: 2022-08-03 14:56:31: step 9740/200000, loss = 0.521445 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:56:32 INFO: 2022-08-03 14:56:32: step 9760/200000, loss = 0.502184 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:56:33 INFO: 2022-08-03 14:56:33: step 9780/200000, loss = 0.347756 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:56:34 INFO: 2022-08-03 14:56:34: step 9800/200000, loss = 0.223765 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:56:35 INFO: 2022-08-03 14:56:35: step 9820/200000, loss = 0.962104 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:56:36 INFO: 2022-08-03 14:56:36: step 9840/200000, loss = 0.504133 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:56:36 INFO: 2022-08-03 14:56:36: step 9860/200000, loss = 0.714492 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:56:37 INFO: 2022-08-03 14:56:37: step 9880/200000, loss = 0.403649 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:56:38 INFO: 2022-08-03 14:56:38: step 9900/200000, loss = 0.133051 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:56:39 INFO: 2022-08-03 14:56:39: step 9920/200000, loss = 0.878078 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:56:40 INFO: 2022-08-03 14:56:40: step 9940/200000, loss = 1.232062 (0.046 sec/batch), lr: 0.100000
2022-08-03 14:56:41 INFO: 2022-08-03 14:56:41: step 9960/200000, loss = 0.343645 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:56:42 INFO: 2022-08-03 14:56:42: step 9980/200000, loss = 0.194598 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:56:43 INFO: 2022-08-03 14:56:43: step 10000/200000, loss = 1.026245 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:56:43 INFO: Evaluating on dev set...
2022-08-03 14:57:07 INFO: Score by entity:
Prec.	Rec.	F1
92.38	91.59	91.99
2022-08-03 14:57:07 INFO: step 10000: train_loss = 0.580905, dev_score = 0.9199
2022-08-03 14:57:08 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:57:08 INFO: New best model saved.
2022-08-03 14:57:08 INFO: 
2022-08-03 14:57:09 INFO: 2022-08-03 14:57:09: step 10020/200000, loss = 0.527914 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:10 INFO: 2022-08-03 14:57:10: step 10040/200000, loss = 0.747130 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:11 INFO: 2022-08-03 14:57:11: step 10060/200000, loss = 1.038608 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:57:12 INFO: 2022-08-03 14:57:12: step 10080/200000, loss = 1.205594 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:57:13 INFO: 2022-08-03 14:57:13: step 10100/200000, loss = 0.325675 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:57:14 INFO: 2022-08-03 14:57:14: step 10120/200000, loss = 0.559915 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:57:15 INFO: 2022-08-03 14:57:15: step 10140/200000, loss = 0.278278 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:57:16 INFO: 2022-08-03 14:57:16: step 10160/200000, loss = 0.129720 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:57:17 INFO: 2022-08-03 14:57:17: step 10180/200000, loss = 0.934854 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:17 INFO: 2022-08-03 14:57:17: step 10200/200000, loss = 0.845326 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:57:18 INFO: 2022-08-03 14:57:18: step 10220/200000, loss = 0.206179 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:57:19 INFO: 2022-08-03 14:57:19: step 10240/200000, loss = 0.485698 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:57:20 INFO: 2022-08-03 14:57:20: step 10260/200000, loss = 1.084250 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:21 INFO: 2022-08-03 14:57:21: step 10280/200000, loss = 0.663812 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:57:22 INFO: 2022-08-03 14:57:22: step 10300/200000, loss = 0.265393 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:57:23 INFO: 2022-08-03 14:57:23: step 10320/200000, loss = 0.322097 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:57:24 INFO: 2022-08-03 14:57:24: step 10340/200000, loss = 0.658061 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:57:25 INFO: 2022-08-03 14:57:25: step 10360/200000, loss = 0.601190 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:57:25 INFO: 2022-08-03 14:57:25: step 10380/200000, loss = 0.358103 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:57:26 INFO: 2022-08-03 14:57:26: step 10400/200000, loss = 1.582505 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:57:27 INFO: 2022-08-03 14:57:27: step 10420/200000, loss = 0.618611 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:57:28 INFO: 2022-08-03 14:57:28: step 10440/200000, loss = 0.270119 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:57:29 INFO: 2022-08-03 14:57:29: step 10460/200000, loss = 1.461693 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:57:30 INFO: 2022-08-03 14:57:30: step 10480/200000, loss = 1.290180 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:31 INFO: 2022-08-03 14:57:31: step 10500/200000, loss = 0.447124 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:57:31 INFO: Evaluating on dev set...
2022-08-03 14:57:55 INFO: Score by entity:
Prec.	Rec.	F1
92.97	90.97	91.96
2022-08-03 14:57:55 INFO: step 10500: train_loss = 0.579715, dev_score = 0.9196
2022-08-03 14:57:55 INFO: 
2022-08-03 14:57:56 INFO: 2022-08-03 14:57:56: step 10520/200000, loss = 0.382542 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:57:57 INFO: 2022-08-03 14:57:57: step 10540/200000, loss = 0.820044 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:57:58 INFO: 2022-08-03 14:57:58: step 10560/200000, loss = 1.033458 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:57:59 INFO: 2022-08-03 14:57:59: step 10580/200000, loss = 1.783533 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:58:00 INFO: 2022-08-03 14:58:00: step 10600/200000, loss = 0.523232 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:58:00 INFO: 2022-08-03 14:58:00: step 10620/200000, loss = 0.822543 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:58:01 INFO: 2022-08-03 14:58:01: step 10640/200000, loss = 0.645972 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:58:02 INFO: 2022-08-03 14:58:02: step 10660/200000, loss = 1.178233 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:58:03 INFO: 2022-08-03 14:58:03: step 10680/200000, loss = 0.454224 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:58:04 INFO: 2022-08-03 14:58:04: step 10700/200000, loss = 0.562193 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:58:05 INFO: 2022-08-03 14:58:05: step 10720/200000, loss = 0.431448 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:58:06 INFO: 2022-08-03 14:58:06: step 10740/200000, loss = 0.568101 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:58:07 INFO: 2022-08-03 14:58:07: step 10760/200000, loss = 0.191058 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:08 INFO: 2022-08-03 14:58:08: step 10780/200000, loss = 0.321548 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:58:09 INFO: 2022-08-03 14:58:09: step 10800/200000, loss = 0.788459 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:58:09 INFO: 2022-08-03 14:58:09: step 10820/200000, loss = 0.212587 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:58:10 INFO: 2022-08-03 14:58:10: step 10840/200000, loss = 0.406764 (0.032 sec/batch), lr: 0.100000
2022-08-03 14:58:11 INFO: 2022-08-03 14:58:11: step 10860/200000, loss = 0.254658 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:58:12 INFO: 2022-08-03 14:58:12: step 10880/200000, loss = 0.298107 (0.031 sec/batch), lr: 0.100000
2022-08-03 14:58:13 INFO: 2022-08-03 14:58:13: step 10900/200000, loss = 0.572765 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:58:14 INFO: 2022-08-03 14:58:14: step 10920/200000, loss = 0.644727 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:15 INFO: 2022-08-03 14:58:15: step 10940/200000, loss = 0.264465 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:58:16 INFO: 2022-08-03 14:58:16: step 10960/200000, loss = 0.265734 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:58:17 INFO: 2022-08-03 14:58:17: step 10980/200000, loss = 0.656627 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:17 INFO: 2022-08-03 14:58:17: step 11000/200000, loss = 0.532578 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:17 INFO: Evaluating on dev set...
2022-08-03 14:58:42 INFO: Score by entity:
Prec.	Rec.	F1
93.10	91.96	92.53
2022-08-03 14:58:42 INFO: step 11000: train_loss = 0.552338, dev_score = 0.9253
2022-08-03 14:58:43 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 14:58:43 INFO: New best model saved.
2022-08-03 14:58:43 INFO: 
2022-08-03 14:58:44 INFO: 2022-08-03 14:58:44: step 11020/200000, loss = 0.596547 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:58:45 INFO: 2022-08-03 14:58:45: step 11040/200000, loss = 0.792712 (0.050 sec/batch), lr: 0.100000
2022-08-03 14:58:46 INFO: 2022-08-03 14:58:46: step 11060/200000, loss = 0.560572 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:58:47 INFO: 2022-08-03 14:58:47: step 11080/200000, loss = 0.682372 (0.050 sec/batch), lr: 0.100000
2022-08-03 14:58:48 INFO: 2022-08-03 14:58:48: step 11100/200000, loss = 1.271105 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:58:49 INFO: 2022-08-03 14:58:49: step 11120/200000, loss = 0.483095 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:58:49 INFO: 2022-08-03 14:58:49: step 11140/200000, loss = 0.681072 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:50 INFO: 2022-08-03 14:58:50: step 11160/200000, loss = 0.494658 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:58:51 INFO: 2022-08-03 14:58:51: step 11180/200000, loss = 0.309199 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:58:52 INFO: 2022-08-03 14:58:52: step 11200/200000, loss = 1.126964 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:58:53 INFO: 2022-08-03 14:58:53: step 11220/200000, loss = 0.813425 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:54 INFO: 2022-08-03 14:58:54: step 11240/200000, loss = 0.233547 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:58:55 INFO: 2022-08-03 14:58:55: step 11260/200000, loss = 0.362078 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:58:56 INFO: 2022-08-03 14:58:56: step 11280/200000, loss = 0.517104 (0.030 sec/batch), lr: 0.100000
2022-08-03 14:58:57 INFO: 2022-08-03 14:58:57: step 11300/200000, loss = 0.319448 (0.044 sec/batch), lr: 0.100000
2022-08-03 14:58:58 INFO: 2022-08-03 14:58:58: step 11320/200000, loss = 0.215796 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:58:59 INFO: 2022-08-03 14:58:59: step 11340/200000, loss = 0.615662 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:58:59 INFO: 2022-08-03 14:58:59: step 11360/200000, loss = 0.691257 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:59:00 INFO: 2022-08-03 14:59:00: step 11380/200000, loss = 0.208088 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:01 INFO: 2022-08-03 14:59:01: step 11400/200000, loss = 0.046027 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:59:02 INFO: 2022-08-03 14:59:02: step 11420/200000, loss = 0.407605 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:59:03 INFO: 2022-08-03 14:59:03: step 11440/200000, loss = 0.393097 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:59:04 INFO: 2022-08-03 14:59:04: step 11460/200000, loss = 0.295963 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:59:05 INFO: 2022-08-03 14:59:05: step 11480/200000, loss = 0.357039 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:06 INFO: 2022-08-03 14:59:06: step 11500/200000, loss = 0.777290 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:06 INFO: Evaluating on dev set...
2022-08-03 14:59:30 INFO: Score by entity:
Prec.	Rec.	F1
92.96	91.97	92.46
2022-08-03 14:59:30 INFO: step 11500: train_loss = 0.517603, dev_score = 0.9246
2022-08-03 14:59:30 INFO: 
2022-08-03 14:59:31 INFO: 2022-08-03 14:59:31: step 11520/200000, loss = 0.492013 (0.045 sec/batch), lr: 0.100000
2022-08-03 14:59:32 INFO: 2022-08-03 14:59:32: step 11540/200000, loss = 0.236911 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:33 INFO: 2022-08-03 14:59:33: step 11560/200000, loss = 0.341439 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:59:34 INFO: 2022-08-03 14:59:34: step 11580/200000, loss = 0.144638 (0.034 sec/batch), lr: 0.100000
2022-08-03 14:59:34 INFO: 2022-08-03 14:59:34: step 11600/200000, loss = 0.158571 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:59:35 INFO: 2022-08-03 14:59:35: step 11620/200000, loss = 0.366644 (0.047 sec/batch), lr: 0.100000
2022-08-03 14:59:36 INFO: 2022-08-03 14:59:36: step 11640/200000, loss = 0.255121 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:37 INFO: 2022-08-03 14:59:37: step 11660/200000, loss = 0.267007 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:38 INFO: 2022-08-03 14:59:38: step 11680/200000, loss = 0.232487 (0.037 sec/batch), lr: 0.100000
2022-08-03 14:59:39 INFO: 2022-08-03 14:59:39: step 11700/200000, loss = 0.401615 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:59:40 INFO: 2022-08-03 14:59:40: step 11720/200000, loss = 0.383229 (0.033 sec/batch), lr: 0.100000
2022-08-03 14:59:41 INFO: 2022-08-03 14:59:41: step 11740/200000, loss = 0.553063 (0.041 sec/batch), lr: 0.100000
2022-08-03 14:59:42 INFO: 2022-08-03 14:59:42: step 11760/200000, loss = 0.302034 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:59:42 INFO: 2022-08-03 14:59:42: step 11780/200000, loss = 0.224399 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:59:43 INFO: 2022-08-03 14:59:43: step 11800/200000, loss = 0.369313 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:59:44 INFO: 2022-08-03 14:59:44: step 11820/200000, loss = 0.561776 (0.054 sec/batch), lr: 0.100000
2022-08-03 14:59:45 INFO: 2022-08-03 14:59:45: step 11840/200000, loss = 0.882391 (0.035 sec/batch), lr: 0.100000
2022-08-03 14:59:46 INFO: 2022-08-03 14:59:46: step 11860/200000, loss = 0.316877 (0.038 sec/batch), lr: 0.100000
2022-08-03 14:59:47 INFO: 2022-08-03 14:59:47: step 11880/200000, loss = 0.224761 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:59:48 INFO: 2022-08-03 14:59:48: step 11900/200000, loss = 0.438947 (0.042 sec/batch), lr: 0.100000
2022-08-03 14:59:49 INFO: 2022-08-03 14:59:49: step 11920/200000, loss = 0.776421 (0.039 sec/batch), lr: 0.100000
2022-08-03 14:59:50 INFO: 2022-08-03 14:59:50: step 11940/200000, loss = 0.423244 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:59:50 INFO: 2022-08-03 14:59:50: step 11960/200000, loss = 0.286092 (0.043 sec/batch), lr: 0.100000
2022-08-03 14:59:51 INFO: 2022-08-03 14:59:51: step 11980/200000, loss = 0.382917 (0.036 sec/batch), lr: 0.100000
2022-08-03 14:59:52 INFO: 2022-08-03 14:59:52: step 12000/200000, loss = 0.427228 (0.040 sec/batch), lr: 0.100000
2022-08-03 14:59:52 INFO: Evaluating on dev set...
2022-08-03 15:00:16 INFO: Score by entity:
Prec.	Rec.	F1
93.46	91.58	92.51
2022-08-03 15:00:16 INFO: step 12000: train_loss = 0.448973, dev_score = 0.9251
2022-08-03 15:00:16 INFO: 
2022-08-03 15:00:17 INFO: 2022-08-03 15:00:17: step 12020/200000, loss = 0.365809 (0.035 sec/batch), lr: 0.100000
2022-08-03 15:00:18 INFO: 2022-08-03 15:00:18: step 12040/200000, loss = 0.645878 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:19 INFO: 2022-08-03 15:00:19: step 12060/200000, loss = 0.564664 (0.043 sec/batch), lr: 0.100000
2022-08-03 15:00:20 INFO: 2022-08-03 15:00:20: step 12080/200000, loss = 0.298291 (0.033 sec/batch), lr: 0.100000
2022-08-03 15:00:21 INFO: 2022-08-03 15:00:21: step 12100/200000, loss = 0.394574 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:00:22 INFO: 2022-08-03 15:00:22: step 12120/200000, loss = 0.744842 (0.045 sec/batch), lr: 0.100000
2022-08-03 15:00:23 INFO: 2022-08-03 15:00:23: step 12140/200000, loss = 0.167721 (0.032 sec/batch), lr: 0.100000
2022-08-03 15:00:24 INFO: 2022-08-03 15:00:24: step 12160/200000, loss = 0.153645 (0.034 sec/batch), lr: 0.100000
2022-08-03 15:00:24 INFO: 2022-08-03 15:00:24: step 12180/200000, loss = 0.420186 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:25 INFO: 2022-08-03 15:00:25: step 12200/200000, loss = 0.443623 (0.035 sec/batch), lr: 0.100000
2022-08-03 15:00:26 INFO: 2022-08-03 15:00:26: step 12220/200000, loss = 0.797705 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:27 INFO: 2022-08-03 15:00:27: step 12240/200000, loss = 0.393261 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:28 INFO: 2022-08-03 15:00:28: step 12260/200000, loss = 0.216727 (0.041 sec/batch), lr: 0.100000
2022-08-03 15:00:29 INFO: 2022-08-03 15:00:29: step 12280/200000, loss = 0.299095 (0.046 sec/batch), lr: 0.100000
2022-08-03 15:00:30 INFO: 2022-08-03 15:00:30: step 12300/200000, loss = 0.569255 (0.046 sec/batch), lr: 0.100000
2022-08-03 15:00:31 INFO: 2022-08-03 15:00:31: step 12320/200000, loss = 0.315153 (0.050 sec/batch), lr: 0.100000
2022-08-03 15:00:32 INFO: 2022-08-03 15:00:32: step 12340/200000, loss = 0.717929 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:32 INFO: 2022-08-03 15:00:32: step 12360/200000, loss = 0.404773 (0.047 sec/batch), lr: 0.100000
2022-08-03 15:00:33 INFO: 2022-08-03 15:00:33: step 12380/200000, loss = 0.231198 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:34 INFO: 2022-08-03 15:00:34: step 12400/200000, loss = 0.584625 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:35 INFO: 2022-08-03 15:00:35: step 12420/200000, loss = 0.154941 (0.034 sec/batch), lr: 0.100000
2022-08-03 15:00:36 INFO: 2022-08-03 15:00:36: step 12440/200000, loss = 0.331209 (0.035 sec/batch), lr: 0.100000
2022-08-03 15:00:37 INFO: 2022-08-03 15:00:37: step 12460/200000, loss = 0.103595 (0.045 sec/batch), lr: 0.100000
2022-08-03 15:00:38 INFO: 2022-08-03 15:00:38: step 12480/200000, loss = 0.560887 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:00:39 INFO: 2022-08-03 15:00:39: step 12500/200000, loss = 0.791408 (0.046 sec/batch), lr: 0.100000
2022-08-03 15:00:39 INFO: Evaluating on dev set...
2022-08-03 15:01:03 INFO: Score by entity:
Prec.	Rec.	F1
92.81	92.20	92.50
2022-08-03 15:01:03 INFO: step 12500: train_loss = 0.474980, dev_score = 0.9250
2022-08-03 15:01:03 INFO: 
2022-08-03 15:01:04 INFO: 2022-08-03 15:01:04: step 12520/200000, loss = 0.734157 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:01:05 INFO: 2022-08-03 15:01:05: step 12540/200000, loss = 0.097692 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:01:06 INFO: 2022-08-03 15:01:06: step 12560/200000, loss = 0.425205 (0.037 sec/batch), lr: 0.100000
2022-08-03 15:01:07 INFO: 2022-08-03 15:01:07: step 12580/200000, loss = 0.254559 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:01:08 INFO: 2022-08-03 15:01:08: step 12600/200000, loss = 0.361380 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:01:08 INFO: 2022-08-03 15:01:08: step 12620/200000, loss = 0.439946 (0.041 sec/batch), lr: 0.100000
2022-08-03 15:01:09 INFO: 2022-08-03 15:01:09: step 12640/200000, loss = 0.209403 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:01:10 INFO: 2022-08-03 15:01:10: step 12660/200000, loss = 0.259980 (0.047 sec/batch), lr: 0.100000
2022-08-03 15:01:11 INFO: 2022-08-03 15:01:11: step 12680/200000, loss = 0.169374 (0.040 sec/batch), lr: 0.100000
2022-08-03 15:01:12 INFO: 2022-08-03 15:01:12: step 12700/200000, loss = 0.249838 (0.034 sec/batch), lr: 0.100000
2022-08-03 15:01:13 INFO: 2022-08-03 15:01:13: step 12720/200000, loss = 0.623446 (0.032 sec/batch), lr: 0.100000
2022-08-03 15:01:14 INFO: 2022-08-03 15:01:14: step 12740/200000, loss = 0.814806 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:01:15 INFO: 2022-08-03 15:01:15: step 12760/200000, loss = 0.286684 (0.042 sec/batch), lr: 0.100000
2022-08-03 15:01:16 INFO: 2022-08-03 15:01:16: step 12780/200000, loss = 0.801068 (0.042 sec/batch), lr: 0.100000
2022-08-03 15:01:17 INFO: 2022-08-03 15:01:17: step 12800/200000, loss = 0.136187 (0.037 sec/batch), lr: 0.100000
2022-08-03 15:01:17 INFO: 2022-08-03 15:01:17: step 12820/200000, loss = 0.312813 (0.044 sec/batch), lr: 0.100000
2022-08-03 15:01:18 INFO: 2022-08-03 15:01:18: step 12840/200000, loss = 0.487722 (0.038 sec/batch), lr: 0.100000
2022-08-03 15:01:19 INFO: 2022-08-03 15:01:19: step 12860/200000, loss = 0.293299 (0.040 sec/batch), lr: 0.100000
2022-08-03 15:01:20 INFO: 2022-08-03 15:01:20: step 12880/200000, loss = 1.454629 (0.039 sec/batch), lr: 0.100000
2022-08-03 15:01:21 INFO: 2022-08-03 15:01:21: step 12900/200000, loss = 0.324392 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:01:22 INFO: 2022-08-03 15:01:22: step 12920/200000, loss = 0.216774 (0.036 sec/batch), lr: 0.100000
2022-08-03 15:01:23 INFO: 2022-08-03 15:01:23: step 12940/200000, loss = 0.305289 (0.041 sec/batch), lr: 0.100000
2022-08-03 15:01:24 INFO: 2022-08-03 15:01:24: step 12960/200000, loss = 0.725127 (0.033 sec/batch), lr: 0.100000
2022-08-03 15:01:25 INFO: 2022-08-03 15:01:25: step 12980/200000, loss = 0.771798 (0.040 sec/batch), lr: 0.100000
2022-08-03 15:01:25 INFO: 2022-08-03 15:01:25: step 13000/200000, loss = 0.547979 (0.037 sec/batch), lr: 0.100000
2022-08-03 15:01:25 INFO: Evaluating on dev set...
2022-08-03 15:01:50 INFO: Score by entity:
Prec.	Rec.	F1
92.93	92.13	92.53
2022-08-03 15:01:50 INFO: step 13000: train_loss = 0.461179, dev_score = 0.9253
2022-08-03 15:01:51 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:01:51 INFO: New best model saved.
2022-08-03 15:01:51 INFO: 
Epoch 00026: reducing learning rate of group 0 to 5.0000e-02.
2022-08-03 15:01:52 INFO: 2022-08-03 15:01:52: step 13020/200000, loss = 0.364369 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:01:53 INFO: 2022-08-03 15:01:53: step 13040/200000, loss = 0.775639 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:01:54 INFO: 2022-08-03 15:01:54: step 13060/200000, loss = 0.785816 (0.047 sec/batch), lr: 0.050000
2022-08-03 15:01:55 INFO: 2022-08-03 15:01:55: step 13080/200000, loss = 0.366121 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:01:56 INFO: 2022-08-03 15:01:56: step 13100/200000, loss = 1.018822 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:01:57 INFO: 2022-08-03 15:01:57: step 13120/200000, loss = 0.269974 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:01:57 INFO: 2022-08-03 15:01:57: step 13140/200000, loss = 0.730567 (0.044 sec/batch), lr: 0.050000
2022-08-03 15:01:58 INFO: 2022-08-03 15:01:58: step 13160/200000, loss = 0.377861 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:01:59 INFO: 2022-08-03 15:01:59: step 13180/200000, loss = 0.287091 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:02:00 INFO: 2022-08-03 15:02:00: step 13200/200000, loss = 0.416573 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:02:01 INFO: 2022-08-03 15:02:01: step 13220/200000, loss = 0.250088 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:02 INFO: 2022-08-03 15:02:02: step 13240/200000, loss = 0.164274 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:02:03 INFO: 2022-08-03 15:02:03: step 13260/200000, loss = 0.407972 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:04 INFO: 2022-08-03 15:02:04: step 13280/200000, loss = 0.293237 (0.049 sec/batch), lr: 0.050000
2022-08-03 15:02:05 INFO: 2022-08-03 15:02:05: step 13300/200000, loss = 0.136710 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:02:05 INFO: 2022-08-03 15:02:05: step 13320/200000, loss = 0.230358 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:02:06 INFO: 2022-08-03 15:02:06: step 13340/200000, loss = 0.263954 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:02:07 INFO: 2022-08-03 15:02:07: step 13360/200000, loss = 0.160912 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:08 INFO: 2022-08-03 15:02:08: step 13380/200000, loss = 0.676909 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:02:09 INFO: 2022-08-03 15:02:09: step 13400/200000, loss = 0.166313 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:02:10 INFO: 2022-08-03 15:02:10: step 13420/200000, loss = 0.477095 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:02:11 INFO: 2022-08-03 15:02:11: step 13440/200000, loss = 0.343217 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:02:12 INFO: 2022-08-03 15:02:12: step 13460/200000, loss = 0.527860 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:02:12 INFO: 2022-08-03 15:02:12: step 13480/200000, loss = 0.265766 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:02:13 INFO: 2022-08-03 15:02:13: step 13500/200000, loss = 0.269835 (0.046 sec/batch), lr: 0.050000
2022-08-03 15:02:13 INFO: Evaluating on dev set...
2022-08-03 15:02:38 INFO: Score by entity:
Prec.	Rec.	F1
93.50	92.49	92.99
2022-08-03 15:02:38 INFO: step 13500: train_loss = 0.445784, dev_score = 0.9299
2022-08-03 15:02:39 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:02:39 INFO: New best model saved.
2022-08-03 15:02:39 INFO: 
2022-08-03 15:02:40 INFO: 2022-08-03 15:02:40: step 13520/200000, loss = 0.484383 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:02:41 INFO: 2022-08-03 15:02:41: step 13540/200000, loss = 0.417122 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:02:42 INFO: 2022-08-03 15:02:42: step 13560/200000, loss = 0.515785 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:02:43 INFO: 2022-08-03 15:02:43: step 13580/200000, loss = 0.088997 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:02:44 INFO: 2022-08-03 15:02:44: step 13600/200000, loss = 0.657601 (0.044 sec/batch), lr: 0.050000
2022-08-03 15:02:45 INFO: 2022-08-03 15:02:45: step 13620/200000, loss = 0.608873 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:02:45 INFO: 2022-08-03 15:02:45: step 13640/200000, loss = 0.440666 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:02:46 INFO: 2022-08-03 15:02:46: step 13660/200000, loss = 0.419279 (0.031 sec/batch), lr: 0.050000
2022-08-03 15:02:47 INFO: 2022-08-03 15:02:47: step 13680/200000, loss = 0.189821 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:02:48 INFO: 2022-08-03 15:02:48: step 13700/200000, loss = 0.374415 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:49 INFO: 2022-08-03 15:02:49: step 13720/200000, loss = 0.553573 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:50 INFO: 2022-08-03 15:02:50: step 13740/200000, loss = 0.861700 (0.044 sec/batch), lr: 0.050000
2022-08-03 15:02:51 INFO: 2022-08-03 15:02:51: step 13760/200000, loss = 0.124489 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:02:52 INFO: 2022-08-03 15:02:52: step 13780/200000, loss = 0.497142 (0.047 sec/batch), lr: 0.050000
2022-08-03 15:02:53 INFO: 2022-08-03 15:02:53: step 13800/200000, loss = 0.561902 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:02:53 INFO: 2022-08-03 15:02:53: step 13820/200000, loss = 0.075892 (0.047 sec/batch), lr: 0.050000
2022-08-03 15:02:54 INFO: 2022-08-03 15:02:54: step 13840/200000, loss = 0.718182 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:02:55 INFO: 2022-08-03 15:02:55: step 13860/200000, loss = 0.447454 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:02:56 INFO: 2022-08-03 15:02:56: step 13880/200000, loss = 0.660160 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:02:57 INFO: 2022-08-03 15:02:57: step 13900/200000, loss = 0.268572 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:02:58 INFO: 2022-08-03 15:02:58: step 13920/200000, loss = 0.402854 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:02:59 INFO: 2022-08-03 15:02:59: step 13940/200000, loss = 0.904854 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:03:00 INFO: 2022-08-03 15:03:00: step 13960/200000, loss = 0.360796 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:03:01 INFO: 2022-08-03 15:03:01: step 13980/200000, loss = 0.241938 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:03:01 INFO: 2022-08-03 15:03:01: step 14000/200000, loss = 0.555358 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:03:01 INFO: Evaluating on dev set...
2022-08-03 15:03:26 INFO: Score by entity:
Prec.	Rec.	F1
93.93	92.57	93.25
2022-08-03 15:03:26 INFO: step 14000: train_loss = 0.432174, dev_score = 0.9325
2022-08-03 15:03:27 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:03:27 INFO: New best model saved.
2022-08-03 15:03:27 INFO: 
2022-08-03 15:03:28 INFO: 2022-08-03 15:03:28: step 14020/200000, loss = 0.657861 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:03:29 INFO: 2022-08-03 15:03:29: step 14040/200000, loss = 0.408313 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:03:30 INFO: 2022-08-03 15:03:30: step 14060/200000, loss = 0.841235 (0.046 sec/batch), lr: 0.050000
2022-08-03 15:03:31 INFO: 2022-08-03 15:03:31: step 14080/200000, loss = 0.649871 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:03:32 INFO: 2022-08-03 15:03:32: step 14100/200000, loss = 0.369167 (0.031 sec/batch), lr: 0.050000
2022-08-03 15:03:32 INFO: 2022-08-03 15:03:32: step 14120/200000, loss = 0.253252 (0.030 sec/batch), lr: 0.050000
2022-08-03 15:03:33 INFO: 2022-08-03 15:03:33: step 14140/200000, loss = 0.236908 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:03:34 INFO: 2022-08-03 15:03:34: step 14160/200000, loss = 0.456610 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:03:35 INFO: 2022-08-03 15:03:35: step 14180/200000, loss = 0.243859 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:03:36 INFO: 2022-08-03 15:03:36: step 14200/200000, loss = 0.657221 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:03:37 INFO: 2022-08-03 15:03:37: step 14220/200000, loss = 0.485573 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:03:38 INFO: 2022-08-03 15:03:38: step 14240/200000, loss = 0.113670 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:03:39 INFO: 2022-08-03 15:03:39: step 14260/200000, loss = 0.109606 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:03:40 INFO: 2022-08-03 15:03:40: step 14280/200000, loss = 0.989573 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:03:41 INFO: 2022-08-03 15:03:41: step 14300/200000, loss = 0.336765 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:03:41 INFO: 2022-08-03 15:03:41: step 14320/200000, loss = 0.489598 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:03:42 INFO: 2022-08-03 15:03:42: step 14340/200000, loss = 0.082909 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:03:43 INFO: 2022-08-03 15:03:43: step 14360/200000, loss = 0.269079 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:03:44 INFO: 2022-08-03 15:03:44: step 14380/200000, loss = 0.054163 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:03:45 INFO: 2022-08-03 15:03:45: step 14400/200000, loss = 0.182184 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:03:46 INFO: 2022-08-03 15:03:46: step 14420/200000, loss = 0.330780 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:03:47 INFO: 2022-08-03 15:03:47: step 14440/200000, loss = 0.357357 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:03:48 INFO: 2022-08-03 15:03:48: step 14460/200000, loss = 0.390422 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:03:49 INFO: 2022-08-03 15:03:49: step 14480/200000, loss = 0.509835 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:03:49 INFO: 2022-08-03 15:03:49: step 14500/200000, loss = 0.446868 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:03:49 INFO: Evaluating on dev set...
2022-08-03 15:04:14 INFO: Score by entity:
Prec.	Rec.	F1
93.72	92.87	93.29
2022-08-03 15:04:14 INFO: step 14500: train_loss = 0.351162, dev_score = 0.9329
2022-08-03 15:04:15 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:04:15 INFO: New best model saved.
2022-08-03 15:04:15 INFO: 
2022-08-03 15:04:16 INFO: 2022-08-03 15:04:16: step 14520/200000, loss = 0.575032 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:04:17 INFO: 2022-08-03 15:04:17: step 14540/200000, loss = 0.407620 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:18 INFO: 2022-08-03 15:04:18: step 14560/200000, loss = 0.602503 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:04:19 INFO: 2022-08-03 15:04:19: step 14580/200000, loss = 1.161549 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:20 INFO: 2022-08-03 15:04:20: step 14600/200000, loss = 0.237490 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:04:21 INFO: 2022-08-03 15:04:21: step 14620/200000, loss = 0.410696 (0.031 sec/batch), lr: 0.050000
2022-08-03 15:04:21 INFO: 2022-08-03 15:04:21: step 14640/200000, loss = 0.083543 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:22 INFO: 2022-08-03 15:04:22: step 14660/200000, loss = 0.317179 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:04:23 INFO: 2022-08-03 15:04:23: step 14680/200000, loss = 0.268771 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:24 INFO: 2022-08-03 15:04:24: step 14700/200000, loss = 0.563826 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:04:25 INFO: 2022-08-03 15:04:25: step 14720/200000, loss = 0.530002 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:04:26 INFO: 2022-08-03 15:04:26: step 14740/200000, loss = 0.477436 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:04:27 INFO: 2022-08-03 15:04:27: step 14760/200000, loss = 0.264088 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:04:28 INFO: 2022-08-03 15:04:28: step 14780/200000, loss = 0.383343 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:04:29 INFO: 2022-08-03 15:04:29: step 14800/200000, loss = 0.342640 (0.044 sec/batch), lr: 0.050000
2022-08-03 15:04:29 INFO: 2022-08-03 15:04:29: step 14820/200000, loss = 0.466999 (0.044 sec/batch), lr: 0.050000
2022-08-03 15:04:30 INFO: 2022-08-03 15:04:30: step 14840/200000, loss = 0.371627 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:04:31 INFO: 2022-08-03 15:04:31: step 14860/200000, loss = 0.163124 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:04:32 INFO: 2022-08-03 15:04:32: step 14880/200000, loss = 0.791279 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:04:33 INFO: 2022-08-03 15:04:33: step 14900/200000, loss = 0.349402 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:04:34 INFO: 2022-08-03 15:04:34: step 14920/200000, loss = 0.454516 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:04:35 INFO: 2022-08-03 15:04:35: step 14940/200000, loss = 0.380905 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:36 INFO: 2022-08-03 15:04:36: step 14960/200000, loss = 0.322403 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:04:36 INFO: 2022-08-03 15:04:36: step 14980/200000, loss = 0.528571 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:04:37 INFO: 2022-08-03 15:04:37: step 15000/200000, loss = 0.315708 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:04:37 INFO: Evaluating on dev set...
2022-08-03 15:05:02 INFO: Score by entity:
Prec.	Rec.	F1
93.97	93.01	93.48
2022-08-03 15:05:02 INFO: step 15000: train_loss = 0.356539, dev_score = 0.9348
2022-08-03 15:05:03 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:05:03 INFO: New best model saved.
2022-08-03 15:05:03 INFO: 
2022-08-03 15:05:04 INFO: 2022-08-03 15:05:04: step 15020/200000, loss = 0.286700 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:05:05 INFO: 2022-08-03 15:05:05: step 15040/200000, loss = 0.239889 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:05:06 INFO: 2022-08-03 15:05:06: step 15060/200000, loss = 0.124673 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:05:07 INFO: 2022-08-03 15:05:07: step 15080/200000, loss = 0.352690 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:05:08 INFO: 2022-08-03 15:05:08: step 15100/200000, loss = 0.343870 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:05:08 INFO: 2022-08-03 15:05:08: step 15120/200000, loss = 0.135337 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:05:09 INFO: 2022-08-03 15:05:09: step 15140/200000, loss = 0.229544 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:05:10 INFO: 2022-08-03 15:05:10: step 15160/200000, loss = 0.165774 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:05:11 INFO: 2022-08-03 15:05:11: step 15180/200000, loss = 0.289287 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:05:12 INFO: 2022-08-03 15:05:12: step 15200/200000, loss = 0.207824 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:05:13 INFO: 2022-08-03 15:05:13: step 15220/200000, loss = 0.388178 (0.048 sec/batch), lr: 0.050000
2022-08-03 15:05:14 INFO: 2022-08-03 15:05:14: step 15240/200000, loss = 0.553913 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:05:15 INFO: 2022-08-03 15:05:15: step 15260/200000, loss = 0.395363 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:05:16 INFO: 2022-08-03 15:05:16: step 15280/200000, loss = 0.126208 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:05:17 INFO: 2022-08-03 15:05:17: step 15300/200000, loss = 0.251987 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:05:17 INFO: 2022-08-03 15:05:17: step 15320/200000, loss = 0.694439 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:05:18 INFO: 2022-08-03 15:05:18: step 15340/200000, loss = 0.259462 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:05:19 INFO: 2022-08-03 15:05:19: step 15360/200000, loss = 0.417651 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:05:20 INFO: 2022-08-03 15:05:20: step 15380/200000, loss = 0.306197 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:05:21 INFO: 2022-08-03 15:05:21: step 15400/200000, loss = 0.359359 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:05:22 INFO: 2022-08-03 15:05:22: step 15420/200000, loss = 0.178580 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:05:23 INFO: 2022-08-03 15:05:23: step 15440/200000, loss = 0.232781 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:05:24 INFO: 2022-08-03 15:05:24: step 15460/200000, loss = 0.351183 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:05:24 INFO: 2022-08-03 15:05:24: step 15480/200000, loss = 0.370417 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:05:25 INFO: 2022-08-03 15:05:25: step 15500/200000, loss = 0.216022 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:05:25 INFO: Evaluating on dev set...
2022-08-03 15:05:50 INFO: Score by entity:
Prec.	Rec.	F1
94.18	92.65	93.41
2022-08-03 15:05:50 INFO: step 15500: train_loss = 0.368177, dev_score = 0.9341
2022-08-03 15:05:50 INFO: 
2022-08-03 15:05:51 INFO: 2022-08-03 15:05:51: step 15520/200000, loss = 0.182979 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:05:51 INFO: 2022-08-03 15:05:51: step 15540/200000, loss = 0.436900 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:05:52 INFO: 2022-08-03 15:05:52: step 15560/200000, loss = 0.375528 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:05:53 INFO: 2022-08-03 15:05:53: step 15580/200000, loss = 0.545378 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:05:54 INFO: 2022-08-03 15:05:54: step 15600/200000, loss = 0.487918 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:05:55 INFO: 2022-08-03 15:05:55: step 15620/200000, loss = 0.362284 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:05:56 INFO: 2022-08-03 15:05:56: step 15640/200000, loss = 0.304844 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:05:57 INFO: 2022-08-03 15:05:57: step 15660/200000, loss = 0.460568 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:05:58 INFO: 2022-08-03 15:05:58: step 15680/200000, loss = 0.316338 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:05:59 INFO: 2022-08-03 15:05:59: step 15700/200000, loss = 0.294119 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:05:59 INFO: 2022-08-03 15:05:59: step 15720/200000, loss = 0.391773 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:06:00 INFO: 2022-08-03 15:06:00: step 15740/200000, loss = 0.350731 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:06:01 INFO: 2022-08-03 15:06:01: step 15760/200000, loss = 0.297205 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:06:02 INFO: 2022-08-03 15:06:02: step 15780/200000, loss = 0.105444 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:06:03 INFO: 2022-08-03 15:06:03: step 15800/200000, loss = 0.378682 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:06:04 INFO: 2022-08-03 15:06:04: step 15820/200000, loss = 0.176914 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:06:05 INFO: 2022-08-03 15:06:05: step 15840/200000, loss = 0.523949 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:06:06 INFO: 2022-08-03 15:06:06: step 15860/200000, loss = 0.242269 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:06:07 INFO: 2022-08-03 15:06:07: step 15880/200000, loss = 0.587778 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:06:07 INFO: 2022-08-03 15:06:07: step 15900/200000, loss = 0.183582 (0.031 sec/batch), lr: 0.050000
2022-08-03 15:06:08 INFO: 2022-08-03 15:06:08: step 15920/200000, loss = 0.391776 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:06:09 INFO: 2022-08-03 15:06:09: step 15940/200000, loss = 0.105356 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:06:10 INFO: 2022-08-03 15:06:10: step 15960/200000, loss = 0.884655 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:06:11 INFO: 2022-08-03 15:06:11: step 15980/200000, loss = 0.347051 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:06:12 INFO: 2022-08-03 15:06:12: step 16000/200000, loss = 0.831789 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:06:12 INFO: Evaluating on dev set...
2022-08-03 15:06:36 INFO: Score by entity:
Prec.	Rec.	F1
93.66	92.95	93.30
2022-08-03 15:06:36 INFO: step 16000: train_loss = 0.369046, dev_score = 0.9330
2022-08-03 15:06:36 INFO: 
2022-08-03 15:06:37 INFO: 2022-08-03 15:06:37: step 16020/200000, loss = 0.152659 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:06:38 INFO: 2022-08-03 15:06:38: step 16040/200000, loss = 0.227407 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:06:39 INFO: 2022-08-03 15:06:39: step 16060/200000, loss = 0.177239 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:06:40 INFO: 2022-08-03 15:06:40: step 16080/200000, loss = 0.290108 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:06:41 INFO: 2022-08-03 15:06:41: step 16100/200000, loss = 0.409018 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:06:42 INFO: 2022-08-03 15:06:42: step 16120/200000, loss = 0.152527 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:06:43 INFO: 2022-08-03 15:06:43: step 16140/200000, loss = 0.402008 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:06:43 INFO: 2022-08-03 15:06:43: step 16160/200000, loss = 0.115832 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:06:44 INFO: 2022-08-03 15:06:44: step 16180/200000, loss = 0.317127 (0.040 sec/batch), lr: 0.050000
2022-08-03 15:06:45 INFO: 2022-08-03 15:06:45: step 16200/200000, loss = 0.561746 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:06:46 INFO: 2022-08-03 15:06:46: step 16220/200000, loss = 0.402234 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:06:47 INFO: 2022-08-03 15:06:47: step 16240/200000, loss = 0.141413 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:06:48 INFO: 2022-08-03 15:06:48: step 16260/200000, loss = 0.302754 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:06:49 INFO: 2022-08-03 15:06:49: step 16280/200000, loss = 0.507539 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:06:50 INFO: 2022-08-03 15:06:50: step 16300/200000, loss = 0.746868 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:06:51 INFO: 2022-08-03 15:06:51: step 16320/200000, loss = 0.448132 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:06:52 INFO: 2022-08-03 15:06:52: step 16340/200000, loss = 0.273507 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:06:52 INFO: 2022-08-03 15:06:52: step 16360/200000, loss = 0.248094 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:06:53 INFO: 2022-08-03 15:06:53: step 16380/200000, loss = 1.737038 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:06:54 INFO: 2022-08-03 15:06:54: step 16400/200000, loss = 0.504759 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:06:55 INFO: 2022-08-03 15:06:55: step 16420/200000, loss = 0.447275 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:06:56 INFO: 2022-08-03 15:06:56: step 16440/200000, loss = 0.586728 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:06:57 INFO: 2022-08-03 15:06:57: step 16460/200000, loss = 0.286563 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:06:58 INFO: 2022-08-03 15:06:58: step 16480/200000, loss = 0.353008 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:06:59 INFO: 2022-08-03 15:06:59: step 16500/200000, loss = 0.337842 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:06:59 INFO: Evaluating on dev set...
2022-08-03 15:07:23 INFO: Score by entity:
Prec.	Rec.	F1
94.16	92.71	93.43
2022-08-03 15:07:23 INFO: step 16500: train_loss = 0.366203, dev_score = 0.9343
2022-08-03 15:07:23 INFO: 
2022-08-03 15:07:24 INFO: 2022-08-03 15:07:24: step 16520/200000, loss = 0.200763 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:25 INFO: 2022-08-03 15:07:25: step 16540/200000, loss = 0.156742 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:07:26 INFO: 2022-08-03 15:07:26: step 16560/200000, loss = 0.534684 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:26 INFO: 2022-08-03 15:07:26: step 16580/200000, loss = 0.357264 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:07:27 INFO: 2022-08-03 15:07:27: step 16600/200000, loss = 0.272707 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:07:28 INFO: 2022-08-03 15:07:28: step 16620/200000, loss = 0.157928 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:07:29 INFO: 2022-08-03 15:07:29: step 16640/200000, loss = 0.264275 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:07:30 INFO: 2022-08-03 15:07:30: step 16660/200000, loss = 0.483557 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:07:31 INFO: 2022-08-03 15:07:31: step 16680/200000, loss = 0.516049 (0.045 sec/batch), lr: 0.050000
2022-08-03 15:07:32 INFO: 2022-08-03 15:07:32: step 16700/200000, loss = 0.091262 (0.037 sec/batch), lr: 0.050000
2022-08-03 15:07:33 INFO: 2022-08-03 15:07:33: step 16720/200000, loss = 0.805965 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:07:34 INFO: 2022-08-03 15:07:34: step 16740/200000, loss = 0.059619 (0.035 sec/batch), lr: 0.050000
2022-08-03 15:07:35 INFO: 2022-08-03 15:07:35: step 16760/200000, loss = 0.924023 (0.042 sec/batch), lr: 0.050000
2022-08-03 15:07:35 INFO: 2022-08-03 15:07:35: step 16780/200000, loss = 0.529738 (0.032 sec/batch), lr: 0.050000
2022-08-03 15:07:36 INFO: 2022-08-03 15:07:36: step 16800/200000, loss = 0.301942 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:37 INFO: 2022-08-03 15:07:37: step 16820/200000, loss = 0.217971 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:07:38 INFO: 2022-08-03 15:07:38: step 16840/200000, loss = 0.403328 (0.043 sec/batch), lr: 0.050000
2022-08-03 15:07:39 INFO: 2022-08-03 15:07:39: step 16860/200000, loss = 0.216094 (0.033 sec/batch), lr: 0.050000
2022-08-03 15:07:40 INFO: 2022-08-03 15:07:40: step 16880/200000, loss = 0.768033 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:41 INFO: 2022-08-03 15:07:41: step 16900/200000, loss = 0.123862 (0.041 sec/batch), lr: 0.050000
2022-08-03 15:07:42 INFO: 2022-08-03 15:07:42: step 16920/200000, loss = 0.219209 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:43 INFO: 2022-08-03 15:07:43: step 16940/200000, loss = 0.120836 (0.038 sec/batch), lr: 0.050000
2022-08-03 15:07:43 INFO: 2022-08-03 15:07:43: step 16960/200000, loss = 0.205760 (0.036 sec/batch), lr: 0.050000
2022-08-03 15:07:44 INFO: 2022-08-03 15:07:44: step 16980/200000, loss = 0.202773 (0.034 sec/batch), lr: 0.050000
2022-08-03 15:07:45 INFO: 2022-08-03 15:07:45: step 17000/200000, loss = 0.175119 (0.039 sec/batch), lr: 0.050000
2022-08-03 15:07:45 INFO: Evaluating on dev set...
2022-08-03 15:08:10 INFO: Score by entity:
Prec.	Rec.	F1
94.15	92.79	93.46
2022-08-03 15:08:10 INFO: step 17000: train_loss = 0.352084, dev_score = 0.9346
2022-08-03 15:08:10 INFO: 
Epoch 00034: reducing learning rate of group 0 to 2.5000e-02.
2022-08-03 15:08:10 INFO: 2022-08-03 15:08:10: step 17020/200000, loss = 0.250280 (0.049 sec/batch), lr: 0.025000
2022-08-03 15:08:11 INFO: 2022-08-03 15:08:11: step 17040/200000, loss = 0.291453 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:08:12 INFO: 2022-08-03 15:08:12: step 17060/200000, loss = 0.097337 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:08:13 INFO: 2022-08-03 15:08:13: step 17080/200000, loss = 0.190879 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:08:14 INFO: 2022-08-03 15:08:14: step 17100/200000, loss = 0.579688 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:08:15 INFO: 2022-08-03 15:08:15: step 17120/200000, loss = 0.192224 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:08:16 INFO: 2022-08-03 15:08:16: step 17140/200000, loss = 0.492856 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:08:17 INFO: 2022-08-03 15:08:17: step 17160/200000, loss = 0.168008 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:08:18 INFO: 2022-08-03 15:08:18: step 17180/200000, loss = 0.283420 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:08:18 INFO: 2022-08-03 15:08:18: step 17200/200000, loss = 0.543498 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:08:19 INFO: 2022-08-03 15:08:19: step 17220/200000, loss = 0.146609 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:08:20 INFO: 2022-08-03 15:08:20: step 17240/200000, loss = 0.659250 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:08:21 INFO: 2022-08-03 15:08:21: step 17260/200000, loss = 0.215539 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:08:22 INFO: 2022-08-03 15:08:22: step 17280/200000, loss = 0.463186 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:08:23 INFO: 2022-08-03 15:08:23: step 17300/200000, loss = 0.158123 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:08:24 INFO: 2022-08-03 15:08:24: step 17320/200000, loss = 0.402029 (0.048 sec/batch), lr: 0.025000
2022-08-03 15:08:25 INFO: 2022-08-03 15:08:25: step 17340/200000, loss = 0.694158 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:08:25 INFO: 2022-08-03 15:08:25: step 17360/200000, loss = 0.080358 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:08:26 INFO: 2022-08-03 15:08:26: step 17380/200000, loss = 0.160481 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:08:27 INFO: 2022-08-03 15:08:27: step 17400/200000, loss = 0.363780 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:08:28 INFO: 2022-08-03 15:08:28: step 17420/200000, loss = 0.211063 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:08:29 INFO: 2022-08-03 15:08:29: step 17440/200000, loss = 0.351209 (0.049 sec/batch), lr: 0.025000
2022-08-03 15:08:30 INFO: 2022-08-03 15:08:30: step 17460/200000, loss = 0.184477 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:08:31 INFO: 2022-08-03 15:08:31: step 17480/200000, loss = 0.082664 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:08:32 INFO: 2022-08-03 15:08:32: step 17500/200000, loss = 0.066005 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:08:32 INFO: Evaluating on dev set...
2022-08-03 15:08:56 INFO: Score by entity:
Prec.	Rec.	F1
94.07	92.90	93.48
2022-08-03 15:08:56 INFO: step 17500: train_loss = 0.315325, dev_score = 0.9348
2022-08-03 15:08:58 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:08:58 INFO: New best model saved.
2022-08-03 15:08:58 INFO: 
2022-08-03 15:08:58 INFO: 2022-08-03 15:08:58: step 17520/200000, loss = 0.054227 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:08:59 INFO: 2022-08-03 15:08:59: step 17540/200000, loss = 0.170462 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:09:00 INFO: 2022-08-03 15:09:00: step 17560/200000, loss = 0.222023 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:09:01 INFO: 2022-08-03 15:09:01: step 17580/200000, loss = 0.243492 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:09:02 INFO: 2022-08-03 15:09:02: step 17600/200000, loss = 0.217000 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:09:03 INFO: 2022-08-03 15:09:03: step 17620/200000, loss = 0.223160 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:09:04 INFO: 2022-08-03 15:09:04: step 17640/200000, loss = 0.185907 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:09:05 INFO: 2022-08-03 15:09:05: step 17660/200000, loss = 0.309568 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:06 INFO: 2022-08-03 15:09:06: step 17680/200000, loss = 0.227500 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:09:06 INFO: 2022-08-03 15:09:06: step 17700/200000, loss = 0.412212 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:09:07 INFO: 2022-08-03 15:09:07: step 17720/200000, loss = 0.773982 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:08 INFO: 2022-08-03 15:09:08: step 17740/200000, loss = 0.473191 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:09 INFO: 2022-08-03 15:09:09: step 17760/200000, loss = 0.288597 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:09:10 INFO: 2022-08-03 15:09:10: step 17780/200000, loss = 0.392372 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:09:11 INFO: 2022-08-03 15:09:11: step 17800/200000, loss = 0.120581 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:09:12 INFO: 2022-08-03 15:09:12: step 17820/200000, loss = 0.363305 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:13 INFO: 2022-08-03 15:09:13: step 17840/200000, loss = 0.035550 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:09:14 INFO: 2022-08-03 15:09:14: step 17860/200000, loss = 0.499146 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:09:14 INFO: 2022-08-03 15:09:14: step 17880/200000, loss = 0.715082 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:09:15 INFO: 2022-08-03 15:09:15: step 17900/200000, loss = 0.075724 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:16 INFO: 2022-08-03 15:09:16: step 17920/200000, loss = 0.078556 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:09:17 INFO: 2022-08-03 15:09:17: step 17940/200000, loss = 0.245132 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:09:18 INFO: 2022-08-03 15:09:18: step 17960/200000, loss = 0.132892 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:09:19 INFO: 2022-08-03 15:09:19: step 17980/200000, loss = 0.193227 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:09:20 INFO: 2022-08-03 15:09:20: step 18000/200000, loss = 0.311855 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:09:20 INFO: Evaluating on dev set...
2022-08-03 15:09:44 INFO: Score by entity:
Prec.	Rec.	F1
94.14	92.77	93.45
2022-08-03 15:09:44 INFO: step 18000: train_loss = 0.309340, dev_score = 0.9345
2022-08-03 15:09:44 INFO: 
2022-08-03 15:09:45 INFO: 2022-08-03 15:09:45: step 18020/200000, loss = 0.243609 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:09:46 INFO: 2022-08-03 15:09:46: step 18040/200000, loss = 0.037137 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:09:47 INFO: 2022-08-03 15:09:47: step 18060/200000, loss = 0.437668 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:09:48 INFO: 2022-08-03 15:09:48: step 18080/200000, loss = 0.364928 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:09:49 INFO: 2022-08-03 15:09:49: step 18100/200000, loss = 0.259530 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:09:49 INFO: 2022-08-03 15:09:49: step 18120/200000, loss = 0.961759 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:09:50 INFO: 2022-08-03 15:09:50: step 18140/200000, loss = 0.046216 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:09:51 INFO: 2022-08-03 15:09:51: step 18160/200000, loss = 0.312140 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:09:52 INFO: 2022-08-03 15:09:52: step 18180/200000, loss = 0.303772 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:09:53 INFO: 2022-08-03 15:09:53: step 18200/200000, loss = 0.083638 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:09:54 INFO: 2022-08-03 15:09:54: step 18220/200000, loss = 0.196854 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:09:55 INFO: 2022-08-03 15:09:55: step 18240/200000, loss = 0.357762 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:09:56 INFO: 2022-08-03 15:09:56: step 18260/200000, loss = 0.352553 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:57 INFO: 2022-08-03 15:09:57: step 18280/200000, loss = 0.245654 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:09:58 INFO: 2022-08-03 15:09:58: step 18300/200000, loss = 0.128365 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:09:58 INFO: 2022-08-03 15:09:58: step 18320/200000, loss = 0.071898 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:09:59 INFO: 2022-08-03 15:09:59: step 18340/200000, loss = 0.147499 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:10:00 INFO: 2022-08-03 15:10:00: step 18360/200000, loss = 0.425017 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:10:01 INFO: 2022-08-03 15:10:01: step 18380/200000, loss = 0.167964 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:10:02 INFO: 2022-08-03 15:10:02: step 18400/200000, loss = 0.125012 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:10:03 INFO: 2022-08-03 15:10:03: step 18420/200000, loss = 0.232992 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:10:04 INFO: 2022-08-03 15:10:04: step 18440/200000, loss = 0.129144 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:10:05 INFO: 2022-08-03 15:10:05: step 18460/200000, loss = 0.383218 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:10:06 INFO: 2022-08-03 15:10:06: step 18480/200000, loss = 0.500318 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:10:06 INFO: 2022-08-03 15:10:06: step 18500/200000, loss = 0.363743 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:10:06 INFO: Evaluating on dev set...
2022-08-03 15:10:31 INFO: Score by entity:
Prec.	Rec.	F1
94.31	92.85	93.58
2022-08-03 15:10:31 INFO: step 18500: train_loss = 0.314425, dev_score = 0.9358
2022-08-03 15:10:32 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:10:32 INFO: New best model saved.
2022-08-03 15:10:32 INFO: 
2022-08-03 15:10:33 INFO: 2022-08-03 15:10:33: step 18520/200000, loss = 0.127882 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:10:34 INFO: 2022-08-03 15:10:34: step 18540/200000, loss = 0.128910 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:10:35 INFO: 2022-08-03 15:10:35: step 18560/200000, loss = 0.253688 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:10:36 INFO: 2022-08-03 15:10:36: step 18580/200000, loss = 0.181785 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:10:37 INFO: 2022-08-03 15:10:37: step 18600/200000, loss = 0.237857 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:10:37 INFO: 2022-08-03 15:10:37: step 18620/200000, loss = 0.481380 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:10:38 INFO: 2022-08-03 15:10:38: step 18640/200000, loss = 0.267927 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:10:39 INFO: 2022-08-03 15:10:39: step 18660/200000, loss = 0.460310 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:10:40 INFO: 2022-08-03 15:10:40: step 18680/200000, loss = 0.211964 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:10:41 INFO: 2022-08-03 15:10:41: step 18700/200000, loss = 0.296942 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:10:42 INFO: 2022-08-03 15:10:42: step 18720/200000, loss = 0.955847 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:10:43 INFO: 2022-08-03 15:10:43: step 18740/200000, loss = 0.092976 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:10:44 INFO: 2022-08-03 15:10:44: step 18760/200000, loss = 0.487305 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:10:45 INFO: 2022-08-03 15:10:45: step 18780/200000, loss = 0.258309 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:10:45 INFO: 2022-08-03 15:10:45: step 18800/200000, loss = 0.399586 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:10:46 INFO: 2022-08-03 15:10:46: step 18820/200000, loss = 0.433894 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:10:47 INFO: 2022-08-03 15:10:47: step 18840/200000, loss = 0.117347 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:10:48 INFO: 2022-08-03 15:10:48: step 18860/200000, loss = 0.116047 (0.058 sec/batch), lr: 0.025000
2022-08-03 15:10:49 INFO: 2022-08-03 15:10:49: step 18880/200000, loss = 0.153009 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:10:50 INFO: 2022-08-03 15:10:50: step 18900/200000, loss = 0.403798 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:10:51 INFO: 2022-08-03 15:10:51: step 18920/200000, loss = 0.336952 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:10:52 INFO: 2022-08-03 15:10:52: step 18940/200000, loss = 0.305591 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:10:53 INFO: 2022-08-03 15:10:53: step 18960/200000, loss = 0.650908 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:10:54 INFO: 2022-08-03 15:10:54: step 18980/200000, loss = 0.276661 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:10:54 INFO: 2022-08-03 15:10:54: step 19000/200000, loss = 0.058617 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:10:54 INFO: Evaluating on dev set...
2022-08-03 15:11:19 INFO: Score by entity:
Prec.	Rec.	F1
94.17	92.98	93.57
2022-08-03 15:11:19 INFO: step 19000: train_loss = 0.304174, dev_score = 0.9357
2022-08-03 15:11:19 INFO: 
2022-08-03 15:11:20 INFO: 2022-08-03 15:11:20: step 19020/200000, loss = 0.721516 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:11:20 INFO: 2022-08-03 15:11:20: step 19040/200000, loss = 0.262411 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:11:21 INFO: 2022-08-03 15:11:21: step 19060/200000, loss = 0.080342 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:11:22 INFO: 2022-08-03 15:11:22: step 19080/200000, loss = 0.294323 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:11:23 INFO: 2022-08-03 15:11:23: step 19100/200000, loss = 0.424548 (0.048 sec/batch), lr: 0.025000
2022-08-03 15:11:24 INFO: 2022-08-03 15:11:24: step 19120/200000, loss = 0.523674 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:11:25 INFO: 2022-08-03 15:11:25: step 19140/200000, loss = 0.237749 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:11:26 INFO: 2022-08-03 15:11:26: step 19160/200000, loss = 0.264249 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:11:27 INFO: 2022-08-03 15:11:27: step 19180/200000, loss = 0.338990 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:11:28 INFO: 2022-08-03 15:11:28: step 19200/200000, loss = 0.273701 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:11:28 INFO: 2022-08-03 15:11:28: step 19220/200000, loss = 1.050280 (0.050 sec/batch), lr: 0.025000
2022-08-03 15:11:29 INFO: 2022-08-03 15:11:29: step 19240/200000, loss = 0.861869 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:11:30 INFO: 2022-08-03 15:11:30: step 19260/200000, loss = 0.206452 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:11:31 INFO: 2022-08-03 15:11:31: step 19280/200000, loss = 0.471920 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:11:32 INFO: 2022-08-03 15:11:32: step 19300/200000, loss = 0.243064 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:11:33 INFO: 2022-08-03 15:11:33: step 19320/200000, loss = 0.701376 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:11:34 INFO: 2022-08-03 15:11:34: step 19340/200000, loss = 0.345644 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:11:35 INFO: 2022-08-03 15:11:35: step 19360/200000, loss = 0.280604 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:11:36 INFO: 2022-08-03 15:11:36: step 19380/200000, loss = 0.749732 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:11:37 INFO: 2022-08-03 15:11:37: step 19400/200000, loss = 0.066800 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:11:37 INFO: 2022-08-03 15:11:37: step 19420/200000, loss = 0.104935 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:11:38 INFO: 2022-08-03 15:11:38: step 19440/200000, loss = 0.131239 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:11:39 INFO: 2022-08-03 15:11:39: step 19460/200000, loss = 0.167832 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:11:40 INFO: 2022-08-03 15:11:40: step 19480/200000, loss = 0.193718 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:11:41 INFO: 2022-08-03 15:11:41: step 19500/200000, loss = 0.259185 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:11:41 INFO: Evaluating on dev set...
2022-08-03 15:12:05 INFO: Score by entity:
Prec.	Rec.	F1
94.44	93.02	93.73
2022-08-03 15:12:05 INFO: step 19500: train_loss = 0.315928, dev_score = 0.9373
2022-08-03 15:12:07 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:12:07 INFO: New best model saved.
2022-08-03 15:12:07 INFO: 
2022-08-03 15:12:08 INFO: 2022-08-03 15:12:08: step 19520/200000, loss = 0.132343 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:12:09 INFO: 2022-08-03 15:12:09: step 19540/200000, loss = 0.274148 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:12:09 INFO: 2022-08-03 15:12:09: step 19560/200000, loss = 0.375807 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:12:10 INFO: 2022-08-03 15:12:10: step 19580/200000, loss = 0.598993 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:12:11 INFO: 2022-08-03 15:12:11: step 19600/200000, loss = 0.078765 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:12:12 INFO: 2022-08-03 15:12:12: step 19620/200000, loss = 0.080867 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:12:13 INFO: 2022-08-03 15:12:13: step 19640/200000, loss = 0.036303 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:12:14 INFO: 2022-08-03 15:12:14: step 19660/200000, loss = 0.137500 (0.052 sec/batch), lr: 0.025000
2022-08-03 15:12:15 INFO: 2022-08-03 15:12:15: step 19680/200000, loss = 0.267494 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:12:16 INFO: 2022-08-03 15:12:16: step 19700/200000, loss = 0.085517 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:12:17 INFO: 2022-08-03 15:12:17: step 19720/200000, loss = 0.485859 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:12:17 INFO: 2022-08-03 15:12:17: step 19740/200000, loss = 0.946739 (0.052 sec/batch), lr: 0.025000
2022-08-03 15:12:18 INFO: 2022-08-03 15:12:18: step 19760/200000, loss = 0.368348 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:12:19 INFO: 2022-08-03 15:12:19: step 19780/200000, loss = 0.143596 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:12:20 INFO: 2022-08-03 15:12:20: step 19800/200000, loss = 0.199829 (0.052 sec/batch), lr: 0.025000
2022-08-03 15:12:21 INFO: 2022-08-03 15:12:21: step 19820/200000, loss = 0.622015 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:12:22 INFO: 2022-08-03 15:12:22: step 19840/200000, loss = 0.192271 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:12:23 INFO: 2022-08-03 15:12:23: step 19860/200000, loss = 0.184303 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:12:24 INFO: 2022-08-03 15:12:24: step 19880/200000, loss = 0.876661 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:12:25 INFO: 2022-08-03 15:12:25: step 19900/200000, loss = 0.782048 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:12:26 INFO: 2022-08-03 15:12:26: step 19920/200000, loss = 0.570890 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:12:27 INFO: 2022-08-03 15:12:27: step 19940/200000, loss = 0.430085 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:12:27 INFO: 2022-08-03 15:12:27: step 19960/200000, loss = 0.337788 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:12:28 INFO: 2022-08-03 15:12:28: step 19980/200000, loss = 0.161528 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:12:29 INFO: 2022-08-03 15:12:29: step 20000/200000, loss = 0.300691 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:12:29 INFO: Evaluating on dev set...
2022-08-03 15:12:54 INFO: Score by entity:
Prec.	Rec.	F1
94.19	93.20	93.69
2022-08-03 15:12:54 INFO: step 20000: train_loss = 0.300017, dev_score = 0.9369
2022-08-03 15:12:54 INFO: 
2022-08-03 15:12:55 INFO: 2022-08-03 15:12:55: step 20020/200000, loss = 0.089867 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:12:55 INFO: 2022-08-03 15:12:55: step 20040/200000, loss = 0.200615 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:12:56 INFO: 2022-08-03 15:12:56: step 20060/200000, loss = 0.388612 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:12:57 INFO: 2022-08-03 15:12:57: step 20080/200000, loss = 0.045887 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:12:58 INFO: 2022-08-03 15:12:58: step 20100/200000, loss = 0.153463 (0.047 sec/batch), lr: 0.025000
2022-08-03 15:12:59 INFO: 2022-08-03 15:12:59: step 20120/200000, loss = 0.302624 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:13:00 INFO: 2022-08-03 15:13:00: step 20140/200000, loss = 0.228249 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:13:01 INFO: 2022-08-03 15:13:01: step 20160/200000, loss = 0.403026 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:13:02 INFO: 2022-08-03 15:13:02: step 20180/200000, loss = 0.179193 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:13:03 INFO: 2022-08-03 15:13:03: step 20200/200000, loss = 0.157825 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:13:03 INFO: 2022-08-03 15:13:03: step 20220/200000, loss = 0.416674 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:13:04 INFO: 2022-08-03 15:13:04: step 20240/200000, loss = 0.281550 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:13:05 INFO: 2022-08-03 15:13:05: step 20260/200000, loss = 0.112397 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:13:06 INFO: 2022-08-03 15:13:06: step 20280/200000, loss = 0.223804 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:13:07 INFO: 2022-08-03 15:13:07: step 20300/200000, loss = 0.208698 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:13:08 INFO: 2022-08-03 15:13:08: step 20320/200000, loss = 0.151233 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:13:09 INFO: 2022-08-03 15:13:09: step 20340/200000, loss = 0.146639 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:13:10 INFO: 2022-08-03 15:13:10: step 20360/200000, loss = 0.080276 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:13:11 INFO: 2022-08-03 15:13:11: step 20380/200000, loss = 0.193692 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:13:11 INFO: 2022-08-03 15:13:11: step 20400/200000, loss = 0.446750 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:13:12 INFO: 2022-08-03 15:13:12: step 20420/200000, loss = 0.387133 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:13:13 INFO: 2022-08-03 15:13:13: step 20440/200000, loss = 0.267943 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:13:14 INFO: 2022-08-03 15:13:14: step 20460/200000, loss = 0.706800 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:13:15 INFO: 2022-08-03 15:13:15: step 20480/200000, loss = 0.143242 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:13:16 INFO: 2022-08-03 15:13:16: step 20500/200000, loss = 0.191679 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:13:16 INFO: Evaluating on dev set...
2022-08-03 15:13:40 INFO: Score by entity:
Prec.	Rec.	F1
94.35	92.83	93.58
2022-08-03 15:13:40 INFO: step 20500: train_loss = 0.301599, dev_score = 0.9358
2022-08-03 15:13:40 INFO: 
2022-08-03 15:13:41 INFO: 2022-08-03 15:13:41: step 20520/200000, loss = 0.423619 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:13:42 INFO: 2022-08-03 15:13:42: step 20540/200000, loss = 0.313295 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:13:43 INFO: 2022-08-03 15:13:43: step 20560/200000, loss = 0.152507 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:13:44 INFO: 2022-08-03 15:13:44: step 20580/200000, loss = 0.321727 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:13:45 INFO: 2022-08-03 15:13:45: step 20600/200000, loss = 0.033055 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:13:46 INFO: 2022-08-03 15:13:46: step 20620/200000, loss = 0.492709 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:13:47 INFO: 2022-08-03 15:13:47: step 20640/200000, loss = 0.188502 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:13:47 INFO: 2022-08-03 15:13:47: step 20660/200000, loss = 0.255536 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:13:48 INFO: 2022-08-03 15:13:48: step 20680/200000, loss = 0.396396 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:13:49 INFO: 2022-08-03 15:13:49: step 20700/200000, loss = 0.259934 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:13:50 INFO: 2022-08-03 15:13:50: step 20720/200000, loss = 0.572108 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:13:51 INFO: 2022-08-03 15:13:51: step 20740/200000, loss = 0.209878 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:13:52 INFO: 2022-08-03 15:13:52: step 20760/200000, loss = 0.083751 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:13:53 INFO: 2022-08-03 15:13:53: step 20780/200000, loss = 0.146868 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:13:54 INFO: 2022-08-03 15:13:54: step 20800/200000, loss = 0.407855 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:13:55 INFO: 2022-08-03 15:13:55: step 20820/200000, loss = 0.325743 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:13:55 INFO: 2022-08-03 15:13:55: step 20840/200000, loss = 0.218391 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:13:56 INFO: 2022-08-03 15:13:56: step 20860/200000, loss = 0.157459 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:13:57 INFO: 2022-08-03 15:13:57: step 20880/200000, loss = 0.453472 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:13:58 INFO: 2022-08-03 15:13:58: step 20900/200000, loss = 0.345781 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:13:59 INFO: 2022-08-03 15:13:59: step 20920/200000, loss = 0.490481 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:00 INFO: 2022-08-03 15:14:00: step 20940/200000, loss = 0.309946 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:14:01 INFO: 2022-08-03 15:14:01: step 20960/200000, loss = 0.552596 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:14:02 INFO: 2022-08-03 15:14:02: step 20980/200000, loss = 0.104881 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:14:03 INFO: 2022-08-03 15:14:03: step 21000/200000, loss = 0.110999 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:14:03 INFO: Evaluating on dev set...
2022-08-03 15:14:27 INFO: Score by entity:
Prec.	Rec.	F1
94.30	92.94	93.62
2022-08-03 15:14:27 INFO: step 21000: train_loss = 0.286535, dev_score = 0.9362
2022-08-03 15:14:27 INFO: 
2022-08-03 15:14:28 INFO: 2022-08-03 15:14:28: step 21020/200000, loss = 0.223689 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:14:29 INFO: 2022-08-03 15:14:29: step 21040/200000, loss = 0.123137 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:14:30 INFO: 2022-08-03 15:14:30: step 21060/200000, loss = 0.123387 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:30 INFO: 2022-08-03 15:14:30: step 21080/200000, loss = 0.600380 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:14:31 INFO: 2022-08-03 15:14:31: step 21100/200000, loss = 0.036782 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:14:32 INFO: 2022-08-03 15:14:32: step 21120/200000, loss = 0.168141 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:33 INFO: 2022-08-03 15:14:33: step 21140/200000, loss = 0.217308 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:34 INFO: 2022-08-03 15:14:34: step 21160/200000, loss = 0.226652 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:14:35 INFO: 2022-08-03 15:14:35: step 21180/200000, loss = 0.397401 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:14:36 INFO: 2022-08-03 15:14:36: step 21200/200000, loss = 0.265072 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:14:37 INFO: 2022-08-03 15:14:37: step 21220/200000, loss = 0.059716 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:14:38 INFO: 2022-08-03 15:14:38: step 21240/200000, loss = 0.156853 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:14:38 INFO: 2022-08-03 15:14:38: step 21260/200000, loss = 0.234846 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:14:39 INFO: 2022-08-03 15:14:39: step 21280/200000, loss = 0.227475 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:14:40 INFO: 2022-08-03 15:14:40: step 21300/200000, loss = 0.343656 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:14:41 INFO: 2022-08-03 15:14:41: step 21320/200000, loss = 0.658795 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:14:42 INFO: 2022-08-03 15:14:42: step 21340/200000, loss = 0.458304 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:14:43 INFO: 2022-08-03 15:14:43: step 21360/200000, loss = 0.276993 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:44 INFO: 2022-08-03 15:14:44: step 21380/200000, loss = 0.352793 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:14:45 INFO: 2022-08-03 15:14:45: step 21400/200000, loss = 0.321467 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:14:46 INFO: 2022-08-03 15:14:46: step 21420/200000, loss = 0.272360 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:14:46 INFO: 2022-08-03 15:14:46: step 21440/200000, loss = 0.197659 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:14:47 INFO: 2022-08-03 15:14:47: step 21460/200000, loss = 0.145831 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:14:48 INFO: 2022-08-03 15:14:48: step 21480/200000, loss = 1.025235 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:14:49 INFO: 2022-08-03 15:14:49: step 21500/200000, loss = 0.417309 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:14:49 INFO: Evaluating on dev set...
2022-08-03 15:15:14 INFO: Score by entity:
Prec.	Rec.	F1
94.35	93.24	93.79
2022-08-03 15:15:14 INFO: step 21500: train_loss = 0.276474, dev_score = 0.9379
2022-08-03 15:15:15 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:15:15 INFO: New best model saved.
2022-08-03 15:15:15 INFO: 
2022-08-03 15:15:16 INFO: 2022-08-03 15:15:16: step 21520/200000, loss = 0.100841 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:15:17 INFO: 2022-08-03 15:15:17: step 21540/200000, loss = 0.656706 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:15:18 INFO: 2022-08-03 15:15:18: step 21560/200000, loss = 0.263728 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:15:19 INFO: 2022-08-03 15:15:19: step 21580/200000, loss = 0.158917 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:15:19 INFO: 2022-08-03 15:15:19: step 21600/200000, loss = 0.582070 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:15:20 INFO: 2022-08-03 15:15:20: step 21620/200000, loss = 0.033904 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:15:21 INFO: 2022-08-03 15:15:21: step 21640/200000, loss = 0.066791 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:15:22 INFO: 2022-08-03 15:15:22: step 21660/200000, loss = 0.512914 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:15:23 INFO: 2022-08-03 15:15:23: step 21680/200000, loss = 0.190232 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:15:24 INFO: 2022-08-03 15:15:24: step 21700/200000, loss = 0.299451 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:15:25 INFO: 2022-08-03 15:15:25: step 21720/200000, loss = 0.623552 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:15:26 INFO: 2022-08-03 15:15:26: step 21740/200000, loss = 0.163570 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:15:26 INFO: 2022-08-03 15:15:26: step 21760/200000, loss = 0.181350 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:15:27 INFO: 2022-08-03 15:15:27: step 21780/200000, loss = 0.103681 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:15:28 INFO: 2022-08-03 15:15:28: step 21800/200000, loss = 0.331795 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:15:29 INFO: 2022-08-03 15:15:29: step 21820/200000, loss = 0.136872 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:15:30 INFO: 2022-08-03 15:15:30: step 21840/200000, loss = 0.165523 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:15:31 INFO: 2022-08-03 15:15:31: step 21860/200000, loss = 0.202262 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:15:32 INFO: 2022-08-03 15:15:32: step 21880/200000, loss = 0.060058 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:15:33 INFO: 2022-08-03 15:15:33: step 21900/200000, loss = 0.419007 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:15:34 INFO: 2022-08-03 15:15:34: step 21920/200000, loss = 0.289359 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:15:35 INFO: 2022-08-03 15:15:35: step 21940/200000, loss = 0.330175 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:15:35 INFO: 2022-08-03 15:15:35: step 21960/200000, loss = 0.192199 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:15:36 INFO: 2022-08-03 15:15:36: step 21980/200000, loss = 0.402969 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:15:37 INFO: 2022-08-03 15:15:37: step 22000/200000, loss = 0.431010 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:15:37 INFO: Evaluating on dev set...
2022-08-03 15:16:02 INFO: Score by entity:
Prec.	Rec.	F1
94.36	93.28	93.82
2022-08-03 15:16:02 INFO: step 22000: train_loss = 0.294797, dev_score = 0.9382
2022-08-03 15:16:03 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:16:03 INFO: New best model saved.
2022-08-03 15:16:03 INFO: 
2022-08-03 15:16:04 INFO: 2022-08-03 15:16:04: step 22020/200000, loss = 0.394583 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:16:05 INFO: 2022-08-03 15:16:05: step 22040/200000, loss = 0.461642 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:06 INFO: 2022-08-03 15:16:06: step 22060/200000, loss = 0.220863 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:16:07 INFO: 2022-08-03 15:16:07: step 22080/200000, loss = 0.559201 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:08 INFO: 2022-08-03 15:16:08: step 22100/200000, loss = 0.522902 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:08 INFO: 2022-08-03 15:16:08: step 22120/200000, loss = 0.047963 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:09 INFO: 2022-08-03 15:16:09: step 22140/200000, loss = 0.410876 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:10 INFO: 2022-08-03 15:16:10: step 22160/200000, loss = 0.312328 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:11 INFO: 2022-08-03 15:16:11: step 22180/200000, loss = 0.487626 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:16:12 INFO: 2022-08-03 15:16:12: step 22200/200000, loss = 0.204371 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:13 INFO: 2022-08-03 15:16:13: step 22220/200000, loss = 0.215863 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:14 INFO: 2022-08-03 15:16:14: step 22240/200000, loss = 0.513429 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:16:15 INFO: 2022-08-03 15:16:15: step 22260/200000, loss = 0.205661 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:16 INFO: 2022-08-03 15:16:16: step 22280/200000, loss = 0.277830 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:16 INFO: 2022-08-03 15:16:16: step 22300/200000, loss = 0.493574 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:16:17 INFO: 2022-08-03 15:16:17: step 22320/200000, loss = 0.339147 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:16:18 INFO: 2022-08-03 15:16:18: step 22340/200000, loss = 0.281871 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:19 INFO: 2022-08-03 15:16:19: step 22360/200000, loss = 0.092476 (0.055 sec/batch), lr: 0.025000
2022-08-03 15:16:20 INFO: 2022-08-03 15:16:20: step 22380/200000, loss = 0.344470 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:21 INFO: 2022-08-03 15:16:21: step 22400/200000, loss = 0.632177 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:22 INFO: 2022-08-03 15:16:22: step 22420/200000, loss = 0.257961 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:16:23 INFO: 2022-08-03 15:16:23: step 22440/200000, loss = 0.366630 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:16:24 INFO: 2022-08-03 15:16:24: step 22460/200000, loss = 0.267888 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:24 INFO: 2022-08-03 15:16:24: step 22480/200000, loss = 0.082367 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:16:25 INFO: 2022-08-03 15:16:25: step 22500/200000, loss = 0.453369 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:25 INFO: Evaluating on dev set...
2022-08-03 15:16:50 INFO: Score by entity:
Prec.	Rec.	F1
94.35	93.18	93.76
2022-08-03 15:16:50 INFO: step 22500: train_loss = 0.287447, dev_score = 0.9376
2022-08-03 15:16:50 INFO: 
2022-08-03 15:16:51 INFO: 2022-08-03 15:16:51: step 22520/200000, loss = 0.590203 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:16:52 INFO: 2022-08-03 15:16:52: step 22540/200000, loss = 0.122706 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:16:53 INFO: 2022-08-03 15:16:53: step 22560/200000, loss = 0.018509 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:16:54 INFO: 2022-08-03 15:16:54: step 22580/200000, loss = 0.184643 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:16:54 INFO: 2022-08-03 15:16:54: step 22600/200000, loss = 0.243777 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:16:55 INFO: 2022-08-03 15:16:55: step 22620/200000, loss = 0.474064 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:16:56 INFO: 2022-08-03 15:16:56: step 22640/200000, loss = 0.233755 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:57 INFO: 2022-08-03 15:16:57: step 22660/200000, loss = 0.404389 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:16:58 INFO: 2022-08-03 15:16:58: step 22680/200000, loss = 0.121933 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:16:59 INFO: 2022-08-03 15:16:59: step 22700/200000, loss = 0.249511 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:17:00 INFO: 2022-08-03 15:17:00: step 22720/200000, loss = 0.281926 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:01 INFO: 2022-08-03 15:17:01: step 22740/200000, loss = 0.100577 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:17:02 INFO: 2022-08-03 15:17:02: step 22760/200000, loss = 0.315223 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:17:02 INFO: 2022-08-03 15:17:02: step 22780/200000, loss = 0.363754 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:03 INFO: 2022-08-03 15:17:03: step 22800/200000, loss = 0.560824 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:17:04 INFO: 2022-08-03 15:17:04: step 22820/200000, loss = 0.291816 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:17:05 INFO: 2022-08-03 15:17:05: step 22840/200000, loss = 0.309501 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:06 INFO: 2022-08-03 15:17:06: step 22860/200000, loss = 0.028701 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:17:07 INFO: 2022-08-03 15:17:07: step 22880/200000, loss = 0.291860 (0.048 sec/batch), lr: 0.025000
2022-08-03 15:17:08 INFO: 2022-08-03 15:17:08: step 22900/200000, loss = 0.345143 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:09 INFO: 2022-08-03 15:17:09: step 22920/200000, loss = 0.139256 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:17:10 INFO: 2022-08-03 15:17:10: step 22940/200000, loss = 0.122324 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:17:10 INFO: 2022-08-03 15:17:10: step 22960/200000, loss = 0.062787 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:17:11 INFO: 2022-08-03 15:17:11: step 22980/200000, loss = 0.114501 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:17:12 INFO: 2022-08-03 15:17:12: step 23000/200000, loss = 0.326502 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:12 INFO: Evaluating on dev set...
2022-08-03 15:17:37 INFO: Score by entity:
Prec.	Rec.	F1
94.34	93.17	93.75
2022-08-03 15:17:37 INFO: step 23000: train_loss = 0.272551, dev_score = 0.9375
2022-08-03 15:17:37 INFO: 
2022-08-03 15:17:38 INFO: 2022-08-03 15:17:38: step 23020/200000, loss = 0.092974 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:39 INFO: 2022-08-03 15:17:39: step 23040/200000, loss = 0.085158 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:17:39 INFO: 2022-08-03 15:17:39: step 23060/200000, loss = 0.373134 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:17:40 INFO: 2022-08-03 15:17:40: step 23080/200000, loss = 0.205354 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:41 INFO: 2022-08-03 15:17:41: step 23100/200000, loss = 0.699857 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:17:42 INFO: 2022-08-03 15:17:42: step 23120/200000, loss = 0.076462 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:17:43 INFO: 2022-08-03 15:17:43: step 23140/200000, loss = 0.122134 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:44 INFO: 2022-08-03 15:17:44: step 23160/200000, loss = 0.499105 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:45 INFO: 2022-08-03 15:17:45: step 23180/200000, loss = 0.161483 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:46 INFO: 2022-08-03 15:17:46: step 23200/200000, loss = 0.251854 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:17:47 INFO: 2022-08-03 15:17:47: step 23220/200000, loss = 0.307897 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:47 INFO: 2022-08-03 15:17:47: step 23240/200000, loss = 0.104667 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:17:48 INFO: 2022-08-03 15:17:48: step 23260/200000, loss = 0.176098 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:17:49 INFO: 2022-08-03 15:17:49: step 23280/200000, loss = 0.237661 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:17:50 INFO: 2022-08-03 15:17:50: step 23300/200000, loss = 0.070041 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:17:51 INFO: 2022-08-03 15:17:51: step 23320/200000, loss = 0.476447 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:17:52 INFO: 2022-08-03 15:17:52: step 23340/200000, loss = 0.309088 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:17:53 INFO: 2022-08-03 15:17:53: step 23360/200000, loss = 0.268073 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:17:54 INFO: 2022-08-03 15:17:54: step 23380/200000, loss = 0.639308 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:17:55 INFO: 2022-08-03 15:17:55: step 23400/200000, loss = 0.330635 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:55 INFO: 2022-08-03 15:17:55: step 23420/200000, loss = 0.274235 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:17:56 INFO: 2022-08-03 15:17:56: step 23440/200000, loss = 0.376561 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:17:57 INFO: 2022-08-03 15:17:57: step 23460/200000, loss = 0.513441 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:58 INFO: 2022-08-03 15:17:58: step 23480/200000, loss = 0.592354 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:59 INFO: 2022-08-03 15:17:59: step 23500/200000, loss = 0.192504 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:17:59 INFO: Evaluating on dev set...
2022-08-03 15:18:23 INFO: Score by entity:
Prec.	Rec.	F1
94.33	93.32	93.83
2022-08-03 15:18:23 INFO: step 23500: train_loss = 0.278919, dev_score = 0.9383
2022-08-03 15:18:25 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:18:25 INFO: New best model saved.
2022-08-03 15:18:25 INFO: 
2022-08-03 15:18:26 INFO: 2022-08-03 15:18:26: step 23520/200000, loss = 0.069870 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:18:27 INFO: 2022-08-03 15:18:27: step 23540/200000, loss = 0.255604 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:28 INFO: 2022-08-03 15:18:28: step 23560/200000, loss = 0.112899 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:18:28 INFO: 2022-08-03 15:18:28: step 23580/200000, loss = 0.412378 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:29 INFO: 2022-08-03 15:18:29: step 23600/200000, loss = 0.161696 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:30 INFO: 2022-08-03 15:18:30: step 23620/200000, loss = 0.139806 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:18:31 INFO: 2022-08-03 15:18:31: step 23640/200000, loss = 0.403454 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:32 INFO: 2022-08-03 15:18:32: step 23660/200000, loss = 0.434162 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:18:33 INFO: 2022-08-03 15:18:33: step 23680/200000, loss = 0.189842 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:18:34 INFO: 2022-08-03 15:18:34: step 23700/200000, loss = 0.256232 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:18:35 INFO: 2022-08-03 15:18:35: step 23720/200000, loss = 0.157211 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:18:36 INFO: 2022-08-03 15:18:36: step 23740/200000, loss = 0.242064 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:36 INFO: 2022-08-03 15:18:36: step 23760/200000, loss = 0.340936 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:18:37 INFO: 2022-08-03 15:18:37: step 23780/200000, loss = 0.286222 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:38 INFO: 2022-08-03 15:18:38: step 23800/200000, loss = 0.365028 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:39 INFO: 2022-08-03 15:18:39: step 23820/200000, loss = 0.034285 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:18:40 INFO: 2022-08-03 15:18:40: step 23840/200000, loss = 0.076237 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:18:41 INFO: 2022-08-03 15:18:41: step 23860/200000, loss = 0.248471 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:18:42 INFO: 2022-08-03 15:18:42: step 23880/200000, loss = 0.062467 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:18:43 INFO: 2022-08-03 15:18:43: step 23900/200000, loss = 0.435429 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:18:44 INFO: 2022-08-03 15:18:44: step 23920/200000, loss = 0.301162 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:18:44 INFO: 2022-08-03 15:18:44: step 23940/200000, loss = 0.314369 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:18:45 INFO: 2022-08-03 15:18:45: step 23960/200000, loss = 0.271050 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:18:46 INFO: 2022-08-03 15:18:46: step 23980/200000, loss = 1.547052 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:18:47 INFO: 2022-08-03 15:18:47: step 24000/200000, loss = 0.204840 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:18:47 INFO: Evaluating on dev set...
2022-08-03 15:19:12 INFO: Score by entity:
Prec.	Rec.	F1
94.16	93.21	93.69
2022-08-03 15:19:12 INFO: step 24000: train_loss = 0.268367, dev_score = 0.9369
2022-08-03 15:19:12 INFO: 
2022-08-03 15:19:12 INFO: 2022-08-03 15:19:12: step 24020/200000, loss = 0.212421 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:19:13 INFO: 2022-08-03 15:19:13: step 24040/200000, loss = 0.467858 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:19:14 INFO: 2022-08-03 15:19:14: step 24060/200000, loss = 0.044763 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:19:15 INFO: 2022-08-03 15:19:15: step 24080/200000, loss = 0.242768 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:19:16 INFO: 2022-08-03 15:19:16: step 24100/200000, loss = 0.209214 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:19:17 INFO: 2022-08-03 15:19:17: step 24120/200000, loss = 0.561931 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:19:18 INFO: 2022-08-03 15:19:18: step 24140/200000, loss = 0.222247 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:19:19 INFO: 2022-08-03 15:19:19: step 24160/200000, loss = 0.333739 (0.052 sec/batch), lr: 0.025000
2022-08-03 15:19:20 INFO: 2022-08-03 15:19:20: step 24180/200000, loss = 0.105394 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:19:21 INFO: 2022-08-03 15:19:21: step 24200/200000, loss = 0.064729 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:19:21 INFO: 2022-08-03 15:19:21: step 24220/200000, loss = 0.318356 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:19:22 INFO: 2022-08-03 15:19:22: step 24240/200000, loss = 0.076628 (0.047 sec/batch), lr: 0.025000
2022-08-03 15:19:23 INFO: 2022-08-03 15:19:23: step 24260/200000, loss = 0.208874 (0.049 sec/batch), lr: 0.025000
2022-08-03 15:19:24 INFO: 2022-08-03 15:19:24: step 24280/200000, loss = 0.042517 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:19:25 INFO: 2022-08-03 15:19:25: step 24300/200000, loss = 0.132676 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:19:26 INFO: 2022-08-03 15:19:26: step 24320/200000, loss = 0.456963 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:19:27 INFO: 2022-08-03 15:19:27: step 24340/200000, loss = 0.451338 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:19:28 INFO: 2022-08-03 15:19:28: step 24360/200000, loss = 0.295167 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:19:29 INFO: 2022-08-03 15:19:29: step 24380/200000, loss = 0.618558 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:19:29 INFO: 2022-08-03 15:19:29: step 24400/200000, loss = 0.205544 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:19:30 INFO: 2022-08-03 15:19:30: step 24420/200000, loss = 0.498014 (0.051 sec/batch), lr: 0.025000
2022-08-03 15:19:31 INFO: 2022-08-03 15:19:31: step 24440/200000, loss = 0.143164 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:19:32 INFO: 2022-08-03 15:19:32: step 24460/200000, loss = 0.431000 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:19:33 INFO: 2022-08-03 15:19:33: step 24480/200000, loss = 0.081197 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:19:34 INFO: 2022-08-03 15:19:34: step 24500/200000, loss = 0.118226 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:19:34 INFO: Evaluating on dev set...
2022-08-03 15:19:57 INFO: Score by entity:
Prec.	Rec.	F1
94.42	93.27	93.84
2022-08-03 15:19:57 INFO: step 24500: train_loss = 0.283138, dev_score = 0.9384
2022-08-03 15:19:59 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:19:59 INFO: New best model saved.
2022-08-03 15:19:59 INFO: 
2022-08-03 15:20:00 INFO: 2022-08-03 15:20:00: step 24520/200000, loss = 0.493393 (0.028 sec/batch), lr: 0.025000
2022-08-03 15:20:01 INFO: 2022-08-03 15:20:01: step 24540/200000, loss = 0.307342 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:20:01 INFO: 2022-08-03 15:20:01: step 24560/200000, loss = 0.465881 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:20:02 INFO: 2022-08-03 15:20:02: step 24580/200000, loss = 0.675794 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:20:03 INFO: 2022-08-03 15:20:03: step 24600/200000, loss = 0.357911 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:20:04 INFO: 2022-08-03 15:20:04: step 24620/200000, loss = 0.064773 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:20:05 INFO: 2022-08-03 15:20:05: step 24640/200000, loss = 0.154792 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:20:05 INFO: 2022-08-03 15:20:05: step 24660/200000, loss = 0.349705 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:20:06 INFO: 2022-08-03 15:20:06: step 24680/200000, loss = 0.397828 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:20:07 INFO: 2022-08-03 15:20:07: step 24700/200000, loss = 0.201734 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:20:08 INFO: 2022-08-03 15:20:08: step 24720/200000, loss = 0.251335 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:20:09 INFO: 2022-08-03 15:20:09: step 24740/200000, loss = 0.057557 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:20:10 INFO: 2022-08-03 15:20:10: step 24760/200000, loss = 0.073512 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:20:10 INFO: 2022-08-03 15:20:10: step 24780/200000, loss = 0.347017 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:20:11 INFO: 2022-08-03 15:20:11: step 24800/200000, loss = 0.490855 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:20:12 INFO: 2022-08-03 15:20:12: step 24820/200000, loss = 0.357553 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:20:13 INFO: 2022-08-03 15:20:13: step 24840/200000, loss = 0.054211 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:20:14 INFO: 2022-08-03 15:20:14: step 24860/200000, loss = 0.067213 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:20:14 INFO: 2022-08-03 15:20:14: step 24880/200000, loss = 0.229260 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:20:15 INFO: 2022-08-03 15:20:15: step 24900/200000, loss = 0.168913 (0.029 sec/batch), lr: 0.025000
2022-08-03 15:20:16 INFO: 2022-08-03 15:20:16: step 24920/200000, loss = 0.448977 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:20:17 INFO: 2022-08-03 15:20:17: step 24940/200000, loss = 0.499631 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:20:18 INFO: 2022-08-03 15:20:18: step 24960/200000, loss = 0.042607 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:20:19 INFO: 2022-08-03 15:20:19: step 24980/200000, loss = 0.248413 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:20:19 INFO: 2022-08-03 15:20:19: step 25000/200000, loss = 0.102104 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:20:19 INFO: Evaluating on dev set...
2022-08-03 15:20:42 INFO: Score by entity:
Prec.	Rec.	F1
94.34	93.27	93.80
2022-08-03 15:20:42 INFO: step 25000: train_loss = 0.271961, dev_score = 0.9380
2022-08-03 15:20:42 INFO: 
2022-08-03 15:20:43 INFO: 2022-08-03 15:20:43: step 25020/200000, loss = 0.275697 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:20:44 INFO: 2022-08-03 15:20:44: step 25040/200000, loss = 0.206896 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:20:45 INFO: 2022-08-03 15:20:45: step 25060/200000, loss = 0.317157 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:20:46 INFO: 2022-08-03 15:20:46: step 25080/200000, loss = 0.285437 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:20:47 INFO: 2022-08-03 15:20:47: step 25100/200000, loss = 0.157354 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:20:47 INFO: 2022-08-03 15:20:47: step 25120/200000, loss = 0.175488 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:20:48 INFO: 2022-08-03 15:20:48: step 25140/200000, loss = 0.391012 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:20:49 INFO: 2022-08-03 15:20:49: step 25160/200000, loss = 0.087648 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:20:50 INFO: 2022-08-03 15:20:50: step 25180/200000, loss = 0.553489 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:20:51 INFO: 2022-08-03 15:20:51: step 25200/200000, loss = 0.251605 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:20:52 INFO: 2022-08-03 15:20:52: step 25220/200000, loss = 0.340358 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:20:53 INFO: 2022-08-03 15:20:53: step 25240/200000, loss = 0.245080 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:20:54 INFO: 2022-08-03 15:20:54: step 25260/200000, loss = 0.136436 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:20:54 INFO: 2022-08-03 15:20:54: step 25280/200000, loss = 0.347947 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:20:55 INFO: 2022-08-03 15:20:55: step 25300/200000, loss = 0.087523 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:20:56 INFO: 2022-08-03 15:20:56: step 25320/200000, loss = 0.095184 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:20:57 INFO: 2022-08-03 15:20:57: step 25340/200000, loss = 0.085533 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:20:58 INFO: 2022-08-03 15:20:58: step 25360/200000, loss = 0.487767 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:20:59 INFO: 2022-08-03 15:20:59: step 25380/200000, loss = 0.962461 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:21:00 INFO: 2022-08-03 15:21:00: step 25400/200000, loss = 0.406663 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:21:01 INFO: 2022-08-03 15:21:01: step 25420/200000, loss = 0.554816 (0.031 sec/batch), lr: 0.025000
2022-08-03 15:21:02 INFO: 2022-08-03 15:21:02: step 25440/200000, loss = 0.068243 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:03 INFO: 2022-08-03 15:21:03: step 25460/200000, loss = 0.099268 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:03 INFO: 2022-08-03 15:21:03: step 25480/200000, loss = 0.304080 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:21:04 INFO: 2022-08-03 15:21:04: step 25500/200000, loss = 0.227763 (0.044 sec/batch), lr: 0.025000
2022-08-03 15:21:04 INFO: Evaluating on dev set...
2022-08-03 15:21:29 INFO: Score by entity:
Prec.	Rec.	F1
94.34	93.30	93.82
2022-08-03 15:21:29 INFO: step 25500: train_loss = 0.278037, dev_score = 0.9382
2022-08-03 15:21:29 INFO: 
2022-08-03 15:21:30 INFO: 2022-08-03 15:21:30: step 25520/200000, loss = 0.229968 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:21:30 INFO: 2022-08-03 15:21:30: step 25540/200000, loss = 0.241948 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:21:31 INFO: 2022-08-03 15:21:31: step 25560/200000, loss = 0.072273 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:21:32 INFO: 2022-08-03 15:21:32: step 25580/200000, loss = 0.191950 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:21:33 INFO: 2022-08-03 15:21:33: step 25600/200000, loss = 0.121150 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:21:34 INFO: 2022-08-03 15:21:34: step 25620/200000, loss = 0.278358 (0.048 sec/batch), lr: 0.025000
2022-08-03 15:21:35 INFO: 2022-08-03 15:21:35: step 25640/200000, loss = 0.117424 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:21:36 INFO: 2022-08-03 15:21:36: step 25660/200000, loss = 0.185447 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:21:37 INFO: 2022-08-03 15:21:37: step 25680/200000, loss = 0.363557 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:21:38 INFO: 2022-08-03 15:21:38: step 25700/200000, loss = 0.471570 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:38 INFO: 2022-08-03 15:21:38: step 25720/200000, loss = 0.804247 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:21:39 INFO: 2022-08-03 15:21:39: step 25740/200000, loss = 0.087060 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:21:40 INFO: 2022-08-03 15:21:40: step 25760/200000, loss = 0.160589 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:21:41 INFO: 2022-08-03 15:21:41: step 25780/200000, loss = 0.390668 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:21:42 INFO: 2022-08-03 15:21:42: step 25800/200000, loss = 0.550687 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:43 INFO: 2022-08-03 15:21:43: step 25820/200000, loss = 0.212776 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:21:44 INFO: 2022-08-03 15:21:44: step 25840/200000, loss = 0.289937 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:45 INFO: 2022-08-03 15:21:45: step 25860/200000, loss = 0.224355 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:21:46 INFO: 2022-08-03 15:21:46: step 25880/200000, loss = 0.071602 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:21:46 INFO: 2022-08-03 15:21:46: step 25900/200000, loss = 0.196821 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:21:47 INFO: 2022-08-03 15:21:47: step 25920/200000, loss = 0.679884 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:21:48 INFO: 2022-08-03 15:21:48: step 25940/200000, loss = 0.084850 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:21:49 INFO: 2022-08-03 15:21:49: step 25960/200000, loss = 0.313922 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:21:50 INFO: 2022-08-03 15:21:50: step 25980/200000, loss = 0.024413 (0.049 sec/batch), lr: 0.025000
2022-08-03 15:21:51 INFO: 2022-08-03 15:21:51: step 26000/200000, loss = 0.121442 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:21:51 INFO: Evaluating on dev set...
2022-08-03 15:22:15 INFO: Score by entity:
Prec.	Rec.	F1
94.41	93.25	93.83
2022-08-03 15:22:15 INFO: step 26000: train_loss = 0.270045, dev_score = 0.9383
2022-08-03 15:22:15 INFO: 
2022-08-03 15:22:16 INFO: 2022-08-03 15:22:16: step 26020/200000, loss = 0.356920 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:22:17 INFO: 2022-08-03 15:22:17: step 26040/200000, loss = 0.133458 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:22:18 INFO: 2022-08-03 15:22:18: step 26060/200000, loss = 0.556316 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:22:19 INFO: 2022-08-03 15:22:19: step 26080/200000, loss = 0.312799 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:22:20 INFO: 2022-08-03 15:22:20: step 26100/200000, loss = 0.106946 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:22:21 INFO: 2022-08-03 15:22:21: step 26120/200000, loss = 0.077521 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:22:21 INFO: 2022-08-03 15:22:21: step 26140/200000, loss = 0.263999 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:22:22 INFO: 2022-08-03 15:22:22: step 26160/200000, loss = 0.283019 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:22:23 INFO: 2022-08-03 15:22:23: step 26180/200000, loss = 0.421645 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:22:24 INFO: 2022-08-03 15:22:24: step 26200/200000, loss = 0.563594 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:22:25 INFO: 2022-08-03 15:22:25: step 26220/200000, loss = 0.066746 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:26 INFO: 2022-08-03 15:22:26: step 26240/200000, loss = 0.465818 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:22:27 INFO: 2022-08-03 15:22:27: step 26260/200000, loss = 0.329059 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:22:28 INFO: 2022-08-03 15:22:28: step 26280/200000, loss = 0.213888 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:22:29 INFO: 2022-08-03 15:22:29: step 26300/200000, loss = 0.230267 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:22:29 INFO: 2022-08-03 15:22:29: step 26320/200000, loss = 0.296864 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:22:30 INFO: 2022-08-03 15:22:30: step 26340/200000, loss = 0.083072 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:31 INFO: 2022-08-03 15:22:31: step 26360/200000, loss = 0.112670 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:32 INFO: 2022-08-03 15:22:32: step 26380/200000, loss = 0.437867 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:22:33 INFO: 2022-08-03 15:22:33: step 26400/200000, loss = 0.395706 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:34 INFO: 2022-08-03 15:22:34: step 26420/200000, loss = 0.127475 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:35 INFO: 2022-08-03 15:22:35: step 26440/200000, loss = 0.249561 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:22:36 INFO: 2022-08-03 15:22:36: step 26460/200000, loss = 0.113141 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:22:37 INFO: 2022-08-03 15:22:37: step 26480/200000, loss = 0.104785 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:22:37 INFO: 2022-08-03 15:22:37: step 26500/200000, loss = 0.402537 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:22:37 INFO: Evaluating on dev set...
2022-08-03 15:23:02 INFO: Score by entity:
Prec.	Rec.	F1
94.38	93.34	93.85
2022-08-03 15:23:02 INFO: step 26500: train_loss = 0.266467, dev_score = 0.9385
2022-08-03 15:23:03 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:23:03 INFO: New best model saved.
2022-08-03 15:23:03 INFO: 
2022-08-03 15:23:04 INFO: 2022-08-03 15:23:04: step 26520/200000, loss = 0.125147 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:23:05 INFO: 2022-08-03 15:23:05: step 26540/200000, loss = 0.157646 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:23:06 INFO: 2022-08-03 15:23:06: step 26560/200000, loss = 0.267371 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:23:07 INFO: 2022-08-03 15:23:07: step 26580/200000, loss = 0.132586 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:23:08 INFO: 2022-08-03 15:23:08: step 26600/200000, loss = 0.164740 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:23:09 INFO: 2022-08-03 15:23:09: step 26620/200000, loss = 0.182379 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:23:09 INFO: 2022-08-03 15:23:09: step 26640/200000, loss = 0.383583 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:23:10 INFO: 2022-08-03 15:23:10: step 26660/200000, loss = 0.072278 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:23:11 INFO: 2022-08-03 15:23:11: step 26680/200000, loss = 0.222572 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:23:12 INFO: 2022-08-03 15:23:12: step 26700/200000, loss = 0.393523 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:23:13 INFO: 2022-08-03 15:23:13: step 26720/200000, loss = 0.079423 (0.050 sec/batch), lr: 0.025000
2022-08-03 15:23:14 INFO: 2022-08-03 15:23:14: step 26740/200000, loss = 0.158920 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:23:15 INFO: 2022-08-03 15:23:15: step 26760/200000, loss = 0.303212 (0.029 sec/batch), lr: 0.025000
2022-08-03 15:23:16 INFO: 2022-08-03 15:23:16: step 26780/200000, loss = 0.220242 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:23:17 INFO: 2022-08-03 15:23:17: step 26800/200000, loss = 0.191060 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:23:17 INFO: 2022-08-03 15:23:17: step 26820/200000, loss = 0.046159 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:23:18 INFO: 2022-08-03 15:23:18: step 26840/200000, loss = 0.265846 (0.047 sec/batch), lr: 0.025000
2022-08-03 15:23:19 INFO: 2022-08-03 15:23:19: step 26860/200000, loss = 0.715920 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:23:20 INFO: 2022-08-03 15:23:20: step 26880/200000, loss = 0.052027 (0.072 sec/batch), lr: 0.025000
2022-08-03 15:23:21 INFO: 2022-08-03 15:23:21: step 26900/200000, loss = 0.176159 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:23:22 INFO: 2022-08-03 15:23:22: step 26920/200000, loss = 0.104401 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:23:23 INFO: 2022-08-03 15:23:23: step 26940/200000, loss = 0.099046 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:23:24 INFO: 2022-08-03 15:23:24: step 26960/200000, loss = 0.830184 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:23:24 INFO: 2022-08-03 15:23:24: step 26980/200000, loss = 0.269297 (0.050 sec/batch), lr: 0.025000
2022-08-03 15:23:25 INFO: 2022-08-03 15:23:25: step 27000/200000, loss = 0.085496 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:23:25 INFO: Evaluating on dev set...
2022-08-03 15:23:50 INFO: Score by entity:
Prec.	Rec.	F1
94.59	93.41	93.99
2022-08-03 15:23:50 INFO: step 27000: train_loss = 0.250533, dev_score = 0.9399
2022-08-03 15:23:51 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:23:51 INFO: New best model saved.
2022-08-03 15:23:51 INFO: 
2022-08-03 15:23:52 INFO: 2022-08-03 15:23:52: step 27020/200000, loss = 0.305170 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:23:53 INFO: 2022-08-03 15:23:53: step 27040/200000, loss = 0.148453 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:23:54 INFO: 2022-08-03 15:23:54: step 27060/200000, loss = 0.107494 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:23:55 INFO: 2022-08-03 15:23:55: step 27080/200000, loss = 0.327463 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:23:56 INFO: 2022-08-03 15:23:56: step 27100/200000, loss = 1.814213 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:23:56 INFO: 2022-08-03 15:23:56: step 27120/200000, loss = 0.057744 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:23:57 INFO: 2022-08-03 15:23:57: step 27140/200000, loss = 0.328849 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:23:58 INFO: 2022-08-03 15:23:58: step 27160/200000, loss = 0.095021 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:23:59 INFO: 2022-08-03 15:23:59: step 27180/200000, loss = 0.581950 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:24:00 INFO: 2022-08-03 15:24:00: step 27200/200000, loss = 0.410391 (0.049 sec/batch), lr: 0.025000
2022-08-03 15:24:01 INFO: 2022-08-03 15:24:01: step 27220/200000, loss = 0.422515 (0.050 sec/batch), lr: 0.025000
2022-08-03 15:24:02 INFO: 2022-08-03 15:24:02: step 27240/200000, loss = 0.082649 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:03 INFO: 2022-08-03 15:24:03: step 27260/200000, loss = 0.944453 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:24:04 INFO: 2022-08-03 15:24:04: step 27280/200000, loss = 0.525921 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:04 INFO: 2022-08-03 15:24:04: step 27300/200000, loss = 0.060428 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:05 INFO: 2022-08-03 15:24:05: step 27320/200000, loss = 0.300460 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:24:06 INFO: 2022-08-03 15:24:06: step 27340/200000, loss = 0.344766 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:07 INFO: 2022-08-03 15:24:07: step 27360/200000, loss = 0.078165 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:24:08 INFO: 2022-08-03 15:24:08: step 27380/200000, loss = 0.475511 (0.054 sec/batch), lr: 0.025000
2022-08-03 15:24:09 INFO: 2022-08-03 15:24:09: step 27400/200000, loss = 0.510392 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:10 INFO: 2022-08-03 15:24:10: step 27420/200000, loss = 0.290717 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:24:11 INFO: 2022-08-03 15:24:11: step 27440/200000, loss = 0.363581 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:24:12 INFO: 2022-08-03 15:24:12: step 27460/200000, loss = 0.095244 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:12 INFO: 2022-08-03 15:24:12: step 27480/200000, loss = 0.295669 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:13 INFO: 2022-08-03 15:24:13: step 27500/200000, loss = 0.112613 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:24:13 INFO: Evaluating on dev set...
2022-08-03 15:24:38 INFO: Score by entity:
Prec.	Rec.	F1
94.35	93.36	93.85
2022-08-03 15:24:38 INFO: step 27500: train_loss = 0.262153, dev_score = 0.9385
2022-08-03 15:24:38 INFO: 
2022-08-03 15:24:39 INFO: 2022-08-03 15:24:39: step 27520/200000, loss = 0.385593 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:24:40 INFO: 2022-08-03 15:24:40: step 27540/200000, loss = 0.384866 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:24:40 INFO: 2022-08-03 15:24:40: step 27560/200000, loss = 0.091551 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:24:41 INFO: 2022-08-03 15:24:41: step 27580/200000, loss = 0.282223 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:42 INFO: 2022-08-03 15:24:42: step 27600/200000, loss = 0.110549 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:24:43 INFO: 2022-08-03 15:24:43: step 27620/200000, loss = 0.338562 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:44 INFO: 2022-08-03 15:24:44: step 27640/200000, loss = 0.546698 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:24:45 INFO: 2022-08-03 15:24:45: step 27660/200000, loss = 0.426564 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:24:46 INFO: 2022-08-03 15:24:46: step 27680/200000, loss = 0.122463 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:47 INFO: 2022-08-03 15:24:47: step 27700/200000, loss = 0.594167 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:47 INFO: 2022-08-03 15:24:47: step 27720/200000, loss = 0.109432 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:24:48 INFO: 2022-08-03 15:24:48: step 27740/200000, loss = 0.270238 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:24:49 INFO: 2022-08-03 15:24:49: step 27760/200000, loss = 0.281353 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:24:50 INFO: 2022-08-03 15:24:50: step 27780/200000, loss = 0.416554 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:24:51 INFO: 2022-08-03 15:24:51: step 27800/200000, loss = 0.403121 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:24:52 INFO: 2022-08-03 15:24:52: step 27820/200000, loss = 0.375969 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:24:53 INFO: 2022-08-03 15:24:53: step 27840/200000, loss = 0.178719 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:24:54 INFO: 2022-08-03 15:24:54: step 27860/200000, loss = 0.208028 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:24:55 INFO: 2022-08-03 15:24:55: step 27880/200000, loss = 0.158946 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:24:56 INFO: 2022-08-03 15:24:56: step 27900/200000, loss = 0.081908 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:24:56 INFO: 2022-08-03 15:24:56: step 27920/200000, loss = 0.456797 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:24:57 INFO: 2022-08-03 15:24:57: step 27940/200000, loss = 0.330777 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:24:58 INFO: 2022-08-03 15:24:58: step 27960/200000, loss = 0.077918 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:24:59 INFO: 2022-08-03 15:24:59: step 27980/200000, loss = 0.270199 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:25:00 INFO: 2022-08-03 15:25:00: step 28000/200000, loss = 0.222397 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:25:00 INFO: Evaluating on dev set...
2022-08-03 15:25:24 INFO: Score by entity:
Prec.	Rec.	F1
94.50	93.23	93.86
2022-08-03 15:25:24 INFO: step 28000: train_loss = 0.265127, dev_score = 0.9386
2022-08-03 15:25:24 INFO: 
2022-08-03 15:25:25 INFO: 2022-08-03 15:25:25: step 28020/200000, loss = 0.231747 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:25:26 INFO: 2022-08-03 15:25:26: step 28040/200000, loss = 0.048198 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:25:27 INFO: 2022-08-03 15:25:27: step 28060/200000, loss = 0.422890 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:25:28 INFO: 2022-08-03 15:25:28: step 28080/200000, loss = 1.384634 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:25:29 INFO: 2022-08-03 15:25:29: step 28100/200000, loss = 0.901230 (0.046 sec/batch), lr: 0.025000
2022-08-03 15:25:30 INFO: 2022-08-03 15:25:30: step 28120/200000, loss = 0.055546 (0.047 sec/batch), lr: 0.025000
2022-08-03 15:25:30 INFO: 2022-08-03 15:25:30: step 28140/200000, loss = 0.309262 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:25:31 INFO: 2022-08-03 15:25:31: step 28160/200000, loss = 0.167062 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:25:32 INFO: 2022-08-03 15:25:32: step 28180/200000, loss = 0.358004 (0.048 sec/batch), lr: 0.025000
2022-08-03 15:25:33 INFO: 2022-08-03 15:25:33: step 28200/200000, loss = 0.035295 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:25:34 INFO: 2022-08-03 15:25:34: step 28220/200000, loss = 0.185109 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:25:35 INFO: 2022-08-03 15:25:35: step 28240/200000, loss = 0.594813 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:25:36 INFO: 2022-08-03 15:25:36: step 28260/200000, loss = 0.307896 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:25:37 INFO: 2022-08-03 15:25:37: step 28280/200000, loss = 0.103493 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:25:38 INFO: 2022-08-03 15:25:38: step 28300/200000, loss = 0.156001 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:25:39 INFO: 2022-08-03 15:25:39: step 28320/200000, loss = 0.147803 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:25:40 INFO: 2022-08-03 15:25:40: step 28340/200000, loss = 0.236201 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:25:40 INFO: 2022-08-03 15:25:40: step 28360/200000, loss = 0.357431 (0.042 sec/batch), lr: 0.025000
2022-08-03 15:25:41 INFO: 2022-08-03 15:25:41: step 28380/200000, loss = 0.170961 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:25:42 INFO: 2022-08-03 15:25:42: step 28400/200000, loss = 0.060673 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:25:43 INFO: 2022-08-03 15:25:43: step 28420/200000, loss = 0.134749 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:25:44 INFO: 2022-08-03 15:25:44: step 28440/200000, loss = 0.328604 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:25:45 INFO: 2022-08-03 15:25:45: step 28460/200000, loss = 0.171080 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:25:46 INFO: 2022-08-03 15:25:46: step 28480/200000, loss = 0.136148 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:25:47 INFO: 2022-08-03 15:25:47: step 28500/200000, loss = 0.117337 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:25:47 INFO: Evaluating on dev set...
2022-08-03 15:26:11 INFO: Score by entity:
Prec.	Rec.	F1
94.43	93.36	93.89
2022-08-03 15:26:11 INFO: step 28500: train_loss = 0.265268, dev_score = 0.9389
2022-08-03 15:26:11 INFO: 
2022-08-03 15:26:12 INFO: 2022-08-03 15:26:12: step 28520/200000, loss = 0.163252 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:26:13 INFO: 2022-08-03 15:26:13: step 28540/200000, loss = 0.242652 (0.039 sec/batch), lr: 0.025000
2022-08-03 15:26:14 INFO: 2022-08-03 15:26:14: step 28560/200000, loss = 0.091827 (0.045 sec/batch), lr: 0.025000
2022-08-03 15:26:15 INFO: 2022-08-03 15:26:15: step 28580/200000, loss = 0.125000 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:26:16 INFO: 2022-08-03 15:26:16: step 28600/200000, loss = 0.306079 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:26:16 INFO: 2022-08-03 15:26:16: step 28620/200000, loss = 0.082337 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:26:17 INFO: 2022-08-03 15:26:17: step 28640/200000, loss = 0.036484 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:26:18 INFO: 2022-08-03 15:26:18: step 28660/200000, loss = 0.440828 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:26:19 INFO: 2022-08-03 15:26:19: step 28680/200000, loss = 0.384485 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:26:20 INFO: 2022-08-03 15:26:20: step 28700/200000, loss = 0.580122 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:26:21 INFO: 2022-08-03 15:26:21: step 28720/200000, loss = 0.292834 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:26:22 INFO: 2022-08-03 15:26:22: step 28740/200000, loss = 0.260743 (0.043 sec/batch), lr: 0.025000
2022-08-03 15:26:23 INFO: 2022-08-03 15:26:23: step 28760/200000, loss = 0.252137 (0.037 sec/batch), lr: 0.025000
2022-08-03 15:26:23 INFO: 2022-08-03 15:26:23: step 28780/200000, loss = 0.080659 (0.030 sec/batch), lr: 0.025000
2022-08-03 15:26:24 INFO: 2022-08-03 15:26:24: step 28800/200000, loss = 0.054852 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:26:25 INFO: 2022-08-03 15:26:25: step 28820/200000, loss = 0.185624 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:26:26 INFO: 2022-08-03 15:26:26: step 28840/200000, loss = 0.322624 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:26:27 INFO: 2022-08-03 15:26:27: step 28860/200000, loss = 0.059200 (0.034 sec/batch), lr: 0.025000
2022-08-03 15:26:28 INFO: 2022-08-03 15:26:28: step 28880/200000, loss = 0.317225 (0.036 sec/batch), lr: 0.025000
2022-08-03 15:26:29 INFO: 2022-08-03 15:26:29: step 28900/200000, loss = 0.080600 (0.032 sec/batch), lr: 0.025000
2022-08-03 15:26:30 INFO: 2022-08-03 15:26:30: step 28920/200000, loss = 0.093112 (0.033 sec/batch), lr: 0.025000
2022-08-03 15:26:30 INFO: 2022-08-03 15:26:30: step 28940/200000, loss = 0.078290 (0.041 sec/batch), lr: 0.025000
2022-08-03 15:26:31 INFO: 2022-08-03 15:26:31: step 28960/200000, loss = 0.267913 (0.038 sec/batch), lr: 0.025000
2022-08-03 15:26:32 INFO: 2022-08-03 15:26:32: step 28980/200000, loss = 0.142260 (0.035 sec/batch), lr: 0.025000
2022-08-03 15:26:33 INFO: 2022-08-03 15:26:33: step 29000/200000, loss = 0.306323 (0.040 sec/batch), lr: 0.025000
2022-08-03 15:26:33 INFO: Evaluating on dev set...
2022-08-03 15:26:58 INFO: Score by entity:
Prec.	Rec.	F1
94.42	93.41	93.91
2022-08-03 15:26:58 INFO: step 29000: train_loss = 0.257408, dev_score = 0.9391
2022-08-03 15:26:58 INFO: 
Epoch 00058: reducing learning rate of group 0 to 1.2500e-02.
2022-08-03 15:26:58 INFO: 2022-08-03 15:26:58: step 29020/200000, loss = 0.132500 (0.050 sec/batch), lr: 0.012500
2022-08-03 15:26:59 INFO: 2022-08-03 15:26:59: step 29040/200000, loss = 0.439805 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:27:00 INFO: 2022-08-03 15:27:00: step 29060/200000, loss = 0.591778 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:27:01 INFO: 2022-08-03 15:27:01: step 29080/200000, loss = 0.166567 (0.042 sec/batch), lr: 0.012500
2022-08-03 15:27:02 INFO: 2022-08-03 15:27:02: step 29100/200000, loss = 0.205927 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:27:03 INFO: 2022-08-03 15:27:03: step 29120/200000, loss = 0.119215 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:27:04 INFO: 2022-08-03 15:27:04: step 29140/200000, loss = 0.275117 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:27:05 INFO: 2022-08-03 15:27:05: step 29160/200000, loss = 0.171389 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:05 INFO: 2022-08-03 15:27:05: step 29180/200000, loss = 0.195141 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:27:06 INFO: 2022-08-03 15:27:06: step 29200/200000, loss = 0.450692 (0.031 sec/batch), lr: 0.012500
2022-08-03 15:27:07 INFO: 2022-08-03 15:27:07: step 29220/200000, loss = 0.817484 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:27:08 INFO: 2022-08-03 15:27:08: step 29240/200000, loss = 0.137847 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:27:09 INFO: 2022-08-03 15:27:09: step 29260/200000, loss = 0.362588 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:27:10 INFO: 2022-08-03 15:27:10: step 29280/200000, loss = 0.457814 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:27:11 INFO: 2022-08-03 15:27:11: step 29300/200000, loss = 0.307289 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:27:12 INFO: 2022-08-03 15:27:12: step 29320/200000, loss = 0.152019 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:27:13 INFO: 2022-08-03 15:27:13: step 29340/200000, loss = 0.073190 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:27:13 INFO: 2022-08-03 15:27:13: step 29360/200000, loss = 0.262159 (0.030 sec/batch), lr: 0.012500
2022-08-03 15:27:14 INFO: 2022-08-03 15:27:14: step 29380/200000, loss = 0.389419 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:27:15 INFO: 2022-08-03 15:27:15: step 29400/200000, loss = 0.139599 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:27:16 INFO: 2022-08-03 15:27:16: step 29420/200000, loss = 0.152215 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:17 INFO: 2022-08-03 15:27:17: step 29440/200000, loss = 0.287552 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:27:18 INFO: 2022-08-03 15:27:18: step 29460/200000, loss = 0.119114 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:27:19 INFO: 2022-08-03 15:27:19: step 29480/200000, loss = 0.516072 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:20 INFO: 2022-08-03 15:27:20: step 29500/200000, loss = 0.123310 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:27:20 INFO: Evaluating on dev set...
2022-08-03 15:27:44 INFO: Score by entity:
Prec.	Rec.	F1
94.40	93.44	93.92
2022-08-03 15:27:44 INFO: step 29500: train_loss = 0.249672, dev_score = 0.9392
2022-08-03 15:27:44 INFO: 
2022-08-03 15:27:45 INFO: 2022-08-03 15:27:45: step 29520/200000, loss = 0.555024 (0.045 sec/batch), lr: 0.012500
2022-08-03 15:27:46 INFO: 2022-08-03 15:27:46: step 29540/200000, loss = 0.352931 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:47 INFO: 2022-08-03 15:27:47: step 29560/200000, loss = 0.101765 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:27:48 INFO: 2022-08-03 15:27:48: step 29580/200000, loss = 0.200592 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:27:49 INFO: 2022-08-03 15:27:49: step 29600/200000, loss = 0.105550 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:49 INFO: 2022-08-03 15:27:49: step 29620/200000, loss = 0.157050 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:27:50 INFO: 2022-08-03 15:27:50: step 29640/200000, loss = 0.366795 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:27:51 INFO: 2022-08-03 15:27:51: step 29660/200000, loss = 0.169768 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:52 INFO: 2022-08-03 15:27:52: step 29680/200000, loss = 0.387259 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:27:53 INFO: 2022-08-03 15:27:53: step 29700/200000, loss = 0.203462 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:27:54 INFO: 2022-08-03 15:27:54: step 29720/200000, loss = 0.308895 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:27:55 INFO: 2022-08-03 15:27:55: step 29740/200000, loss = 0.160719 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:27:56 INFO: 2022-08-03 15:27:56: step 29760/200000, loss = 0.228251 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:27:57 INFO: 2022-08-03 15:27:57: step 29780/200000, loss = 0.149905 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:27:57 INFO: 2022-08-03 15:27:57: step 29800/200000, loss = 0.156397 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:27:58 INFO: 2022-08-03 15:27:58: step 29820/200000, loss = 0.063123 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:27:59 INFO: 2022-08-03 15:27:59: step 29840/200000, loss = 0.139843 (0.050 sec/batch), lr: 0.012500
2022-08-03 15:28:00 INFO: 2022-08-03 15:28:00: step 29860/200000, loss = 0.151001 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:28:01 INFO: 2022-08-03 15:28:01: step 29880/200000, loss = 0.388815 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:28:02 INFO: 2022-08-03 15:28:02: step 29900/200000, loss = 0.192291 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:28:03 INFO: 2022-08-03 15:28:03: step 29920/200000, loss = 0.276046 (0.048 sec/batch), lr: 0.012500
2022-08-03 15:28:04 INFO: 2022-08-03 15:28:04: step 29940/200000, loss = 0.186870 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:28:05 INFO: 2022-08-03 15:28:05: step 29960/200000, loss = 0.620343 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:28:06 INFO: 2022-08-03 15:28:06: step 29980/200000, loss = 0.079417 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:28:06 INFO: 2022-08-03 15:28:06: step 30000/200000, loss = 0.311244 (0.049 sec/batch), lr: 0.012500
2022-08-03 15:28:06 INFO: Evaluating on dev set...
2022-08-03 15:28:31 INFO: Score by entity:
Prec.	Rec.	F1
94.62	93.40	94.01
2022-08-03 15:28:31 INFO: step 30000: train_loss = 0.257103, dev_score = 0.9401
2022-08-03 15:28:32 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:28:32 INFO: New best model saved.
2022-08-03 15:28:32 INFO: 
2022-08-03 15:28:33 INFO: 2022-08-03 15:28:33: step 30020/200000, loss = 0.292009 (0.030 sec/batch), lr: 0.012500
2022-08-03 15:28:34 INFO: 2022-08-03 15:28:34: step 30040/200000, loss = 0.267557 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:28:35 INFO: 2022-08-03 15:28:35: step 30060/200000, loss = 0.289639 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:28:36 INFO: 2022-08-03 15:28:36: step 30080/200000, loss = 0.667324 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:28:37 INFO: 2022-08-03 15:28:37: step 30100/200000, loss = 0.398636 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:28:38 INFO: 2022-08-03 15:28:38: step 30120/200000, loss = 0.237066 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:28:39 INFO: 2022-08-03 15:28:39: step 30140/200000, loss = 0.182313 (0.043 sec/batch), lr: 0.012500
2022-08-03 15:28:39 INFO: 2022-08-03 15:28:39: step 30160/200000, loss = 0.302881 (0.047 sec/batch), lr: 0.012500
2022-08-03 15:28:40 INFO: 2022-08-03 15:28:40: step 30180/200000, loss = 0.153599 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:28:41 INFO: 2022-08-03 15:28:41: step 30200/200000, loss = 0.201406 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:28:42 INFO: 2022-08-03 15:28:42: step 30220/200000, loss = 0.040982 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:28:43 INFO: 2022-08-03 15:28:43: step 30240/200000, loss = 0.023366 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:28:44 INFO: 2022-08-03 15:28:44: step 30260/200000, loss = 0.156152 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:28:45 INFO: 2022-08-03 15:28:45: step 30280/200000, loss = 0.272055 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:28:46 INFO: 2022-08-03 15:28:46: step 30300/200000, loss = 0.174418 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:28:47 INFO: 2022-08-03 15:28:47: step 30320/200000, loss = 0.156794 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:28:47 INFO: 2022-08-03 15:28:47: step 30340/200000, loss = 0.100768 (0.045 sec/batch), lr: 0.012500
2022-08-03 15:28:48 INFO: 2022-08-03 15:28:48: step 30360/200000, loss = 0.124760 (0.044 sec/batch), lr: 0.012500
2022-08-03 15:28:49 INFO: 2022-08-03 15:28:49: step 30380/200000, loss = 0.264595 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:28:50 INFO: 2022-08-03 15:28:50: step 30400/200000, loss = 0.232164 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:28:51 INFO: 2022-08-03 15:28:51: step 30420/200000, loss = 0.088068 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:28:52 INFO: 2022-08-03 15:28:52: step 30440/200000, loss = 0.239243 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:28:53 INFO: 2022-08-03 15:28:53: step 30460/200000, loss = 0.216796 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:28:54 INFO: 2022-08-03 15:28:54: step 30480/200000, loss = 0.440103 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:28:55 INFO: 2022-08-03 15:28:55: step 30500/200000, loss = 0.533933 (0.042 sec/batch), lr: 0.012500
2022-08-03 15:28:55 INFO: Evaluating on dev set...
2022-08-03 15:29:19 INFO: Score by entity:
Prec.	Rec.	F1
94.52	93.32	93.92
2022-08-03 15:29:19 INFO: step 30500: train_loss = 0.263659, dev_score = 0.9392
2022-08-03 15:29:19 INFO: 
2022-08-03 15:29:20 INFO: 2022-08-03 15:29:20: step 30520/200000, loss = 0.248273 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:29:21 INFO: 2022-08-03 15:29:21: step 30540/200000, loss = 0.376517 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:29:22 INFO: 2022-08-03 15:29:22: step 30560/200000, loss = 0.073987 (0.043 sec/batch), lr: 0.012500
2022-08-03 15:29:23 INFO: 2022-08-03 15:29:23: step 30580/200000, loss = 0.520330 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:29:24 INFO: 2022-08-03 15:29:24: step 30600/200000, loss = 0.593901 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:29:24 INFO: 2022-08-03 15:29:24: step 30620/200000, loss = 0.137770 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:29:25 INFO: 2022-08-03 15:29:25: step 30640/200000, loss = 0.160165 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:29:26 INFO: 2022-08-03 15:29:26: step 30660/200000, loss = 0.124915 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:29:27 INFO: 2022-08-03 15:29:27: step 30680/200000, loss = 0.113877 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:29:28 INFO: 2022-08-03 15:29:28: step 30700/200000, loss = 0.308027 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:29:29 INFO: 2022-08-03 15:29:29: step 30720/200000, loss = 0.250651 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:29:30 INFO: 2022-08-03 15:29:30: step 30740/200000, loss = 0.169095 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:29:31 INFO: 2022-08-03 15:29:31: step 30760/200000, loss = 0.255423 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:29:32 INFO: 2022-08-03 15:29:32: step 30780/200000, loss = 0.090843 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:29:32 INFO: 2022-08-03 15:29:32: step 30800/200000, loss = 0.213308 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:29:33 INFO: 2022-08-03 15:29:33: step 30820/200000, loss = 0.117210 (0.043 sec/batch), lr: 0.012500
2022-08-03 15:29:34 INFO: 2022-08-03 15:29:34: step 30840/200000, loss = 0.311072 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:29:35 INFO: 2022-08-03 15:29:35: step 30860/200000, loss = 0.258724 (0.046 sec/batch), lr: 0.012500
2022-08-03 15:29:36 INFO: 2022-08-03 15:29:36: step 30880/200000, loss = 0.457058 (0.042 sec/batch), lr: 0.012500
2022-08-03 15:29:37 INFO: 2022-08-03 15:29:37: step 30900/200000, loss = 0.202609 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:29:38 INFO: 2022-08-03 15:29:38: step 30920/200000, loss = 0.119085 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:29:39 INFO: 2022-08-03 15:29:39: step 30940/200000, loss = 0.395261 (0.033 sec/batch), lr: 0.012500
2022-08-03 15:29:39 INFO: 2022-08-03 15:29:39: step 30960/200000, loss = 0.562353 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:29:40 INFO: 2022-08-03 15:29:40: step 30980/200000, loss = 0.228294 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:29:41 INFO: 2022-08-03 15:29:41: step 31000/200000, loss = 0.131622 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:29:41 INFO: Evaluating on dev set...
2022-08-03 15:30:06 INFO: Score by entity:
Prec.	Rec.	F1
94.54	93.41	93.98
2022-08-03 15:30:06 INFO: step 31000: train_loss = 0.251016, dev_score = 0.9398
2022-08-03 15:30:06 INFO: 
2022-08-03 15:30:07 INFO: 2022-08-03 15:30:07: step 31020/200000, loss = 0.038662 (0.026 sec/batch), lr: 0.012500
2022-08-03 15:30:08 INFO: 2022-08-03 15:30:08: step 31040/200000, loss = 0.059686 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:30:09 INFO: 2022-08-03 15:30:09: step 31060/200000, loss = 0.141516 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:30:09 INFO: 2022-08-03 15:30:09: step 31080/200000, loss = 0.033922 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:30:10 INFO: 2022-08-03 15:30:10: step 31100/200000, loss = 0.024986 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:11 INFO: 2022-08-03 15:30:11: step 31120/200000, loss = 0.268996 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:12 INFO: 2022-08-03 15:30:12: step 31140/200000, loss = 0.183401 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:30:13 INFO: 2022-08-03 15:30:13: step 31160/200000, loss = 0.440881 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:30:14 INFO: 2022-08-03 15:30:14: step 31180/200000, loss = 0.466641 (0.046 sec/batch), lr: 0.012500
2022-08-03 15:30:15 INFO: 2022-08-03 15:30:15: step 31200/200000, loss = 0.241891 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:30:16 INFO: 2022-08-03 15:30:16: step 31220/200000, loss = 0.180786 (0.046 sec/batch), lr: 0.012500
2022-08-03 15:30:17 INFO: 2022-08-03 15:30:17: step 31240/200000, loss = 0.256523 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:17 INFO: 2022-08-03 15:30:17: step 31260/200000, loss = 0.087020 (0.046 sec/batch), lr: 0.012500
2022-08-03 15:30:18 INFO: 2022-08-03 15:30:18: step 31280/200000, loss = 0.261692 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:30:19 INFO: 2022-08-03 15:30:19: step 31300/200000, loss = 0.437016 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:20 INFO: 2022-08-03 15:30:20: step 31320/200000, loss = 0.246163 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:30:21 INFO: 2022-08-03 15:30:21: step 31340/200000, loss = 0.158270 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:30:22 INFO: 2022-08-03 15:30:22: step 31360/200000, loss = 0.398704 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:30:23 INFO: 2022-08-03 15:30:23: step 31380/200000, loss = 0.423368 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:30:24 INFO: 2022-08-03 15:30:24: step 31400/200000, loss = 0.310350 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:30:25 INFO: 2022-08-03 15:30:25: step 31420/200000, loss = 0.088254 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:30:25 INFO: 2022-08-03 15:30:25: step 31440/200000, loss = 0.077650 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:26 INFO: 2022-08-03 15:30:26: step 31460/200000, loss = 0.378010 (0.042 sec/batch), lr: 0.012500
2022-08-03 15:30:27 INFO: 2022-08-03 15:30:27: step 31480/200000, loss = 0.172413 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:28 INFO: 2022-08-03 15:30:28: step 31500/200000, loss = 0.183262 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:30:28 INFO: Evaluating on dev set...
2022-08-03 15:30:53 INFO: Score by entity:
Prec.	Rec.	F1
94.47	93.24	93.85
2022-08-03 15:30:53 INFO: step 31500: train_loss = 0.241358, dev_score = 0.9385
2022-08-03 15:30:53 INFO: 
2022-08-03 15:30:54 INFO: 2022-08-03 15:30:54: step 31520/200000, loss = 0.638985 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:30:54 INFO: 2022-08-03 15:30:54: step 31540/200000, loss = 0.139162 (0.044 sec/batch), lr: 0.012500
2022-08-03 15:30:55 INFO: 2022-08-03 15:30:55: step 31560/200000, loss = 0.068989 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:30:56 INFO: 2022-08-03 15:30:56: step 31580/200000, loss = 0.408407 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:30:57 INFO: 2022-08-03 15:30:57: step 31600/200000, loss = 1.175404 (0.077 sec/batch), lr: 0.012500
2022-08-03 15:30:58 INFO: 2022-08-03 15:30:58: step 31620/200000, loss = 0.119173 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:30:59 INFO: 2022-08-03 15:30:59: step 31640/200000, loss = 0.114236 (0.035 sec/batch), lr: 0.012500
2022-08-03 15:31:00 INFO: 2022-08-03 15:31:00: step 31660/200000, loss = 0.095509 (0.044 sec/batch), lr: 0.012500
2022-08-03 15:31:01 INFO: 2022-08-03 15:31:01: step 31680/200000, loss = 0.172799 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:31:02 INFO: 2022-08-03 15:31:02: step 31700/200000, loss = 0.037400 (0.036 sec/batch), lr: 0.012500
2022-08-03 15:31:03 INFO: 2022-08-03 15:31:03: step 31720/200000, loss = 0.151978 (0.043 sec/batch), lr: 0.012500
2022-08-03 15:31:03 INFO: 2022-08-03 15:31:03: step 31740/200000, loss = 0.181041 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:31:04 INFO: 2022-08-03 15:31:04: step 31760/200000, loss = 0.401401 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:31:05 INFO: 2022-08-03 15:31:05: step 31780/200000, loss = 0.342825 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:31:06 INFO: 2022-08-03 15:31:06: step 31800/200000, loss = 0.525769 (0.040 sec/batch), lr: 0.012500
2022-08-03 15:31:07 INFO: 2022-08-03 15:31:07: step 31820/200000, loss = 0.072947 (0.032 sec/batch), lr: 0.012500
2022-08-03 15:31:08 INFO: 2022-08-03 15:31:08: step 31840/200000, loss = 0.093745 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:31:09 INFO: 2022-08-03 15:31:09: step 31860/200000, loss = 0.100915 (0.034 sec/batch), lr: 0.012500
2022-08-03 15:31:10 INFO: 2022-08-03 15:31:10: step 31880/200000, loss = 0.296382 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:31:11 INFO: 2022-08-03 15:31:11: step 31900/200000, loss = 0.310155 (0.041 sec/batch), lr: 0.012500
2022-08-03 15:31:11 INFO: 2022-08-03 15:31:11: step 31920/200000, loss = 0.157208 (0.038 sec/batch), lr: 0.012500
2022-08-03 15:31:12 INFO: 2022-08-03 15:31:12: step 31940/200000, loss = 0.227218 (0.051 sec/batch), lr: 0.012500
2022-08-03 15:31:13 INFO: 2022-08-03 15:31:13: step 31960/200000, loss = 0.354927 (0.042 sec/batch), lr: 0.012500
2022-08-03 15:31:14 INFO: 2022-08-03 15:31:14: step 31980/200000, loss = 0.234523 (0.037 sec/batch), lr: 0.012500
2022-08-03 15:31:15 INFO: 2022-08-03 15:31:15: step 32000/200000, loss = 0.275669 (0.039 sec/batch), lr: 0.012500
2022-08-03 15:31:15 INFO: Evaluating on dev set...
2022-08-03 15:31:39 INFO: Score by entity:
Prec.	Rec.	F1
94.58	93.41	93.99
2022-08-03 15:31:39 INFO: step 32000: train_loss = 0.243491, dev_score = 0.9399
2022-08-03 15:31:39 INFO: 
Epoch 00064: reducing learning rate of group 0 to 6.2500e-03.
2022-08-03 15:31:40 INFO: 2022-08-03 15:31:40: step 32020/200000, loss = 0.206063 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:31:41 INFO: 2022-08-03 15:31:41: step 32040/200000, loss = 0.339140 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:31:42 INFO: 2022-08-03 15:31:42: step 32060/200000, loss = 0.109728 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:31:43 INFO: 2022-08-03 15:31:43: step 32080/200000, loss = 0.225950 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:31:44 INFO: 2022-08-03 15:31:44: step 32100/200000, loss = 0.221019 (0.043 sec/batch), lr: 0.006250
2022-08-03 15:31:45 INFO: 2022-08-03 15:31:45: step 32120/200000, loss = 0.218551 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:31:46 INFO: 2022-08-03 15:31:46: step 32140/200000, loss = 0.228983 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:31:47 INFO: 2022-08-03 15:31:47: step 32160/200000, loss = 0.521551 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:31:48 INFO: 2022-08-03 15:31:48: step 32180/200000, loss = 0.137478 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:31:48 INFO: 2022-08-03 15:31:48: step 32200/200000, loss = 0.634773 (0.047 sec/batch), lr: 0.006250
2022-08-03 15:31:49 INFO: 2022-08-03 15:31:49: step 32220/200000, loss = 0.278021 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:31:50 INFO: 2022-08-03 15:31:50: step 32240/200000, loss = 0.189726 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:31:51 INFO: 2022-08-03 15:31:51: step 32260/200000, loss = 0.339305 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:31:52 INFO: 2022-08-03 15:31:52: step 32280/200000, loss = 0.476977 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:31:53 INFO: 2022-08-03 15:31:53: step 32300/200000, loss = 0.186693 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:31:54 INFO: 2022-08-03 15:31:54: step 32320/200000, loss = 0.207615 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:31:55 INFO: 2022-08-03 15:31:55: step 32340/200000, loss = 0.144445 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:31:55 INFO: 2022-08-03 15:31:55: step 32360/200000, loss = 0.155299 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:31:56 INFO: 2022-08-03 15:31:56: step 32380/200000, loss = 0.400648 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:31:57 INFO: 2022-08-03 15:31:57: step 32400/200000, loss = 0.159111 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:31:58 INFO: 2022-08-03 15:31:58: step 32420/200000, loss = 0.061987 (0.031 sec/batch), lr: 0.006250
2022-08-03 15:31:59 INFO: 2022-08-03 15:31:59: step 32440/200000, loss = 0.148949 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:00 INFO: 2022-08-03 15:32:00: step 32460/200000, loss = 0.266848 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:01 INFO: 2022-08-03 15:32:01: step 32480/200000, loss = 0.038469 (0.045 sec/batch), lr: 0.006250
2022-08-03 15:32:02 INFO: 2022-08-03 15:32:02: step 32500/200000, loss = 0.146421 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:32:02 INFO: Evaluating on dev set...
2022-08-03 15:32:26 INFO: Score by entity:
Prec.	Rec.	F1
94.61	93.41	94.01
2022-08-03 15:32:26 INFO: step 32500: train_loss = 0.243391, dev_score = 0.9401
2022-08-03 15:32:28 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:32:28 INFO: New best model saved.
2022-08-03 15:32:28 INFO: 
2022-08-03 15:32:29 INFO: 2022-08-03 15:32:29: step 32520/200000, loss = 0.209923 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:32:30 INFO: 2022-08-03 15:32:30: step 32540/200000, loss = 0.116142 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:30 INFO: 2022-08-03 15:32:30: step 32560/200000, loss = 0.053334 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:31 INFO: 2022-08-03 15:32:31: step 32580/200000, loss = 0.185918 (0.045 sec/batch), lr: 0.006250
2022-08-03 15:32:32 INFO: 2022-08-03 15:32:32: step 32600/200000, loss = 0.043671 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:32:33 INFO: 2022-08-03 15:32:33: step 32620/200000, loss = 0.390381 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:32:34 INFO: 2022-08-03 15:32:34: step 32640/200000, loss = 0.240731 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:32:35 INFO: 2022-08-03 15:32:35: step 32660/200000, loss = 0.184462 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:32:36 INFO: 2022-08-03 15:32:36: step 32680/200000, loss = 0.432557 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:37 INFO: 2022-08-03 15:32:37: step 32700/200000, loss = 0.147174 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:32:38 INFO: 2022-08-03 15:32:38: step 32720/200000, loss = 0.068665 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:38 INFO: 2022-08-03 15:32:38: step 32740/200000, loss = 0.630128 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:39 INFO: 2022-08-03 15:32:39: step 32760/200000, loss = 0.159062 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:40 INFO: 2022-08-03 15:32:40: step 32780/200000, loss = 0.186906 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:32:41 INFO: 2022-08-03 15:32:41: step 32800/200000, loss = 0.139659 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:32:42 INFO: 2022-08-03 15:32:42: step 32820/200000, loss = 0.121551 (0.031 sec/batch), lr: 0.006250
2022-08-03 15:32:43 INFO: 2022-08-03 15:32:43: step 32840/200000, loss = 0.074196 (0.043 sec/batch), lr: 0.006250
2022-08-03 15:32:44 INFO: 2022-08-03 15:32:44: step 32860/200000, loss = 0.211882 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:32:45 INFO: 2022-08-03 15:32:45: step 32880/200000, loss = 0.545035 (0.046 sec/batch), lr: 0.006250
2022-08-03 15:32:46 INFO: 2022-08-03 15:32:46: step 32900/200000, loss = 0.111995 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:32:47 INFO: 2022-08-03 15:32:47: step 32920/200000, loss = 0.080793 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:32:47 INFO: 2022-08-03 15:32:47: step 32940/200000, loss = 0.295928 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:32:48 INFO: 2022-08-03 15:32:48: step 32960/200000, loss = 0.394559 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:32:49 INFO: 2022-08-03 15:32:49: step 32980/200000, loss = 0.264921 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:32:50 INFO: 2022-08-03 15:32:50: step 33000/200000, loss = 0.069570 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:32:50 INFO: Evaluating on dev set...
2022-08-03 15:33:15 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.36	94.02
2022-08-03 15:33:15 INFO: step 33000: train_loss = 0.236982, dev_score = 0.9402
2022-08-03 15:33:16 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:33:16 INFO: New best model saved.
2022-08-03 15:33:16 INFO: 
2022-08-03 15:33:17 INFO: 2022-08-03 15:33:17: step 33020/200000, loss = 0.050732 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:33:18 INFO: 2022-08-03 15:33:18: step 33040/200000, loss = 0.053158 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:33:19 INFO: 2022-08-03 15:33:19: step 33060/200000, loss = 0.185714 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:33:20 INFO: 2022-08-03 15:33:20: step 33080/200000, loss = 0.216348 (0.051 sec/batch), lr: 0.006250
2022-08-03 15:33:21 INFO: 2022-08-03 15:33:21: step 33100/200000, loss = 0.409753 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:33:21 INFO: 2022-08-03 15:33:21: step 33120/200000, loss = 0.222946 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:33:22 INFO: 2022-08-03 15:33:22: step 33140/200000, loss = 0.271784 (0.045 sec/batch), lr: 0.006250
2022-08-03 15:33:23 INFO: 2022-08-03 15:33:23: step 33160/200000, loss = 0.238232 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:33:24 INFO: 2022-08-03 15:33:24: step 33180/200000, loss = 0.109663 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:33:25 INFO: 2022-08-03 15:33:25: step 33200/200000, loss = 0.452758 (0.045 sec/batch), lr: 0.006250
2022-08-03 15:33:26 INFO: 2022-08-03 15:33:26: step 33220/200000, loss = 0.450569 (0.044 sec/batch), lr: 0.006250
2022-08-03 15:33:27 INFO: 2022-08-03 15:33:27: step 33240/200000, loss = 0.292404 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:33:28 INFO: 2022-08-03 15:33:28: step 33260/200000, loss = 0.335596 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:33:29 INFO: 2022-08-03 15:33:29: step 33280/200000, loss = 0.247418 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:33:29 INFO: 2022-08-03 15:33:29: step 33300/200000, loss = 0.087481 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:33:30 INFO: 2022-08-03 15:33:30: step 33320/200000, loss = 0.112225 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:33:31 INFO: 2022-08-03 15:33:31: step 33340/200000, loss = 0.093433 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:33:32 INFO: 2022-08-03 15:33:32: step 33360/200000, loss = 0.146821 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:33:33 INFO: 2022-08-03 15:33:33: step 33380/200000, loss = 0.730847 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:33:34 INFO: 2022-08-03 15:33:34: step 33400/200000, loss = 0.045115 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:33:35 INFO: 2022-08-03 15:33:35: step 33420/200000, loss = 0.295838 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:33:36 INFO: 2022-08-03 15:33:36: step 33440/200000, loss = 0.420152 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:33:37 INFO: 2022-08-03 15:33:37: step 33460/200000, loss = 0.149199 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:33:37 INFO: 2022-08-03 15:33:37: step 33480/200000, loss = 0.071730 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:33:38 INFO: 2022-08-03 15:33:38: step 33500/200000, loss = 0.198873 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:33:38 INFO: Evaluating on dev set...
2022-08-03 15:34:03 INFO: Score by entity:
Prec.	Rec.	F1
94.67	93.44	94.05
2022-08-03 15:34:03 INFO: step 33500: train_loss = 0.235309, dev_score = 0.9405
2022-08-03 15:34:04 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:34:04 INFO: New best model saved.
2022-08-03 15:34:04 INFO: 
2022-08-03 15:34:05 INFO: 2022-08-03 15:34:05: step 33520/200000, loss = 0.038806 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:34:06 INFO: 2022-08-03 15:34:06: step 33540/200000, loss = 0.286221 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:34:07 INFO: 2022-08-03 15:34:07: step 33560/200000, loss = 0.074822 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:34:08 INFO: 2022-08-03 15:34:08: step 33580/200000, loss = 0.489250 (0.043 sec/batch), lr: 0.006250
2022-08-03 15:34:09 INFO: 2022-08-03 15:34:09: step 33600/200000, loss = 0.130035 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:34:10 INFO: 2022-08-03 15:34:10: step 33620/200000, loss = 0.269381 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:34:10 INFO: 2022-08-03 15:34:10: step 33640/200000, loss = 0.138931 (0.048 sec/batch), lr: 0.006250
2022-08-03 15:34:11 INFO: 2022-08-03 15:34:11: step 33660/200000, loss = 0.093719 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:34:12 INFO: 2022-08-03 15:34:12: step 33680/200000, loss = 0.327088 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:34:13 INFO: 2022-08-03 15:34:13: step 33700/200000, loss = 0.111264 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:34:14 INFO: 2022-08-03 15:34:14: step 33720/200000, loss = 0.455112 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:34:15 INFO: 2022-08-03 15:34:15: step 33740/200000, loss = 0.235390 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:34:16 INFO: 2022-08-03 15:34:16: step 33760/200000, loss = 0.050060 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:34:17 INFO: 2022-08-03 15:34:17: step 33780/200000, loss = 0.092154 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:34:17 INFO: 2022-08-03 15:34:17: step 33800/200000, loss = 0.340219 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:34:18 INFO: 2022-08-03 15:34:18: step 33820/200000, loss = 0.176421 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:34:19 INFO: 2022-08-03 15:34:19: step 33840/200000, loss = 0.074513 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:34:20 INFO: 2022-08-03 15:34:20: step 33860/200000, loss = 0.259983 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:34:21 INFO: 2022-08-03 15:34:21: step 33880/200000, loss = 0.345028 (0.046 sec/batch), lr: 0.006250
2022-08-03 15:34:22 INFO: 2022-08-03 15:34:22: step 33900/200000, loss = 0.302355 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:34:23 INFO: 2022-08-03 15:34:23: step 33920/200000, loss = 0.039043 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:34:24 INFO: 2022-08-03 15:34:24: step 33940/200000, loss = 0.132680 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:34:25 INFO: 2022-08-03 15:34:25: step 33960/200000, loss = 0.121897 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:34:25 INFO: 2022-08-03 15:34:25: step 33980/200000, loss = 0.346370 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:34:26 INFO: 2022-08-03 15:34:26: step 34000/200000, loss = 0.075150 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:34:26 INFO: Evaluating on dev set...
2022-08-03 15:34:51 INFO: Score by entity:
Prec.	Rec.	F1
94.57	93.48	94.02
2022-08-03 15:34:51 INFO: step 34000: train_loss = 0.247839, dev_score = 0.9402
2022-08-03 15:34:51 INFO: 
2022-08-03 15:34:52 INFO: 2022-08-03 15:34:52: step 34020/200000, loss = 0.201717 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:34:53 INFO: 2022-08-03 15:34:53: step 34040/200000, loss = 0.299426 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:34:53 INFO: 2022-08-03 15:34:53: step 34060/200000, loss = 0.872863 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:34:54 INFO: 2022-08-03 15:34:54: step 34080/200000, loss = 0.177319 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:34:55 INFO: 2022-08-03 15:34:55: step 34100/200000, loss = 0.218349 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:34:56 INFO: 2022-08-03 15:34:56: step 34120/200000, loss = 0.325437 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:34:57 INFO: 2022-08-03 15:34:57: step 34140/200000, loss = 0.058222 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:34:58 INFO: 2022-08-03 15:34:58: step 34160/200000, loss = 0.175204 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:34:59 INFO: 2022-08-03 15:34:59: step 34180/200000, loss = 0.078599 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:35:00 INFO: 2022-08-03 15:35:00: step 34200/200000, loss = 0.152993 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:01 INFO: 2022-08-03 15:35:01: step 34220/200000, loss = 0.145034 (0.044 sec/batch), lr: 0.006250
2022-08-03 15:35:01 INFO: 2022-08-03 15:35:01: step 34240/200000, loss = 0.232730 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:02 INFO: 2022-08-03 15:35:02: step 34260/200000, loss = 0.118096 (0.047 sec/batch), lr: 0.006250
2022-08-03 15:35:03 INFO: 2022-08-03 15:35:03: step 34280/200000, loss = 0.395710 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:04 INFO: 2022-08-03 15:35:04: step 34300/200000, loss = 0.210496 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:05 INFO: 2022-08-03 15:35:05: step 34320/200000, loss = 0.321620 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:06 INFO: 2022-08-03 15:35:06: step 34340/200000, loss = 0.155605 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:35:07 INFO: 2022-08-03 15:35:07: step 34360/200000, loss = 0.093637 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:35:08 INFO: 2022-08-03 15:35:08: step 34380/200000, loss = 0.184294 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:08 INFO: 2022-08-03 15:35:08: step 34400/200000, loss = 0.533544 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:35:09 INFO: 2022-08-03 15:35:09: step 34420/200000, loss = 0.297180 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:35:10 INFO: 2022-08-03 15:35:10: step 34440/200000, loss = 0.290278 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:35:11 INFO: 2022-08-03 15:35:11: step 34460/200000, loss = 0.070655 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:35:12 INFO: 2022-08-03 15:35:12: step 34480/200000, loss = 0.936032 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:13 INFO: 2022-08-03 15:35:13: step 34500/200000, loss = 0.121863 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:35:13 INFO: Evaluating on dev set...
2022-08-03 15:35:37 INFO: Score by entity:
Prec.	Rec.	F1
94.64	93.57	94.10
2022-08-03 15:35:37 INFO: step 34500: train_loss = 0.237768, dev_score = 0.9410
2022-08-03 15:35:39 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:35:39 INFO: New best model saved.
2022-08-03 15:35:39 INFO: 
2022-08-03 15:35:40 INFO: 2022-08-03 15:35:40: step 34520/200000, loss = 0.232952 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:41 INFO: 2022-08-03 15:35:41: step 34540/200000, loss = 0.392083 (0.051 sec/batch), lr: 0.006250
2022-08-03 15:35:41 INFO: 2022-08-03 15:35:41: step 34560/200000, loss = 0.447044 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:35:42 INFO: 2022-08-03 15:35:42: step 34580/200000, loss = 0.223552 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:35:43 INFO: 2022-08-03 15:35:43: step 34600/200000, loss = 0.873180 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:44 INFO: 2022-08-03 15:35:44: step 34620/200000, loss = 0.319996 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:45 INFO: 2022-08-03 15:35:45: step 34640/200000, loss = 0.229833 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:46 INFO: 2022-08-03 15:35:46: step 34660/200000, loss = 0.322315 (0.043 sec/batch), lr: 0.006250
2022-08-03 15:35:47 INFO: 2022-08-03 15:35:47: step 34680/200000, loss = 0.145284 (0.043 sec/batch), lr: 0.006250
2022-08-03 15:35:48 INFO: 2022-08-03 15:35:48: step 34700/200000, loss = 0.070976 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:49 INFO: 2022-08-03 15:35:49: step 34720/200000, loss = 0.235510 (0.045 sec/batch), lr: 0.006250
2022-08-03 15:35:49 INFO: 2022-08-03 15:35:49: step 34740/200000, loss = 0.100820 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:50 INFO: 2022-08-03 15:35:50: step 34760/200000, loss = 0.192311 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:51 INFO: 2022-08-03 15:35:51: step 34780/200000, loss = 0.050861 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:52 INFO: 2022-08-03 15:35:52: step 34800/200000, loss = 0.154860 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:35:53 INFO: 2022-08-03 15:35:53: step 34820/200000, loss = 0.366299 (0.044 sec/batch), lr: 0.006250
2022-08-03 15:35:54 INFO: 2022-08-03 15:35:54: step 34840/200000, loss = 0.322808 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:55 INFO: 2022-08-03 15:35:55: step 34860/200000, loss = 0.345473 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:35:56 INFO: 2022-08-03 15:35:56: step 34880/200000, loss = 0.777167 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:57 INFO: 2022-08-03 15:35:57: step 34900/200000, loss = 0.257555 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:35:57 INFO: 2022-08-03 15:35:57: step 34920/200000, loss = 0.381742 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:35:58 INFO: 2022-08-03 15:35:58: step 34940/200000, loss = 0.593369 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:35:59 INFO: 2022-08-03 15:35:59: step 34960/200000, loss = 0.177946 (0.051 sec/batch), lr: 0.006250
2022-08-03 15:36:00 INFO: 2022-08-03 15:36:00: step 34980/200000, loss = 0.049972 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:36:01 INFO: 2022-08-03 15:36:01: step 35000/200000, loss = 0.421502 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:36:01 INFO: Evaluating on dev set...
2022-08-03 15:36:25 INFO: Score by entity:
Prec.	Rec.	F1
94.65	93.39	94.02
2022-08-03 15:36:25 INFO: step 35000: train_loss = 0.230338, dev_score = 0.9402
2022-08-03 15:36:25 INFO: 
2022-08-03 15:36:26 INFO: 2022-08-03 15:36:26: step 35020/200000, loss = 0.162119 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:36:27 INFO: 2022-08-03 15:36:27: step 35040/200000, loss = 0.143978 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:36:28 INFO: 2022-08-03 15:36:28: step 35060/200000, loss = 0.284917 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:36:29 INFO: 2022-08-03 15:36:29: step 35080/200000, loss = 0.120419 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:36:30 INFO: 2022-08-03 15:36:30: step 35100/200000, loss = 0.032625 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:36:31 INFO: 2022-08-03 15:36:31: step 35120/200000, loss = 0.208247 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:36:32 INFO: 2022-08-03 15:36:32: step 35140/200000, loss = 0.207575 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:36:32 INFO: 2022-08-03 15:36:32: step 35160/200000, loss = 0.273274 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:36:33 INFO: 2022-08-03 15:36:33: step 35180/200000, loss = 0.074121 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:36:34 INFO: 2022-08-03 15:36:34: step 35200/200000, loss = 1.343077 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:36:35 INFO: 2022-08-03 15:36:35: step 35220/200000, loss = 0.168060 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:36:36 INFO: 2022-08-03 15:36:36: step 35240/200000, loss = 0.067578 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:36:37 INFO: 2022-08-03 15:36:37: step 35260/200000, loss = 0.047505 (0.031 sec/batch), lr: 0.006250
2022-08-03 15:36:38 INFO: 2022-08-03 15:36:38: step 35280/200000, loss = 0.948153 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:36:39 INFO: 2022-08-03 15:36:39: step 35300/200000, loss = 0.490420 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:36:40 INFO: 2022-08-03 15:36:40: step 35320/200000, loss = 0.233320 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:36:41 INFO: 2022-08-03 15:36:41: step 35340/200000, loss = 0.385103 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:36:41 INFO: 2022-08-03 15:36:41: step 35360/200000, loss = 0.369720 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:36:42 INFO: 2022-08-03 15:36:42: step 35380/200000, loss = 0.285353 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:36:43 INFO: 2022-08-03 15:36:43: step 35400/200000, loss = 0.305351 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:36:44 INFO: 2022-08-03 15:36:44: step 35420/200000, loss = 0.060360 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:36:45 INFO: 2022-08-03 15:36:45: step 35440/200000, loss = 0.628499 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:36:46 INFO: 2022-08-03 15:36:46: step 35460/200000, loss = 0.209883 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:36:47 INFO: 2022-08-03 15:36:47: step 35480/200000, loss = 0.311703 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:36:48 INFO: 2022-08-03 15:36:48: step 35500/200000, loss = 0.192663 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:36:48 INFO: Evaluating on dev set...
2022-08-03 15:37:12 INFO: Score by entity:
Prec.	Rec.	F1
94.65	93.36	94.00
2022-08-03 15:37:12 INFO: step 35500: train_loss = 0.236720, dev_score = 0.9400
2022-08-03 15:37:12 INFO: 
2022-08-03 15:37:13 INFO: 2022-08-03 15:37:13: step 35520/200000, loss = 0.263144 (0.046 sec/batch), lr: 0.006250
2022-08-03 15:37:14 INFO: 2022-08-03 15:37:14: step 35540/200000, loss = 0.168628 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:37:15 INFO: 2022-08-03 15:37:15: step 35560/200000, loss = 0.181277 (0.050 sec/batch), lr: 0.006250
2022-08-03 15:37:16 INFO: 2022-08-03 15:37:16: step 35580/200000, loss = 0.410049 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:37:17 INFO: 2022-08-03 15:37:17: step 35600/200000, loss = 0.094801 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:37:17 INFO: 2022-08-03 15:37:17: step 35620/200000, loss = 0.152266 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:37:18 INFO: 2022-08-03 15:37:18: step 35640/200000, loss = 0.054040 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:37:19 INFO: 2022-08-03 15:37:19: step 35660/200000, loss = 0.324069 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:37:20 INFO: 2022-08-03 15:37:20: step 35680/200000, loss = 0.197095 (0.041 sec/batch), lr: 0.006250
2022-08-03 15:37:21 INFO: 2022-08-03 15:37:21: step 35700/200000, loss = 0.261449 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:37:22 INFO: 2022-08-03 15:37:22: step 35720/200000, loss = 0.090884 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:37:23 INFO: 2022-08-03 15:37:23: step 35740/200000, loss = 0.040039 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:37:24 INFO: 2022-08-03 15:37:24: step 35760/200000, loss = 0.256812 (0.031 sec/batch), lr: 0.006250
2022-08-03 15:37:25 INFO: 2022-08-03 15:37:25: step 35780/200000, loss = 0.694531 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:37:26 INFO: 2022-08-03 15:37:26: step 35800/200000, loss = 0.120381 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:37:26 INFO: 2022-08-03 15:37:26: step 35820/200000, loss = 0.406548 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:37:27 INFO: 2022-08-03 15:37:27: step 35840/200000, loss = 0.132598 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:37:28 INFO: 2022-08-03 15:37:28: step 35860/200000, loss = 0.212917 (0.032 sec/batch), lr: 0.006250
2022-08-03 15:37:29 INFO: 2022-08-03 15:37:29: step 35880/200000, loss = 0.201133 (0.053 sec/batch), lr: 0.006250
2022-08-03 15:37:30 INFO: 2022-08-03 15:37:30: step 35900/200000, loss = 0.259385 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:37:31 INFO: 2022-08-03 15:37:31: step 35920/200000, loss = 0.787665 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:37:32 INFO: 2022-08-03 15:37:32: step 35940/200000, loss = 0.082212 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:37:33 INFO: 2022-08-03 15:37:33: step 35960/200000, loss = 0.127824 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:37:33 INFO: 2022-08-03 15:37:33: step 35980/200000, loss = 0.473198 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:37:34 INFO: 2022-08-03 15:37:34: step 36000/200000, loss = 0.106689 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:37:34 INFO: Evaluating on dev set...
2022-08-03 15:37:59 INFO: Score by entity:
Prec.	Rec.	F1
94.68	93.46	94.07
2022-08-03 15:37:59 INFO: step 36000: train_loss = 0.243294, dev_score = 0.9407
2022-08-03 15:37:59 INFO: 
2022-08-03 15:38:00 INFO: 2022-08-03 15:38:00: step 36020/200000, loss = 0.152270 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:38:01 INFO: 2022-08-03 15:38:01: step 36040/200000, loss = 0.230690 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:38:02 INFO: 2022-08-03 15:38:02: step 36060/200000, loss = 0.285243 (0.047 sec/batch), lr: 0.006250
2022-08-03 15:38:03 INFO: 2022-08-03 15:38:03: step 36080/200000, loss = 0.125378 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:38:04 INFO: 2022-08-03 15:38:04: step 36100/200000, loss = 0.043732 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:38:04 INFO: 2022-08-03 15:38:04: step 36120/200000, loss = 0.417099 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:38:05 INFO: 2022-08-03 15:38:05: step 36140/200000, loss = 0.346326 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:38:06 INFO: 2022-08-03 15:38:06: step 36160/200000, loss = 0.207775 (0.038 sec/batch), lr: 0.006250
2022-08-03 15:38:07 INFO: 2022-08-03 15:38:07: step 36180/200000, loss = 0.102795 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:38:08 INFO: 2022-08-03 15:38:08: step 36200/200000, loss = 0.255508 (0.034 sec/batch), lr: 0.006250
2022-08-03 15:38:09 INFO: 2022-08-03 15:38:09: step 36220/200000, loss = 0.404966 (0.036 sec/batch), lr: 0.006250
2022-08-03 15:38:10 INFO: 2022-08-03 15:38:10: step 36240/200000, loss = 0.412805 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:38:11 INFO: 2022-08-03 15:38:11: step 36260/200000, loss = 0.421686 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:38:12 INFO: 2022-08-03 15:38:12: step 36280/200000, loss = 0.373101 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:38:13 INFO: 2022-08-03 15:38:13: step 36300/200000, loss = 0.237702 (0.037 sec/batch), lr: 0.006250
2022-08-03 15:38:13 INFO: 2022-08-03 15:38:13: step 36320/200000, loss = 0.081059 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:38:14 INFO: 2022-08-03 15:38:14: step 36340/200000, loss = 0.443794 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:38:15 INFO: 2022-08-03 15:38:15: step 36360/200000, loss = 0.087197 (0.074 sec/batch), lr: 0.006250
2022-08-03 15:38:16 INFO: 2022-08-03 15:38:16: step 36380/200000, loss = 0.062963 (0.039 sec/batch), lr: 0.006250
2022-08-03 15:38:17 INFO: 2022-08-03 15:38:17: step 36400/200000, loss = 0.052266 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:38:18 INFO: 2022-08-03 15:38:18: step 36420/200000, loss = 0.094880 (0.033 sec/batch), lr: 0.006250
2022-08-03 15:38:19 INFO: 2022-08-03 15:38:19: step 36440/200000, loss = 0.182874 (0.040 sec/batch), lr: 0.006250
2022-08-03 15:38:20 INFO: 2022-08-03 15:38:20: step 36460/200000, loss = 0.422722 (0.042 sec/batch), lr: 0.006250
2022-08-03 15:38:21 INFO: 2022-08-03 15:38:21: step 36480/200000, loss = 0.155482 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:38:21 INFO: 2022-08-03 15:38:21: step 36500/200000, loss = 0.479358 (0.035 sec/batch), lr: 0.006250
2022-08-03 15:38:21 INFO: Evaluating on dev set...
2022-08-03 15:38:46 INFO: Score by entity:
Prec.	Rec.	F1
94.64	93.43	94.03
2022-08-03 15:38:46 INFO: step 36500: train_loss = 0.232368, dev_score = 0.9403
2022-08-03 15:38:46 INFO: 
Epoch 00073: reducing learning rate of group 0 to 3.1250e-03.
2022-08-03 15:38:47 INFO: 2022-08-03 15:38:47: step 36520/200000, loss = 0.813926 (0.046 sec/batch), lr: 0.003125
2022-08-03 15:38:48 INFO: 2022-08-03 15:38:48: step 36540/200000, loss = 0.299263 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:38:49 INFO: 2022-08-03 15:38:49: step 36560/200000, loss = 0.105003 (0.042 sec/batch), lr: 0.003125
2022-08-03 15:38:50 INFO: 2022-08-03 15:38:50: step 36580/200000, loss = 0.277729 (0.039 sec/batch), lr: 0.003125
2022-08-03 15:38:50 INFO: 2022-08-03 15:38:50: step 36600/200000, loss = 0.621872 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:38:51 INFO: 2022-08-03 15:38:51: step 36620/200000, loss = 0.440139 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:38:52 INFO: 2022-08-03 15:38:52: step 36640/200000, loss = 0.422361 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:38:53 INFO: 2022-08-03 15:38:53: step 36660/200000, loss = 0.108427 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:38:54 INFO: 2022-08-03 15:38:54: step 36680/200000, loss = 0.114195 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:38:55 INFO: 2022-08-03 15:38:55: step 36700/200000, loss = 0.317191 (0.038 sec/batch), lr: 0.003125
2022-08-03 15:38:56 INFO: 2022-08-03 15:38:56: step 36720/200000, loss = 0.141947 (0.048 sec/batch), lr: 0.003125
2022-08-03 15:38:57 INFO: 2022-08-03 15:38:57: step 36740/200000, loss = 0.146661 (0.039 sec/batch), lr: 0.003125
2022-08-03 15:38:58 INFO: 2022-08-03 15:38:58: step 36760/200000, loss = 0.934256 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:38:59 INFO: 2022-08-03 15:38:59: step 36780/200000, loss = 0.235232 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:38:59 INFO: 2022-08-03 15:38:59: step 36800/200000, loss = 0.367300 (0.043 sec/batch), lr: 0.003125
2022-08-03 15:39:00 INFO: 2022-08-03 15:39:00: step 36820/200000, loss = 0.274758 (0.038 sec/batch), lr: 0.003125
2022-08-03 15:39:01 INFO: 2022-08-03 15:39:01: step 36840/200000, loss = 0.146518 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:02 INFO: 2022-08-03 15:39:02: step 36860/200000, loss = 0.208544 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:03 INFO: 2022-08-03 15:39:03: step 36880/200000, loss = 0.102993 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:04 INFO: 2022-08-03 15:39:04: step 36900/200000, loss = 0.279838 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:39:05 INFO: 2022-08-03 15:39:05: step 36920/200000, loss = 0.347527 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:06 INFO: 2022-08-03 15:39:06: step 36940/200000, loss = 0.233123 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:07 INFO: 2022-08-03 15:39:07: step 36960/200000, loss = 0.132103 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:39:08 INFO: 2022-08-03 15:39:08: step 36980/200000, loss = 0.220193 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:39:08 INFO: 2022-08-03 15:39:08: step 37000/200000, loss = 0.184770 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:08 INFO: Evaluating on dev set...
2022-08-03 15:39:33 INFO: Score by entity:
Prec.	Rec.	F1
94.72	93.50	94.11
2022-08-03 15:39:33 INFO: step 37000: train_loss = 0.232105, dev_score = 0.9411
2022-08-03 15:39:34 INFO: Model saved to saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:39:34 INFO: New best model saved.
2022-08-03 15:39:34 INFO: 
2022-08-03 15:39:35 INFO: 2022-08-03 15:39:35: step 37020/200000, loss = 0.311683 (0.041 sec/batch), lr: 0.003125
2022-08-03 15:39:36 INFO: 2022-08-03 15:39:36: step 37040/200000, loss = 0.538827 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:39:37 INFO: 2022-08-03 15:39:37: step 37060/200000, loss = 0.083549 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:38 INFO: 2022-08-03 15:39:38: step 37080/200000, loss = 0.467100 (0.033 sec/batch), lr: 0.003125
2022-08-03 15:39:39 INFO: 2022-08-03 15:39:39: step 37100/200000, loss = 0.186133 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:40 INFO: 2022-08-03 15:39:40: step 37120/200000, loss = 0.220142 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:39:40 INFO: 2022-08-03 15:39:40: step 37140/200000, loss = 0.132899 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:39:41 INFO: 2022-08-03 15:39:41: step 37160/200000, loss = 0.245356 (0.032 sec/batch), lr: 0.003125
2022-08-03 15:39:42 INFO: 2022-08-03 15:39:42: step 37180/200000, loss = 0.203770 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:39:43 INFO: 2022-08-03 15:39:43: step 37200/200000, loss = 0.225623 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:44 INFO: 2022-08-03 15:39:44: step 37220/200000, loss = 0.045640 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:45 INFO: 2022-08-03 15:39:45: step 37240/200000, loss = 0.342866 (0.043 sec/batch), lr: 0.003125
2022-08-03 15:39:46 INFO: 2022-08-03 15:39:46: step 37260/200000, loss = 0.282478 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:47 INFO: 2022-08-03 15:39:47: step 37280/200000, loss = 0.655378 (0.045 sec/batch), lr: 0.003125
2022-08-03 15:39:48 INFO: 2022-08-03 15:39:48: step 37300/200000, loss = 0.218954 (0.032 sec/batch), lr: 0.003125
2022-08-03 15:39:48 INFO: 2022-08-03 15:39:48: step 37320/200000, loss = 0.131568 (0.038 sec/batch), lr: 0.003125
2022-08-03 15:39:49 INFO: 2022-08-03 15:39:49: step 37340/200000, loss = 0.107013 (0.047 sec/batch), lr: 0.003125
2022-08-03 15:39:50 INFO: 2022-08-03 15:39:50: step 37360/200000, loss = 0.287504 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:39:51 INFO: 2022-08-03 15:39:51: step 37380/200000, loss = 0.214157 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:52 INFO: 2022-08-03 15:39:52: step 37400/200000, loss = 0.096581 (0.043 sec/batch), lr: 0.003125
2022-08-03 15:39:53 INFO: 2022-08-03 15:39:53: step 37420/200000, loss = 0.077717 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:39:54 INFO: 2022-08-03 15:39:54: step 37440/200000, loss = 0.111058 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:55 INFO: 2022-08-03 15:39:55: step 37460/200000, loss = 0.105060 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:39:56 INFO: 2022-08-03 15:39:56: step 37480/200000, loss = 0.298404 (0.038 sec/batch), lr: 0.003125
2022-08-03 15:39:56 INFO: 2022-08-03 15:39:56: step 37500/200000, loss = 0.162949 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:39:56 INFO: Evaluating on dev set...
2022-08-03 15:40:21 INFO: Score by entity:
Prec.	Rec.	F1
94.55	93.47	94.01
2022-08-03 15:40:21 INFO: step 37500: train_loss = 0.236090, dev_score = 0.9401
2022-08-03 15:40:21 INFO: 
2022-08-03 15:40:22 INFO: 2022-08-03 15:40:22: step 37520/200000, loss = 0.564960 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:40:23 INFO: 2022-08-03 15:40:23: step 37540/200000, loss = 0.114581 (0.042 sec/batch), lr: 0.003125
2022-08-03 15:40:24 INFO: 2022-08-03 15:40:24: step 37560/200000, loss = 0.428026 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:40:25 INFO: 2022-08-03 15:40:25: step 37580/200000, loss = 0.183621 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:40:26 INFO: 2022-08-03 15:40:26: step 37600/200000, loss = 0.267658 (0.033 sec/batch), lr: 0.003125
2022-08-03 15:40:26 INFO: 2022-08-03 15:40:26: step 37620/200000, loss = 0.244101 (0.059 sec/batch), lr: 0.003125
2022-08-03 15:40:27 INFO: 2022-08-03 15:40:27: step 37640/200000, loss = 0.234341 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:40:28 INFO: 2022-08-03 15:40:28: step 37660/200000, loss = 0.183330 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:40:29 INFO: 2022-08-03 15:40:29: step 37680/200000, loss = 0.759246 (0.043 sec/batch), lr: 0.003125
2022-08-03 15:40:30 INFO: 2022-08-03 15:40:30: step 37700/200000, loss = 0.178829 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:40:31 INFO: 2022-08-03 15:40:31: step 37720/200000, loss = 0.291942 (0.041 sec/batch), lr: 0.003125
2022-08-03 15:40:32 INFO: 2022-08-03 15:40:32: step 37740/200000, loss = 0.170565 (0.050 sec/batch), lr: 0.003125
2022-08-03 15:40:33 INFO: 2022-08-03 15:40:33: step 37760/200000, loss = 0.452326 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:40:34 INFO: 2022-08-03 15:40:34: step 37780/200000, loss = 0.087595 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:40:34 INFO: 2022-08-03 15:40:34: step 37800/200000, loss = 0.051878 (0.041 sec/batch), lr: 0.003125
2022-08-03 15:40:35 INFO: 2022-08-03 15:40:35: step 37820/200000, loss = 0.347758 (0.033 sec/batch), lr: 0.003125
2022-08-03 15:40:36 INFO: 2022-08-03 15:40:36: step 37840/200000, loss = 0.302301 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:40:37 INFO: 2022-08-03 15:40:37: step 37860/200000, loss = 0.195930 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:40:38 INFO: 2022-08-03 15:40:38: step 37880/200000, loss = 0.443121 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:40:39 INFO: 2022-08-03 15:40:39: step 37900/200000, loss = 0.335067 (0.045 sec/batch), lr: 0.003125
2022-08-03 15:40:40 INFO: 2022-08-03 15:40:40: step 37920/200000, loss = 0.080882 (0.039 sec/batch), lr: 0.003125
2022-08-03 15:40:41 INFO: 2022-08-03 15:40:41: step 37940/200000, loss = 0.179292 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:40:41 INFO: 2022-08-03 15:40:41: step 37960/200000, loss = 0.028528 (0.042 sec/batch), lr: 0.003125
2022-08-03 15:40:42 INFO: 2022-08-03 15:40:42: step 37980/200000, loss = 0.099551 (0.033 sec/batch), lr: 0.003125
2022-08-03 15:40:43 INFO: 2022-08-03 15:40:43: step 38000/200000, loss = 0.507751 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:40:43 INFO: Evaluating on dev set...
2022-08-03 15:41:08 INFO: Score by entity:
Prec.	Rec.	F1
94.63	93.55	94.09
2022-08-03 15:41:08 INFO: step 38000: train_loss = 0.234082, dev_score = 0.9409
2022-08-03 15:41:08 INFO: 
2022-08-03 15:41:09 INFO: 2022-08-03 15:41:09: step 38020/200000, loss = 0.131832 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:41:09 INFO: 2022-08-03 15:41:09: step 38040/200000, loss = 0.110686 (0.036 sec/batch), lr: 0.003125
2022-08-03 15:41:10 INFO: 2022-08-03 15:41:10: step 38060/200000, loss = 0.159640 (0.034 sec/batch), lr: 0.003125
2022-08-03 15:41:11 INFO: 2022-08-03 15:41:11: step 38080/200000, loss = 0.234272 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:12 INFO: 2022-08-03 15:41:12: step 38100/200000, loss = 0.531830 (0.044 sec/batch), lr: 0.003125
2022-08-03 15:41:13 INFO: 2022-08-03 15:41:13: step 38120/200000, loss = 0.313598 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:41:14 INFO: 2022-08-03 15:41:14: step 38140/200000, loss = 0.897205 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:41:15 INFO: 2022-08-03 15:41:15: step 38160/200000, loss = 0.338874 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:41:16 INFO: 2022-08-03 15:41:16: step 38180/200000, loss = 0.173939 (0.035 sec/batch), lr: 0.003125
2022-08-03 15:41:17 INFO: 2022-08-03 15:41:17: step 38200/200000, loss = 0.284261 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:18 INFO: 2022-08-03 15:41:18: step 38220/200000, loss = 0.122532 (0.042 sec/batch), lr: 0.003125
2022-08-03 15:41:18 INFO: 2022-08-03 15:41:18: step 38240/200000, loss = 0.325886 (0.039 sec/batch), lr: 0.003125
2022-08-03 15:41:19 INFO: 2022-08-03 15:41:19: step 38260/200000, loss = 0.343202 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:41:20 INFO: 2022-08-03 15:41:20: step 38280/200000, loss = 0.111617 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:21 INFO: 2022-08-03 15:41:21: step 38300/200000, loss = 0.226679 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:22 INFO: 2022-08-03 15:41:22: step 38320/200000, loss = 0.325755 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:41:23 INFO: 2022-08-03 15:41:23: step 38340/200000, loss = 0.285994 (0.037 sec/batch), lr: 0.003125
2022-08-03 15:41:24 INFO: 2022-08-03 15:41:24: step 38360/200000, loss = 0.429372 (0.039 sec/batch), lr: 0.003125
2022-08-03 15:41:25 INFO: 2022-08-03 15:41:25: step 38380/200000, loss = 0.178051 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:26 INFO: 2022-08-03 15:41:26: step 38400/200000, loss = 0.050908 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:26 INFO: 2022-08-03 15:41:26: step 38420/200000, loss = 0.125916 (0.038 sec/batch), lr: 0.003125
2022-08-03 15:41:27 INFO: 2022-08-03 15:41:27: step 38440/200000, loss = 0.282987 (0.049 sec/batch), lr: 0.003125
2022-08-03 15:41:28 INFO: 2022-08-03 15:41:28: step 38460/200000, loss = 0.243019 (0.040 sec/batch), lr: 0.003125
2022-08-03 15:41:29 INFO: 2022-08-03 15:41:29: step 38480/200000, loss = 0.210815 (0.046 sec/batch), lr: 0.003125
2022-08-03 15:41:30 INFO: 2022-08-03 15:41:30: step 38500/200000, loss = 0.363496 (0.044 sec/batch), lr: 0.003125
2022-08-03 15:41:30 INFO: Evaluating on dev set...
2022-08-03 15:41:54 INFO: Score by entity:
Prec.	Rec.	F1
94.64	93.42	94.02
2022-08-03 15:41:54 INFO: step 38500: train_loss = 0.212069, dev_score = 0.9402
2022-08-03 15:41:54 INFO: 
Epoch 00077: reducing learning rate of group 0 to 1.5625e-03.
2022-08-03 15:41:55 INFO: 2022-08-03 15:41:55: step 38520/200000, loss = 0.166943 (0.045 sec/batch), lr: 0.001563
2022-08-03 15:41:56 INFO: 2022-08-03 15:41:56: step 38540/200000, loss = 0.486445 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:41:57 INFO: 2022-08-03 15:41:57: step 38560/200000, loss = 0.065199 (0.054 sec/batch), lr: 0.001563
2022-08-03 15:41:58 INFO: 2022-08-03 15:41:58: step 38580/200000, loss = 0.259962 (0.041 sec/batch), lr: 0.001563
2022-08-03 15:41:59 INFO: 2022-08-03 15:41:59: step 38600/200000, loss = 0.242039 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:42:00 INFO: 2022-08-03 15:42:00: step 38620/200000, loss = 0.120153 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:42:01 INFO: 2022-08-03 15:42:01: step 38640/200000, loss = 0.423289 (0.040 sec/batch), lr: 0.001563
2022-08-03 15:42:02 INFO: 2022-08-03 15:42:02: step 38660/200000, loss = 0.468211 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:42:02 INFO: 2022-08-03 15:42:02: step 38680/200000, loss = 0.110220 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:42:03 INFO: 2022-08-03 15:42:03: step 38700/200000, loss = 0.182581 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:42:04 INFO: 2022-08-03 15:42:04: step 38720/200000, loss = 0.070472 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:42:05 INFO: 2022-08-03 15:42:05: step 38740/200000, loss = 0.243545 (0.033 sec/batch), lr: 0.001563
2022-08-03 15:42:06 INFO: 2022-08-03 15:42:06: step 38760/200000, loss = 0.184237 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:42:07 INFO: 2022-08-03 15:42:07: step 38780/200000, loss = 0.124180 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:42:08 INFO: 2022-08-03 15:42:08: step 38800/200000, loss = 0.108855 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:09 INFO: 2022-08-03 15:42:09: step 38820/200000, loss = 0.048749 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:42:10 INFO: 2022-08-03 15:42:10: step 38840/200000, loss = 0.115097 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:42:11 INFO: 2022-08-03 15:42:11: step 38860/200000, loss = 0.144555 (0.041 sec/batch), lr: 0.001563
2022-08-03 15:42:11 INFO: 2022-08-03 15:42:11: step 38880/200000, loss = 0.093297 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:12 INFO: 2022-08-03 15:42:12: step 38900/200000, loss = 0.220132 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:42:13 INFO: 2022-08-03 15:42:13: step 38920/200000, loss = 0.301309 (0.043 sec/batch), lr: 0.001563
2022-08-03 15:42:14 INFO: 2022-08-03 15:42:14: step 38940/200000, loss = 0.288229 (0.040 sec/batch), lr: 0.001563
2022-08-03 15:42:15 INFO: 2022-08-03 15:42:15: step 38960/200000, loss = 0.142043 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:42:16 INFO: 2022-08-03 15:42:16: step 38980/200000, loss = 0.090212 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:42:17 INFO: 2022-08-03 15:42:17: step 39000/200000, loss = 0.202180 (0.055 sec/batch), lr: 0.001563
2022-08-03 15:42:17 INFO: Evaluating on dev set...
2022-08-03 15:42:41 INFO: Score by entity:
Prec.	Rec.	F1
94.61	93.43	94.02
2022-08-03 15:42:41 INFO: step 39000: train_loss = 0.227946, dev_score = 0.9402
2022-08-03 15:42:41 INFO: 
2022-08-03 15:42:42 INFO: 2022-08-03 15:42:42: step 39020/200000, loss = 0.217942 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:42:43 INFO: 2022-08-03 15:42:43: step 39040/200000, loss = 0.087383 (0.031 sec/batch), lr: 0.001563
2022-08-03 15:42:44 INFO: 2022-08-03 15:42:44: step 39060/200000, loss = 0.265061 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:45 INFO: 2022-08-03 15:42:45: step 39080/200000, loss = 0.159468 (0.042 sec/batch), lr: 0.001563
2022-08-03 15:42:46 INFO: 2022-08-03 15:42:46: step 39100/200000, loss = 0.374484 (0.042 sec/batch), lr: 0.001563
2022-08-03 15:42:47 INFO: 2022-08-03 15:42:47: step 39120/200000, loss = 0.054152 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:42:48 INFO: 2022-08-03 15:42:48: step 39140/200000, loss = 0.213528 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:42:48 INFO: 2022-08-03 15:42:48: step 39160/200000, loss = 0.097258 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:42:49 INFO: 2022-08-03 15:42:49: step 39180/200000, loss = 0.182795 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:50 INFO: 2022-08-03 15:42:50: step 39200/200000, loss = 0.279081 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:42:51 INFO: 2022-08-03 15:42:51: step 39220/200000, loss = 0.075023 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:52 INFO: 2022-08-03 15:42:52: step 39240/200000, loss = 0.241732 (0.033 sec/batch), lr: 0.001563
2022-08-03 15:42:53 INFO: 2022-08-03 15:42:53: step 39260/200000, loss = 0.074009 (0.043 sec/batch), lr: 0.001563
2022-08-03 15:42:54 INFO: 2022-08-03 15:42:54: step 39280/200000, loss = 0.078402 (0.042 sec/batch), lr: 0.001563
2022-08-03 15:42:55 INFO: 2022-08-03 15:42:55: step 39300/200000, loss = 0.162536 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:56 INFO: 2022-08-03 15:42:56: step 39320/200000, loss = 0.053605 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:42:57 INFO: 2022-08-03 15:42:57: step 39340/200000, loss = 0.430551 (0.051 sec/batch), lr: 0.001563
2022-08-03 15:42:57 INFO: 2022-08-03 15:42:57: step 39360/200000, loss = 0.389736 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:42:58 INFO: 2022-08-03 15:42:58: step 39380/200000, loss = 0.074436 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:42:59 INFO: 2022-08-03 15:42:59: step 39400/200000, loss = 0.223350 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:00 INFO: 2022-08-03 15:43:00: step 39420/200000, loss = 0.392372 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:43:01 INFO: 2022-08-03 15:43:01: step 39440/200000, loss = 0.065896 (0.041 sec/batch), lr: 0.001563
2022-08-03 15:43:02 INFO: 2022-08-03 15:43:02: step 39460/200000, loss = 0.253487 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:43:03 INFO: 2022-08-03 15:43:03: step 39480/200000, loss = 0.129588 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:43:04 INFO: 2022-08-03 15:43:04: step 39500/200000, loss = 0.071304 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:43:04 INFO: Evaluating on dev set...
2022-08-03 15:43:28 INFO: Score by entity:
Prec.	Rec.	F1
94.65	93.46	94.05
2022-08-03 15:43:28 INFO: step 39500: train_loss = 0.228920, dev_score = 0.9405
2022-08-03 15:43:28 INFO: 
2022-08-03 15:43:29 INFO: 2022-08-03 15:43:29: step 39520/200000, loss = 0.365260 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:43:30 INFO: 2022-08-03 15:43:30: step 39540/200000, loss = 0.332698 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:43:31 INFO: 2022-08-03 15:43:31: step 39560/200000, loss = 0.225380 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:43:32 INFO: 2022-08-03 15:43:32: step 39580/200000, loss = 0.247434 (0.041 sec/batch), lr: 0.001563
2022-08-03 15:43:33 INFO: 2022-08-03 15:43:33: step 39600/200000, loss = 0.180381 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:43:34 INFO: 2022-08-03 15:43:34: step 39620/200000, loss = 0.304794 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:34 INFO: 2022-08-03 15:43:34: step 39640/200000, loss = 0.052521 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:43:35 INFO: 2022-08-03 15:43:35: step 39660/200000, loss = 0.208115 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:36 INFO: 2022-08-03 15:43:36: step 39680/200000, loss = 0.138936 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:43:37 INFO: 2022-08-03 15:43:37: step 39700/200000, loss = 0.063531 (0.040 sec/batch), lr: 0.001563
2022-08-03 15:43:38 INFO: 2022-08-03 15:43:38: step 39720/200000, loss = 0.601635 (0.047 sec/batch), lr: 0.001563
2022-08-03 15:43:39 INFO: 2022-08-03 15:43:39: step 39740/200000, loss = 0.118479 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:40 INFO: 2022-08-03 15:43:40: step 39760/200000, loss = 0.060378 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:41 INFO: 2022-08-03 15:43:41: step 39780/200000, loss = 0.086661 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:43:42 INFO: 2022-08-03 15:43:42: step 39800/200000, loss = 0.301637 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:43:42 INFO: 2022-08-03 15:43:42: step 39820/200000, loss = 0.195992 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:43:43 INFO: 2022-08-03 15:43:43: step 39840/200000, loss = 0.351268 (0.040 sec/batch), lr: 0.001563
2022-08-03 15:43:44 INFO: 2022-08-03 15:43:44: step 39860/200000, loss = 0.063429 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:45 INFO: 2022-08-03 15:43:45: step 39880/200000, loss = 0.174666 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:43:46 INFO: 2022-08-03 15:43:46: step 39900/200000, loss = 0.188855 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:43:47 INFO: 2022-08-03 15:43:47: step 39920/200000, loss = 0.097184 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:43:48 INFO: 2022-08-03 15:43:48: step 39940/200000, loss = 0.173521 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:43:49 INFO: 2022-08-03 15:43:49: step 39960/200000, loss = 0.262116 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:43:50 INFO: 2022-08-03 15:43:50: step 39980/200000, loss = 0.036102 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:43:50 INFO: 2022-08-03 15:43:50: step 40000/200000, loss = 0.073432 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:43:50 INFO: Evaluating on dev set...
2022-08-03 15:44:15 INFO: Score by entity:
Prec.	Rec.	F1
94.63	93.47	94.05
2022-08-03 15:44:15 INFO: step 40000: train_loss = 0.224087, dev_score = 0.9405
2022-08-03 15:44:15 INFO: 
2022-08-03 15:44:16 INFO: 2022-08-03 15:44:16: step 40020/200000, loss = 0.069101 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:44:17 INFO: 2022-08-03 15:44:17: step 40040/200000, loss = 0.461316 (0.034 sec/batch), lr: 0.001563
2022-08-03 15:44:18 INFO: 2022-08-03 15:44:18: step 40060/200000, loss = 0.186098 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:44:18 INFO: 2022-08-03 15:44:18: step 40080/200000, loss = 0.332105 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:44:19 INFO: 2022-08-03 15:44:19: step 40100/200000, loss = 0.263441 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:44:20 INFO: 2022-08-03 15:44:20: step 40120/200000, loss = 0.108015 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:44:21 INFO: 2022-08-03 15:44:21: step 40140/200000, loss = 0.209858 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:44:22 INFO: 2022-08-03 15:44:22: step 40160/200000, loss = 0.243214 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:44:23 INFO: 2022-08-03 15:44:23: step 40180/200000, loss = 0.160596 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:44:24 INFO: 2022-08-03 15:44:24: step 40200/200000, loss = 0.088191 (0.032 sec/batch), lr: 0.001563
2022-08-03 15:44:25 INFO: 2022-08-03 15:44:25: step 40220/200000, loss = 0.310090 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:44:26 INFO: 2022-08-03 15:44:26: step 40240/200000, loss = 0.118483 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:44:26 INFO: 2022-08-03 15:44:26: step 40260/200000, loss = 0.087724 (0.040 sec/batch), lr: 0.001563
2022-08-03 15:44:27 INFO: 2022-08-03 15:44:27: step 40280/200000, loss = 0.192171 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:44:28 INFO: 2022-08-03 15:44:28: step 40300/200000, loss = 0.553751 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:44:29 INFO: 2022-08-03 15:44:29: step 40320/200000, loss = 0.111593 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:44:30 INFO: 2022-08-03 15:44:30: step 40340/200000, loss = 0.289222 (0.035 sec/batch), lr: 0.001563
2022-08-03 15:44:31 INFO: 2022-08-03 15:44:31: step 40360/200000, loss = 0.205182 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:44:32 INFO: 2022-08-03 15:44:32: step 40380/200000, loss = 0.084029 (0.050 sec/batch), lr: 0.001563
2022-08-03 15:44:33 INFO: 2022-08-03 15:44:33: step 40400/200000, loss = 0.371025 (0.036 sec/batch), lr: 0.001563
2022-08-03 15:44:34 INFO: 2022-08-03 15:44:34: step 40420/200000, loss = 0.020712 (0.043 sec/batch), lr: 0.001563
2022-08-03 15:44:34 INFO: 2022-08-03 15:44:34: step 40440/200000, loss = 0.202787 (0.039 sec/batch), lr: 0.001563
2022-08-03 15:44:35 INFO: 2022-08-03 15:44:35: step 40460/200000, loss = 0.040485 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:44:36 INFO: 2022-08-03 15:44:36: step 40480/200000, loss = 0.413018 (0.038 sec/batch), lr: 0.001563
2022-08-03 15:44:37 INFO: 2022-08-03 15:44:37: step 40500/200000, loss = 0.153215 (0.037 sec/batch), lr: 0.001563
2022-08-03 15:44:37 INFO: Evaluating on dev set...
2022-08-03 15:45:02 INFO: Score by entity:
Prec.	Rec.	F1
94.71	93.47	94.08
2022-08-03 15:45:02 INFO: step 40500: train_loss = 0.228649, dev_score = 0.9408
2022-08-03 15:45:02 INFO: 
Epoch 00081: reducing learning rate of group 0 to 7.8125e-04.
2022-08-03 15:45:02 INFO: 2022-08-03 15:45:02: step 40520/200000, loss = 0.328287 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:45:03 INFO: 2022-08-03 15:45:03: step 40540/200000, loss = 0.327703 (0.041 sec/batch), lr: 0.000781
2022-08-03 15:45:04 INFO: 2022-08-03 15:45:04: step 40560/200000, loss = 0.367027 (0.046 sec/batch), lr: 0.000781
2022-08-03 15:45:05 INFO: 2022-08-03 15:45:05: step 40580/200000, loss = 0.211467 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:45:06 INFO: 2022-08-03 15:45:06: step 40600/200000, loss = 0.802967 (0.043 sec/batch), lr: 0.000781
2022-08-03 15:45:07 INFO: 2022-08-03 15:45:07: step 40620/200000, loss = 0.340520 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:08 INFO: 2022-08-03 15:45:08: step 40640/200000, loss = 0.322592 (0.040 sec/batch), lr: 0.000781
2022-08-03 15:45:09 INFO: 2022-08-03 15:45:09: step 40660/200000, loss = 0.510311 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:45:10 INFO: 2022-08-03 15:45:10: step 40680/200000, loss = 0.180898 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:10 INFO: 2022-08-03 15:45:10: step 40700/200000, loss = 0.093891 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:45:11 INFO: 2022-08-03 15:45:11: step 40720/200000, loss = 0.386282 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:12 INFO: 2022-08-03 15:45:12: step 40740/200000, loss = 0.317023 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:45:13 INFO: 2022-08-03 15:45:13: step 40760/200000, loss = 0.221958 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:45:14 INFO: 2022-08-03 15:45:14: step 40780/200000, loss = 0.050925 (0.045 sec/batch), lr: 0.000781
2022-08-03 15:45:15 INFO: 2022-08-03 15:45:15: step 40800/200000, loss = 0.094670 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:45:16 INFO: 2022-08-03 15:45:16: step 40820/200000, loss = 0.145301 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:45:17 INFO: 2022-08-03 15:45:17: step 40840/200000, loss = 0.270121 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:18 INFO: 2022-08-03 15:45:18: step 40860/200000, loss = 0.260730 (0.040 sec/batch), lr: 0.000781
2022-08-03 15:45:18 INFO: 2022-08-03 15:45:18: step 40880/200000, loss = 0.193133 (0.044 sec/batch), lr: 0.000781
2022-08-03 15:45:19 INFO: 2022-08-03 15:45:19: step 40900/200000, loss = 0.242210 (0.045 sec/batch), lr: 0.000781
2022-08-03 15:45:20 INFO: 2022-08-03 15:45:20: step 40920/200000, loss = 0.166479 (0.052 sec/batch), lr: 0.000781
2022-08-03 15:45:21 INFO: 2022-08-03 15:45:21: step 40940/200000, loss = 0.690349 (0.041 sec/batch), lr: 0.000781
2022-08-03 15:45:22 INFO: 2022-08-03 15:45:22: step 40960/200000, loss = 0.104940 (0.040 sec/batch), lr: 0.000781
2022-08-03 15:45:23 INFO: 2022-08-03 15:45:23: step 40980/200000, loss = 0.438606 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:45:24 INFO: 2022-08-03 15:45:24: step 41000/200000, loss = 0.862914 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:45:24 INFO: Evaluating on dev set...
2022-08-03 15:45:48 INFO: Score by entity:
Prec.	Rec.	F1
94.67	93.44	94.05
2022-08-03 15:45:48 INFO: step 41000: train_loss = 0.233469, dev_score = 0.9405
2022-08-03 15:45:48 INFO: 
2022-08-03 15:45:49 INFO: 2022-08-03 15:45:49: step 41020/200000, loss = 0.350844 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:45:50 INFO: 2022-08-03 15:45:50: step 41040/200000, loss = 0.455701 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:45:51 INFO: 2022-08-03 15:45:51: step 41060/200000, loss = 0.394428 (0.040 sec/batch), lr: 0.000781
2022-08-03 15:45:52 INFO: 2022-08-03 15:45:52: step 41080/200000, loss = 0.072781 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:53 INFO: 2022-08-03 15:45:53: step 41100/200000, loss = 0.131018 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:54 INFO: 2022-08-03 15:45:54: step 41120/200000, loss = 0.210757 (0.045 sec/batch), lr: 0.000781
2022-08-03 15:45:55 INFO: 2022-08-03 15:45:55: step 41140/200000, loss = 0.146157 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:45:55 INFO: 2022-08-03 15:45:55: step 41160/200000, loss = 0.144993 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:45:56 INFO: 2022-08-03 15:45:56: step 41180/200000, loss = 0.036076 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:45:57 INFO: 2022-08-03 15:45:57: step 41200/200000, loss = 0.123367 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:45:58 INFO: 2022-08-03 15:45:58: step 41220/200000, loss = 0.490510 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:45:59 INFO: 2022-08-03 15:45:59: step 41240/200000, loss = 0.036379 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:46:00 INFO: 2022-08-03 15:46:00: step 41260/200000, loss = 0.539258 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:46:01 INFO: 2022-08-03 15:46:01: step 41280/200000, loss = 0.191255 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:02 INFO: 2022-08-03 15:46:02: step 41300/200000, loss = 0.326037 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:46:02 INFO: 2022-08-03 15:46:02: step 41320/200000, loss = 0.120737 (0.040 sec/batch), lr: 0.000781
2022-08-03 15:46:03 INFO: 2022-08-03 15:46:03: step 41340/200000, loss = 0.194020 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:46:04 INFO: 2022-08-03 15:46:04: step 41360/200000, loss = 0.501678 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:05 INFO: 2022-08-03 15:46:05: step 41380/200000, loss = 0.264292 (0.044 sec/batch), lr: 0.000781
2022-08-03 15:46:06 INFO: 2022-08-03 15:46:06: step 41400/200000, loss = 0.037960 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:07 INFO: 2022-08-03 15:46:07: step 41420/200000, loss = 0.076466 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:46:08 INFO: 2022-08-03 15:46:08: step 41440/200000, loss = 0.329965 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:46:09 INFO: 2022-08-03 15:46:09: step 41460/200000, loss = 0.276954 (0.043 sec/batch), lr: 0.000781
2022-08-03 15:46:10 INFO: 2022-08-03 15:46:10: step 41480/200000, loss = 0.222558 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:46:10 INFO: 2022-08-03 15:46:10: step 41500/200000, loss = 0.136365 (0.032 sec/batch), lr: 0.000781
2022-08-03 15:46:10 INFO: Evaluating on dev set...
2022-08-03 15:46:34 INFO: Score by entity:
Prec.	Rec.	F1
94.65	93.47	94.06
2022-08-03 15:46:34 INFO: step 41500: train_loss = 0.228766, dev_score = 0.9406
2022-08-03 15:46:34 INFO: 
2022-08-03 15:46:35 INFO: 2022-08-03 15:46:35: step 41520/200000, loss = 0.217741 (0.030 sec/batch), lr: 0.000781
2022-08-03 15:46:36 INFO: 2022-08-03 15:46:36: step 41540/200000, loss = 0.235750 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:46:36 INFO: 2022-08-03 15:46:36: step 41560/200000, loss = 0.049489 (0.030 sec/batch), lr: 0.000781
2022-08-03 15:46:37 INFO: 2022-08-03 15:46:37: step 41580/200000, loss = 0.156386 (0.047 sec/batch), lr: 0.000781
2022-08-03 15:46:38 INFO: 2022-08-03 15:46:38: step 41600/200000, loss = 0.130608 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:46:39 INFO: 2022-08-03 15:46:39: step 41620/200000, loss = 0.609997 (0.032 sec/batch), lr: 0.000781
2022-08-03 15:46:40 INFO: 2022-08-03 15:46:40: step 41640/200000, loss = 0.091662 (0.032 sec/batch), lr: 0.000781
2022-08-03 15:46:41 INFO: 2022-08-03 15:46:41: step 41660/200000, loss = 0.446776 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:46:41 INFO: 2022-08-03 15:46:41: step 41680/200000, loss = 0.330259 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:42 INFO: 2022-08-03 15:46:42: step 41700/200000, loss = 0.349681 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:46:43 INFO: 2022-08-03 15:46:43: step 41720/200000, loss = 0.288527 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:46:44 INFO: 2022-08-03 15:46:44: step 41740/200000, loss = 0.157292 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:45 INFO: 2022-08-03 15:46:45: step 41760/200000, loss = 0.190134 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:46 INFO: 2022-08-03 15:46:46: step 41780/200000, loss = 0.495314 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:46:46 INFO: 2022-08-03 15:46:46: step 41800/200000, loss = 0.526976 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:47 INFO: 2022-08-03 15:46:47: step 41820/200000, loss = 0.057890 (0.030 sec/batch), lr: 0.000781
2022-08-03 15:46:48 INFO: 2022-08-03 15:46:48: step 41840/200000, loss = 0.371489 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:49 INFO: 2022-08-03 15:46:49: step 41860/200000, loss = 0.160580 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:50 INFO: 2022-08-03 15:46:50: step 41880/200000, loss = 0.141072 (0.037 sec/batch), lr: 0.000781
2022-08-03 15:46:50 INFO: 2022-08-03 15:46:50: step 41900/200000, loss = 0.107549 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:51 INFO: 2022-08-03 15:46:51: step 41920/200000, loss = 0.395036 (0.047 sec/batch), lr: 0.000781
2022-08-03 15:46:52 INFO: 2022-08-03 15:46:52: step 41940/200000, loss = 0.280849 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:53 INFO: 2022-08-03 15:46:53: step 41960/200000, loss = 0.090566 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:46:54 INFO: 2022-08-03 15:46:54: step 41980/200000, loss = 0.163644 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:46:55 INFO: 2022-08-03 15:46:55: step 42000/200000, loss = 0.273087 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:46:55 INFO: Evaluating on dev set...
2022-08-03 15:47:17 INFO: Score by entity:
Prec.	Rec.	F1
94.74	93.47	94.10
2022-08-03 15:47:17 INFO: step 42000: train_loss = 0.238527, dev_score = 0.9410
2022-08-03 15:47:17 INFO: 
2022-08-03 15:47:18 INFO: 2022-08-03 15:47:18: step 42020/200000, loss = 0.324214 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:19 INFO: 2022-08-03 15:47:19: step 42040/200000, loss = 0.132944 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:47:20 INFO: 2022-08-03 15:47:20: step 42060/200000, loss = 0.250120 (0.046 sec/batch), lr: 0.000781
2022-08-03 15:47:21 INFO: 2022-08-03 15:47:21: step 42080/200000, loss = 0.218551 (0.043 sec/batch), lr: 0.000781
2022-08-03 15:47:22 INFO: 2022-08-03 15:47:22: step 42100/200000, loss = 0.597796 (0.074 sec/batch), lr: 0.000781
2022-08-03 15:47:22 INFO: 2022-08-03 15:47:22: step 42120/200000, loss = 0.269448 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:23 INFO: 2022-08-03 15:47:23: step 42140/200000, loss = 0.048631 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:47:24 INFO: 2022-08-03 15:47:24: step 42160/200000, loss = 0.147324 (0.035 sec/batch), lr: 0.000781
2022-08-03 15:47:25 INFO: 2022-08-03 15:47:25: step 42180/200000, loss = 0.748491 (0.043 sec/batch), lr: 0.000781
2022-08-03 15:47:26 INFO: 2022-08-03 15:47:26: step 42200/200000, loss = 0.204627 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:27 INFO: 2022-08-03 15:47:27: step 42220/200000, loss = 0.061650 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:47:28 INFO: 2022-08-03 15:47:28: step 42240/200000, loss = 0.096497 (0.038 sec/batch), lr: 0.000781
2022-08-03 15:47:28 INFO: 2022-08-03 15:47:28: step 42260/200000, loss = 0.235762 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:29 INFO: 2022-08-03 15:47:29: step 42280/200000, loss = 0.036964 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:30 INFO: 2022-08-03 15:47:30: step 42300/200000, loss = 0.141214 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:31 INFO: 2022-08-03 15:47:31: step 42320/200000, loss = 0.057192 (0.044 sec/batch), lr: 0.000781
2022-08-03 15:47:32 INFO: 2022-08-03 15:47:32: step 42340/200000, loss = 0.214943 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:47:33 INFO: 2022-08-03 15:47:33: step 42360/200000, loss = 0.239467 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:47:34 INFO: 2022-08-03 15:47:34: step 42380/200000, loss = 0.245109 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:47:35 INFO: 2022-08-03 15:47:35: step 42400/200000, loss = 0.214257 (0.044 sec/batch), lr: 0.000781
2022-08-03 15:47:36 INFO: 2022-08-03 15:47:36: step 42420/200000, loss = 0.073320 (0.036 sec/batch), lr: 0.000781
2022-08-03 15:47:37 INFO: 2022-08-03 15:47:37: step 42440/200000, loss = 0.196777 (0.039 sec/batch), lr: 0.000781
2022-08-03 15:47:37 INFO: 2022-08-03 15:47:37: step 42460/200000, loss = 0.087266 (0.034 sec/batch), lr: 0.000781
2022-08-03 15:47:38 INFO: 2022-08-03 15:47:38: step 42480/200000, loss = 0.181857 (0.042 sec/batch), lr: 0.000781
2022-08-03 15:47:39 INFO: 2022-08-03 15:47:39: step 42500/200000, loss = 0.248015 (0.033 sec/batch), lr: 0.000781
2022-08-03 15:47:39 INFO: Evaluating on dev set...
2022-08-03 15:48:04 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.44	94.06
2022-08-03 15:48:04 INFO: step 42500: train_loss = 0.230325, dev_score = 0.9406
2022-08-03 15:48:04 INFO: 
Epoch 00085: reducing learning rate of group 0 to 3.9063e-04.
2022-08-03 15:48:05 INFO: 2022-08-03 15:48:05: step 42520/200000, loss = 0.113054 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:48:05 INFO: 2022-08-03 15:48:05: step 42540/200000, loss = 0.396566 (0.031 sec/batch), lr: 0.000391
2022-08-03 15:48:06 INFO: 2022-08-03 15:48:06: step 42560/200000, loss = 2.067606 (0.063 sec/batch), lr: 0.000391
2022-08-03 15:48:07 INFO: 2022-08-03 15:48:07: step 42580/200000, loss = 0.213167 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:48:08 INFO: 2022-08-03 15:48:08: step 42600/200000, loss = 0.116750 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:09 INFO: 2022-08-03 15:48:09: step 42620/200000, loss = 0.228010 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:10 INFO: 2022-08-03 15:48:10: step 42640/200000, loss = 0.290650 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:48:11 INFO: 2022-08-03 15:48:11: step 42660/200000, loss = 0.398140 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:48:12 INFO: 2022-08-03 15:48:12: step 42680/200000, loss = 0.494891 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:48:13 INFO: 2022-08-03 15:48:13: step 42700/200000, loss = 0.214489 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:13 INFO: 2022-08-03 15:48:13: step 42720/200000, loss = 0.404529 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:48:14 INFO: 2022-08-03 15:48:14: step 42740/200000, loss = 0.035287 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:15 INFO: 2022-08-03 15:48:15: step 42760/200000, loss = 0.100584 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:16 INFO: 2022-08-03 15:48:16: step 42780/200000, loss = 0.509106 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:17 INFO: 2022-08-03 15:48:17: step 42800/200000, loss = 0.373453 (0.049 sec/batch), lr: 0.000391
2022-08-03 15:48:18 INFO: 2022-08-03 15:48:18: step 42820/200000, loss = 0.126506 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:48:19 INFO: 2022-08-03 15:48:19: step 42840/200000, loss = 0.327184 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:20 INFO: 2022-08-03 15:48:20: step 42860/200000, loss = 0.189297 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:48:21 INFO: 2022-08-03 15:48:21: step 42880/200000, loss = 0.310467 (0.033 sec/batch), lr: 0.000391
2022-08-03 15:48:21 INFO: 2022-08-03 15:48:21: step 42900/200000, loss = 0.273417 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:48:22 INFO: 2022-08-03 15:48:22: step 42920/200000, loss = 0.079409 (0.033 sec/batch), lr: 0.000391
2022-08-03 15:48:23 INFO: 2022-08-03 15:48:23: step 42940/200000, loss = 0.171199 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:48:24 INFO: 2022-08-03 15:48:24: step 42960/200000, loss = 0.176562 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:25 INFO: 2022-08-03 15:48:25: step 42980/200000, loss = 0.422022 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:26 INFO: 2022-08-03 15:48:26: step 43000/200000, loss = 0.245734 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:48:26 INFO: Evaluating on dev set...
2022-08-03 15:48:51 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.44	94.06
2022-08-03 15:48:51 INFO: step 43000: train_loss = 0.244043, dev_score = 0.9406
2022-08-03 15:48:51 INFO: 
2022-08-03 15:48:51 INFO: 2022-08-03 15:48:51: step 43020/200000, loss = 0.472305 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:48:52 INFO: 2022-08-03 15:48:52: step 43040/200000, loss = 0.256959 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:53 INFO: 2022-08-03 15:48:53: step 43060/200000, loss = 0.110039 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:48:54 INFO: 2022-08-03 15:48:54: step 43080/200000, loss = 0.361242 (0.041 sec/batch), lr: 0.000391
2022-08-03 15:48:55 INFO: 2022-08-03 15:48:55: step 43100/200000, loss = 0.231580 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:48:56 INFO: 2022-08-03 15:48:56: step 43120/200000, loss = 0.334878 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:48:57 INFO: 2022-08-03 15:48:57: step 43140/200000, loss = 0.209605 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:48:58 INFO: 2022-08-03 15:48:58: step 43160/200000, loss = 0.120203 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:48:59 INFO: 2022-08-03 15:48:59: step 43180/200000, loss = 0.223410 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:48:59 INFO: 2022-08-03 15:48:59: step 43200/200000, loss = 0.035294 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:00 INFO: 2022-08-03 15:49:00: step 43220/200000, loss = 0.387112 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:01 INFO: 2022-08-03 15:49:01: step 43240/200000, loss = 0.157511 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:49:02 INFO: 2022-08-03 15:49:02: step 43260/200000, loss = 0.202765 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:03 INFO: 2022-08-03 15:49:03: step 43280/200000, loss = 0.554490 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:49:04 INFO: 2022-08-03 15:49:04: step 43300/200000, loss = 0.467888 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:49:05 INFO: 2022-08-03 15:49:05: step 43320/200000, loss = 0.181539 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:49:06 INFO: 2022-08-03 15:49:06: step 43340/200000, loss = 0.106468 (0.033 sec/batch), lr: 0.000391
2022-08-03 15:49:06 INFO: 2022-08-03 15:49:06: step 43360/200000, loss = 0.182623 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:49:07 INFO: 2022-08-03 15:49:07: step 43380/200000, loss = 0.329370 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:08 INFO: 2022-08-03 15:49:08: step 43400/200000, loss = 0.114496 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:49:09 INFO: 2022-08-03 15:49:09: step 43420/200000, loss = 0.033425 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:49:10 INFO: 2022-08-03 15:49:10: step 43440/200000, loss = 0.140759 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:49:11 INFO: 2022-08-03 15:49:11: step 43460/200000, loss = 0.595850 (0.032 sec/batch), lr: 0.000391
2022-08-03 15:49:12 INFO: 2022-08-03 15:49:12: step 43480/200000, loss = 0.100421 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:13 INFO: 2022-08-03 15:49:13: step 43500/200000, loss = 0.058270 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:49:13 INFO: Evaluating on dev set...
2022-08-03 15:49:37 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.43	94.06
2022-08-03 15:49:37 INFO: step 43500: train_loss = 0.232397, dev_score = 0.9406
2022-08-03 15:49:37 INFO: 
2022-08-03 15:49:38 INFO: 2022-08-03 15:49:38: step 43520/200000, loss = 0.426540 (0.043 sec/batch), lr: 0.000391
2022-08-03 15:49:39 INFO: 2022-08-03 15:49:39: step 43540/200000, loss = 0.397553 (0.047 sec/batch), lr: 0.000391
2022-08-03 15:49:40 INFO: 2022-08-03 15:49:40: step 43560/200000, loss = 0.296126 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:41 INFO: 2022-08-03 15:49:41: step 43580/200000, loss = 0.244391 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:49:42 INFO: 2022-08-03 15:49:42: step 43600/200000, loss = 0.049888 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:49:43 INFO: 2022-08-03 15:49:43: step 43620/200000, loss = 0.404315 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:44 INFO: 2022-08-03 15:49:44: step 43640/200000, loss = 0.198410 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:49:44 INFO: 2022-08-03 15:49:44: step 43660/200000, loss = 0.051138 (0.044 sec/batch), lr: 0.000391
2022-08-03 15:49:45 INFO: 2022-08-03 15:49:45: step 43680/200000, loss = 0.138143 (0.047 sec/batch), lr: 0.000391
2022-08-03 15:49:46 INFO: 2022-08-03 15:49:46: step 43700/200000, loss = 0.124970 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:47 INFO: 2022-08-03 15:49:47: step 43720/200000, loss = 0.146962 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:49:48 INFO: 2022-08-03 15:49:48: step 43740/200000, loss = 0.095537 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:49 INFO: 2022-08-03 15:49:49: step 43760/200000, loss = 0.125446 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:49:50 INFO: 2022-08-03 15:49:50: step 43780/200000, loss = 0.119990 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:49:51 INFO: 2022-08-03 15:49:51: step 43800/200000, loss = 0.146813 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:51 INFO: 2022-08-03 15:49:51: step 43820/200000, loss = 0.428745 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:52 INFO: 2022-08-03 15:49:52: step 43840/200000, loss = 0.157192 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:49:53 INFO: 2022-08-03 15:49:53: step 43860/200000, loss = 0.157274 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:49:54 INFO: 2022-08-03 15:49:54: step 43880/200000, loss = 0.056226 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:49:55 INFO: 2022-08-03 15:49:55: step 43900/200000, loss = 0.357089 (0.045 sec/batch), lr: 0.000391
2022-08-03 15:49:56 INFO: 2022-08-03 15:49:56: step 43920/200000, loss = 0.096070 (0.033 sec/batch), lr: 0.000391
2022-08-03 15:49:57 INFO: 2022-08-03 15:49:57: step 43940/200000, loss = 0.064342 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:49:58 INFO: 2022-08-03 15:49:58: step 43960/200000, loss = 0.333028 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:49:59 INFO: 2022-08-03 15:49:59: step 43980/200000, loss = 0.597569 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:49:59 INFO: 2022-08-03 15:49:59: step 44000/200000, loss = 0.083000 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:49:59 INFO: Evaluating on dev set...
2022-08-03 15:50:24 INFO: Score by entity:
Prec.	Rec.	F1
94.71	93.44	94.07
2022-08-03 15:50:24 INFO: step 44000: train_loss = 0.218513, dev_score = 0.9407
2022-08-03 15:50:24 INFO: 
2022-08-03 15:50:25 INFO: 2022-08-03 15:50:25: step 44020/200000, loss = 0.218290 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:50:26 INFO: 2022-08-03 15:50:26: step 44040/200000, loss = 0.123369 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:50:26 INFO: 2022-08-03 15:50:26: step 44060/200000, loss = 0.210631 (0.036 sec/batch), lr: 0.000391
2022-08-03 15:50:27 INFO: 2022-08-03 15:50:27: step 44080/200000, loss = 0.684672 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:50:28 INFO: 2022-08-03 15:50:28: step 44100/200000, loss = 0.243391 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:50:29 INFO: 2022-08-03 15:50:29: step 44120/200000, loss = 0.309256 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:50:30 INFO: 2022-08-03 15:50:30: step 44140/200000, loss = 0.076328 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:50:31 INFO: 2022-08-03 15:50:31: step 44160/200000, loss = 0.135610 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:50:32 INFO: 2022-08-03 15:50:32: step 44180/200000, loss = 0.059974 (0.033 sec/batch), lr: 0.000391
2022-08-03 15:50:33 INFO: 2022-08-03 15:50:33: step 44200/200000, loss = 0.266493 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:50:34 INFO: 2022-08-03 15:50:34: step 44220/200000, loss = 0.053890 (0.037 sec/batch), lr: 0.000391
2022-08-03 15:50:34 INFO: 2022-08-03 15:50:34: step 44240/200000, loss = 0.239778 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:50:35 INFO: 2022-08-03 15:50:35: step 44260/200000, loss = 0.039575 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:50:36 INFO: 2022-08-03 15:50:36: step 44280/200000, loss = 0.097595 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:50:37 INFO: 2022-08-03 15:50:37: step 44300/200000, loss = 0.116539 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:50:38 INFO: 2022-08-03 15:50:38: step 44320/200000, loss = 0.188355 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:50:39 INFO: 2022-08-03 15:50:39: step 44340/200000, loss = 0.089570 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:50:40 INFO: 2022-08-03 15:50:40: step 44360/200000, loss = 0.195927 (0.048 sec/batch), lr: 0.000391
2022-08-03 15:50:41 INFO: 2022-08-03 15:50:41: step 44380/200000, loss = 0.044853 (0.042 sec/batch), lr: 0.000391
2022-08-03 15:50:42 INFO: 2022-08-03 15:50:42: step 44400/200000, loss = 0.225157 (0.038 sec/batch), lr: 0.000391
2022-08-03 15:50:43 INFO: 2022-08-03 15:50:43: step 44420/200000, loss = 0.285142 (0.040 sec/batch), lr: 0.000391
2022-08-03 15:50:43 INFO: 2022-08-03 15:50:43: step 44440/200000, loss = 0.025148 (0.039 sec/batch), lr: 0.000391
2022-08-03 15:50:44 INFO: 2022-08-03 15:50:44: step 44460/200000, loss = 0.343874 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:50:45 INFO: 2022-08-03 15:50:45: step 44480/200000, loss = 0.110337 (0.035 sec/batch), lr: 0.000391
2022-08-03 15:50:46 INFO: 2022-08-03 15:50:46: step 44500/200000, loss = 0.206769 (0.034 sec/batch), lr: 0.000391
2022-08-03 15:50:46 INFO: Evaluating on dev set...
2022-08-03 15:51:11 INFO: Score by entity:
Prec.	Rec.	F1
94.70	93.45	94.07
2022-08-03 15:51:11 INFO: step 44500: train_loss = 0.231266, dev_score = 0.9407
2022-08-03 15:51:11 INFO: 
Epoch 00089: reducing learning rate of group 0 to 1.9531e-04.
2022-08-03 15:51:11 INFO: 2022-08-03 15:51:11: step 44520/200000, loss = 0.298269 (0.033 sec/batch), lr: 0.000195
2022-08-03 15:51:12 INFO: 2022-08-03 15:51:12: step 44540/200000, loss = 0.190332 (0.040 sec/batch), lr: 0.000195
2022-08-03 15:51:13 INFO: 2022-08-03 15:51:13: step 44560/200000, loss = 0.067070 (0.042 sec/batch), lr: 0.000195
2022-08-03 15:51:14 INFO: 2022-08-03 15:51:14: step 44580/200000, loss = 0.260369 (0.031 sec/batch), lr: 0.000195
2022-08-03 15:51:15 INFO: 2022-08-03 15:51:15: step 44600/200000, loss = 0.266881 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:16 INFO: 2022-08-03 15:51:16: step 44620/200000, loss = 0.260561 (0.043 sec/batch), lr: 0.000195
2022-08-03 15:51:17 INFO: 2022-08-03 15:51:17: step 44640/200000, loss = 0.620279 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:18 INFO: 2022-08-03 15:51:18: step 44660/200000, loss = 0.110762 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:19 INFO: 2022-08-03 15:51:19: step 44680/200000, loss = 0.369334 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:20 INFO: 2022-08-03 15:51:20: step 44700/200000, loss = 0.213205 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:51:20 INFO: 2022-08-03 15:51:20: step 44720/200000, loss = 0.061520 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:51:21 INFO: 2022-08-03 15:51:21: step 44740/200000, loss = 0.105439 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:22 INFO: 2022-08-03 15:51:22: step 44760/200000, loss = 0.093433 (0.046 sec/batch), lr: 0.000195
2022-08-03 15:51:23 INFO: 2022-08-03 15:51:23: step 44780/200000, loss = 0.321600 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:24 INFO: 2022-08-03 15:51:24: step 44800/200000, loss = 0.056694 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:25 INFO: 2022-08-03 15:51:25: step 44820/200000, loss = 0.162412 (0.042 sec/batch), lr: 0.000195
2022-08-03 15:51:26 INFO: 2022-08-03 15:51:26: step 44840/200000, loss = 0.359204 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:27 INFO: 2022-08-03 15:51:27: step 44860/200000, loss = 0.136679 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:28 INFO: 2022-08-03 15:51:28: step 44880/200000, loss = 0.159405 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:28 INFO: 2022-08-03 15:51:28: step 44900/200000, loss = 0.404618 (0.034 sec/batch), lr: 0.000195
2022-08-03 15:51:29 INFO: 2022-08-03 15:51:29: step 44920/200000, loss = 0.220211 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:51:30 INFO: 2022-08-03 15:51:30: step 44940/200000, loss = 0.462715 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:51:31 INFO: 2022-08-03 15:51:31: step 44960/200000, loss = 0.066868 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:51:32 INFO: 2022-08-03 15:51:32: step 44980/200000, loss = 0.252383 (0.046 sec/batch), lr: 0.000195
2022-08-03 15:51:33 INFO: 2022-08-03 15:51:33: step 45000/200000, loss = 0.106912 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:33 INFO: Evaluating on dev set...
2022-08-03 15:51:57 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.44	94.06
2022-08-03 15:51:57 INFO: step 45000: train_loss = 0.215816, dev_score = 0.9406
2022-08-03 15:51:57 INFO: 
2022-08-03 15:51:58 INFO: 2022-08-03 15:51:58: step 45020/200000, loss = 0.058993 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:51:59 INFO: 2022-08-03 15:51:59: step 45040/200000, loss = 0.126545 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:00 INFO: 2022-08-03 15:52:00: step 45060/200000, loss = 0.132281 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:52:01 INFO: 2022-08-03 15:52:01: step 45080/200000, loss = 0.071361 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:02 INFO: 2022-08-03 15:52:02: step 45100/200000, loss = 0.321977 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:03 INFO: 2022-08-03 15:52:03: step 45120/200000, loss = 0.293293 (0.031 sec/batch), lr: 0.000195
2022-08-03 15:52:04 INFO: 2022-08-03 15:52:04: step 45140/200000, loss = 0.094608 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:05 INFO: 2022-08-03 15:52:05: step 45160/200000, loss = 0.616004 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:06 INFO: 2022-08-03 15:52:06: step 45180/200000, loss = 0.116716 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:06 INFO: 2022-08-03 15:52:06: step 45200/200000, loss = 0.439995 (0.044 sec/batch), lr: 0.000195
2022-08-03 15:52:07 INFO: 2022-08-03 15:52:07: step 45220/200000, loss = 0.271330 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:52:08 INFO: 2022-08-03 15:52:08: step 45240/200000, loss = 0.281473 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:09 INFO: 2022-08-03 15:52:09: step 45260/200000, loss = 0.292110 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:10 INFO: 2022-08-03 15:52:10: step 45280/200000, loss = 0.087042 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:11 INFO: 2022-08-03 15:52:11: step 45300/200000, loss = 0.201922 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:52:12 INFO: 2022-08-03 15:52:12: step 45320/200000, loss = 0.104560 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:13 INFO: 2022-08-03 15:52:13: step 45340/200000, loss = 0.044277 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:14 INFO: 2022-08-03 15:52:14: step 45360/200000, loss = 0.103762 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:15 INFO: 2022-08-03 15:52:15: step 45380/200000, loss = 0.122540 (0.034 sec/batch), lr: 0.000195
2022-08-03 15:52:15 INFO: 2022-08-03 15:52:15: step 45400/200000, loss = 0.397085 (0.034 sec/batch), lr: 0.000195
2022-08-03 15:52:16 INFO: 2022-08-03 15:52:16: step 45420/200000, loss = 0.225465 (0.033 sec/batch), lr: 0.000195
2022-08-03 15:52:17 INFO: 2022-08-03 15:52:17: step 45440/200000, loss = 0.093502 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:18 INFO: 2022-08-03 15:52:18: step 45460/200000, loss = 0.278470 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:52:19 INFO: 2022-08-03 15:52:19: step 45480/200000, loss = 0.138828 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:20 INFO: 2022-08-03 15:52:20: step 45500/200000, loss = 0.574050 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:20 INFO: Evaluating on dev set...
2022-08-03 15:52:44 INFO: Score by entity:
Prec.	Rec.	F1
94.69	93.44	94.06
2022-08-03 15:52:44 INFO: step 45500: train_loss = 0.224489, dev_score = 0.9406
2022-08-03 15:52:44 INFO: 
2022-08-03 15:52:45 INFO: 2022-08-03 15:52:45: step 45520/200000, loss = 0.102014 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:52:46 INFO: 2022-08-03 15:52:46: step 45540/200000, loss = 0.118878 (0.040 sec/batch), lr: 0.000195
2022-08-03 15:52:47 INFO: 2022-08-03 15:52:47: step 45560/200000, loss = 0.084499 (0.043 sec/batch), lr: 0.000195
2022-08-03 15:52:48 INFO: 2022-08-03 15:52:48: step 45580/200000, loss = 0.314756 (0.048 sec/batch), lr: 0.000195
2022-08-03 15:52:49 INFO: 2022-08-03 15:52:49: step 45600/200000, loss = 0.332413 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:50 INFO: 2022-08-03 15:52:50: step 45620/200000, loss = 0.115959 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:51 INFO: 2022-08-03 15:52:51: step 45640/200000, loss = 0.181836 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:52:52 INFO: 2022-08-03 15:52:52: step 45660/200000, loss = 0.162783 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:52 INFO: 2022-08-03 15:52:52: step 45680/200000, loss = 0.138741 (0.043 sec/batch), lr: 0.000195
2022-08-03 15:52:53 INFO: 2022-08-03 15:52:53: step 45700/200000, loss = 0.354185 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:52:54 INFO: 2022-08-03 15:52:54: step 45720/200000, loss = 0.067564 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:52:55 INFO: 2022-08-03 15:52:55: step 45740/200000, loss = 0.498894 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:52:56 INFO: 2022-08-03 15:52:56: step 45760/200000, loss = 0.184264 (0.047 sec/batch), lr: 0.000195
2022-08-03 15:52:57 INFO: 2022-08-03 15:52:57: step 45780/200000, loss = 0.433156 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:52:58 INFO: 2022-08-03 15:52:58: step 45800/200000, loss = 0.075260 (0.044 sec/batch), lr: 0.000195
2022-08-03 15:52:59 INFO: 2022-08-03 15:52:59: step 45820/200000, loss = 0.072320 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:53:00 INFO: 2022-08-03 15:53:00: step 45840/200000, loss = 0.224080 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:53:01 INFO: 2022-08-03 15:53:01: step 45860/200000, loss = 0.163202 (0.034 sec/batch), lr: 0.000195
2022-08-03 15:53:01 INFO: 2022-08-03 15:53:01: step 45880/200000, loss = 0.222429 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:53:02 INFO: 2022-08-03 15:53:02: step 45900/200000, loss = 0.056054 (0.033 sec/batch), lr: 0.000195
2022-08-03 15:53:03 INFO: 2022-08-03 15:53:03: step 45920/200000, loss = 0.117321 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:53:04 INFO: 2022-08-03 15:53:04: step 45940/200000, loss = 0.301214 (0.034 sec/batch), lr: 0.000195
2022-08-03 15:53:05 INFO: 2022-08-03 15:53:05: step 45960/200000, loss = 0.493740 (0.033 sec/batch), lr: 0.000195
2022-08-03 15:53:06 INFO: 2022-08-03 15:53:06: step 45980/200000, loss = 0.149690 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:53:07 INFO: 2022-08-03 15:53:07: step 46000/200000, loss = 0.132861 (0.047 sec/batch), lr: 0.000195
2022-08-03 15:53:07 INFO: Evaluating on dev set...
2022-08-03 15:53:31 INFO: Score by entity:
Prec.	Rec.	F1
94.67	93.44	94.05
2022-08-03 15:53:31 INFO: step 46000: train_loss = 0.228307, dev_score = 0.9405
2022-08-03 15:53:31 INFO: 
2022-08-03 15:53:32 INFO: 2022-08-03 15:53:32: step 46020/200000, loss = 0.486276 (0.051 sec/batch), lr: 0.000195
2022-08-03 15:53:33 INFO: 2022-08-03 15:53:33: step 46040/200000, loss = 0.536040 (0.044 sec/batch), lr: 0.000195
2022-08-03 15:53:34 INFO: 2022-08-03 15:53:34: step 46060/200000, loss = 0.326135 (0.041 sec/batch), lr: 0.000195
2022-08-03 15:53:35 INFO: 2022-08-03 15:53:35: step 46080/200000, loss = 0.390577 (0.040 sec/batch), lr: 0.000195
2022-08-03 15:53:36 INFO: 2022-08-03 15:53:36: step 46100/200000, loss = 0.235045 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:53:36 INFO: 2022-08-03 15:53:36: step 46120/200000, loss = 0.187003 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:53:37 INFO: 2022-08-03 15:53:37: step 46140/200000, loss = 0.273542 (0.051 sec/batch), lr: 0.000195
2022-08-03 15:53:38 INFO: 2022-08-03 15:53:38: step 46160/200000, loss = 0.304861 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:53:39 INFO: 2022-08-03 15:53:39: step 46180/200000, loss = 0.099249 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:53:40 INFO: 2022-08-03 15:53:40: step 46200/200000, loss = 0.094956 (0.040 sec/batch), lr: 0.000195
2022-08-03 15:53:41 INFO: 2022-08-03 15:53:41: step 46220/200000, loss = 0.082713 (0.035 sec/batch), lr: 0.000195
2022-08-03 15:53:42 INFO: 2022-08-03 15:53:42: step 46240/200000, loss = 0.051516 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:53:43 INFO: 2022-08-03 15:53:43: step 46260/200000, loss = 0.275003 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:53:43 INFO: 2022-08-03 15:53:43: step 46280/200000, loss = 0.245383 (0.044 sec/batch), lr: 0.000195
2022-08-03 15:53:44 INFO: 2022-08-03 15:53:44: step 46300/200000, loss = 0.360361 (0.031 sec/batch), lr: 0.000195
2022-08-03 15:53:45 INFO: 2022-08-03 15:53:45: step 46320/200000, loss = 0.071698 (0.045 sec/batch), lr: 0.000195
2022-08-03 15:53:46 INFO: 2022-08-03 15:53:46: step 46340/200000, loss = 0.077536 (0.054 sec/batch), lr: 0.000195
2022-08-03 15:53:47 INFO: 2022-08-03 15:53:47: step 46360/200000, loss = 0.252086 (0.042 sec/batch), lr: 0.000195
2022-08-03 15:53:48 INFO: 2022-08-03 15:53:48: step 46380/200000, loss = 0.549497 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:53:49 INFO: 2022-08-03 15:53:49: step 46400/200000, loss = 0.222347 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:53:50 INFO: 2022-08-03 15:53:50: step 46420/200000, loss = 0.607824 (0.039 sec/batch), lr: 0.000195
2022-08-03 15:53:51 INFO: 2022-08-03 15:53:51: step 46440/200000, loss = 0.136421 (0.038 sec/batch), lr: 0.000195
2022-08-03 15:53:52 INFO: 2022-08-03 15:53:52: step 46460/200000, loss = 0.335660 (0.043 sec/batch), lr: 0.000195
2022-08-03 15:53:52 INFO: 2022-08-03 15:53:52: step 46480/200000, loss = 0.046319 (0.036 sec/batch), lr: 0.000195
2022-08-03 15:53:53 INFO: 2022-08-03 15:53:53: step 46500/200000, loss = 0.167053 (0.037 sec/batch), lr: 0.000195
2022-08-03 15:53:53 INFO: Evaluating on dev set...
2022-08-03 15:54:18 INFO: Score by entity:
Prec.	Rec.	F1
94.66	93.44	94.05
2022-08-03 15:54:18 INFO: step 46500: train_loss = 0.231332, dev_score = 0.9405
2022-08-03 15:54:18 INFO: 
Epoch 00093: reducing learning rate of group 0 to 1.0000e-04.
2022-08-03 15:54:18 INFO: Training ended with 46500 steps.
2022-08-03 15:54:18 INFO: Best dev F1 = 94.11, at iteration = 37000
2022-08-03 15:54:18 INFO: Running dev step with args: ['--eval_file', '/nlp/scr/aaydin/kazNERD/data/ner/kk_kazNERD.dev.json', '--lang', 'kk', '--shorthand', 'kk_kazNERD', '--mode', 'predict', '--wordvec_pretrain_file', '/nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt', '--save_name', 'kk_kazNERD_nertagger.pt', '--save_dir', 'saved_models/ner']
2022-08-03 15:54:18 INFO: Running NER tagger in predict mode
2022-08-03 15:54:19 DEBUG: Loaded pretrain from /nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt
2022-08-03 15:54:20 DEBUG: Loaded model for eval from saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:54:20 INFO: Loading data with batch size 32...
2022-08-03 15:54:23 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
2022-08-03 15:54:24 DEBUG: 349 batches created.
2022-08-03 15:54:24 INFO: Start evaluation...
2022-08-03 15:54:49 INFO: Score by entity:
Prec.	Rec.	F1
94.72	93.50	94.11
2022-08-03 15:54:49 INFO: Score by token:
Prec.	Rec.	F1
95.07	93.68	94.37
2022-08-03 15:54:49 INFO: NER tagger score:
2022-08-03 15:54:49 INFO: kk_kazNERD 94.11
2022-08-03 15:54:49 INFO: NER token confusion matrix:
          t\p                     O           ADAGE             ART        CARDINAL         CONTACT            DATE         DISEASE           EVENT        FACILITY             GPE        LANGUAGE             LAW        LOCATION   MISCELLANEOUS           MONEY            NORP         ORDINAL    ORGANISATION      PERCENTAGE          PERSON        POSITION         PRODUCT         PROJECT        QUANTITY            TIME       NON_HUMAN 
                  O          102608              25              11              45               0              49               3              18              35               5               0               7               1               0               1               4               1              64               1               7              29               1              24               2               5               0 
              ADAGE              45              39               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0 
                ART              20               3             433               0               0               0               0               2               1               2               0               0               0               0               0               1               0              10               0               2               4               0               9               0               0               0 
           CARDINAL              49               0               0            4521               0              16               0               0               0               0               0               2               0               0               6               0               0               0               2               0               0               0               0               2               0               0 
            CONTACT               2               0               0               4              27               0               0               0               4               0               0               0               0               0               0               0               3               0               0               0               0               0               0               0               0               0 
               DATE              70               0               0              32               0            5606               0              10               0               2               0               0               0               0               0               1               2               0               0               0               0               1               0               0               2               0 
            DISEASE              18               0               0               0               0               0             143               0               0               0               0               0               0               2               0               1               0               0               0               0               0               1               0               0               0               0 
              EVENT              52               0               9               0               0              12               1             352               1               4               0               0               2               0               0               3               0              13               0               5               0               2              23               0               0               0 
           FACILITY              48               0               0               7               0               0               1               0             407              16               0               0               8               0               0               0               2              35               0               7               0               0               8               0               0               0 
                GPE              25               0               0               0               0               0               0               1              10            1931               0               2               7               0               0               2               0              11               0              21               3               0               0               0               0               0 
           LANGUAGE               4               0               0               0               0               0               0               0               0               0              55               0               0               0               0               3               0               0               0               0               0               0               0               0               0               0 
                LAW              32               0               0               0               0               0               0               0               0               1               0             180               0               0               0               0               1               0               0               1               0               0               0               0               0               0 
           LOCATION              15               0              15               1               0               0               0               0              14               9               0               0             224               0               0               0               0               0               0               4               0               0               0               0               0               0 
      MISCELLANEOUS               6               0               7               1               0               0               0               0               0               0               0               0               0              10               0               0               0               0               0               0               0               1               4               0               0               0 
              MONEY               4               0               0               0               0               0               0               0               0               0               0               0               0               0            1603               0               0               0               0               0               0               0               0               0               0               0 
               NORP              48               0               0               0               0               0               0               2               3               3               1               0               0               0               0             317               0               1               0               1               2               0               0               0               0               0 
            ORDINAL               0               0               0               3               0               1               0               0               4               0               0               0               0               0               0               0             389               5               0               0               0               0               0               0               0               0 
       ORGANISATION             101               0               9               8               0               0               0               6              49              16               0               0               0               0               0               1               2            1343               0               4              26               7               7               0               0               0 
         PERCENTAGE              10               0               0               2               0               0               0               0               0               0               0               0               0               0               0               0               0               0            1242               0               0               0               0               0               0               0 
             PERSON              25               0               5               0               0               0               0               1               7              10               0               0               0               0               0               0               0               0               0            2412               0               0               0               0               0               0 
           POSITION              80               0               2               0               0               0               0               0               0               4               0               8               0               0               0               0               3              27               0               3            1193               0               0               0               0               0 
            PRODUCT              11               0               7               0               0               0               0               0               3               0               0               0               0               3               0               0               0              15               0               0               0              92               0               0               0               0 
            PROJECT              33               0               4               1               0               0               0               0               0               3               0               0               0               0               0               1               0              16               0               0               0               1             491               0               0               0 
           QUANTITY              16               0               0              10               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0            1370               0               0 
               TIME               8               0               0               4               0               6               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0             462               0 
          NON_HUMAN               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               1               0               0               0               0               0               0 
2022-08-03 15:54:49 INFO: Running test step with args: ['--eval_file', '/nlp/scr/aaydin/kazNERD/data/ner/kk_kazNERD.test.json', '--lang', 'kk', '--shorthand', 'kk_kazNERD', '--mode', 'predict', '--wordvec_pretrain_file', '/nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt', '--save_name', 'kk_kazNERD_nertagger.pt', '--save_dir', 'saved_models/ner']
2022-08-03 15:54:49 INFO: Running NER tagger in predict mode
2022-08-03 15:54:49 DEBUG: Loaded pretrain from /nlp/scr/aaydin/stanza_resources/kk/pretrain/kk.pt
2022-08-03 15:54:50 DEBUG: Loaded model for eval from saved_models/ner/kk_kazNERD_nertagger.pt
2022-08-03 15:54:50 INFO: Loading data with batch size 32...
2022-08-03 15:54:52 DEBUG: BIO tagging scheme found in input; converting into BIOES scheme...
2022-08-03 15:54:53 DEBUG: 354 batches created.
2022-08-03 15:54:53 INFO: Start evaluation...
2022-08-03 15:55:18 INFO: Score by entity:
Prec.	Rec.	F1
93.97	93.12	93.54
2022-08-03 15:55:18 INFO: Score by token:
Prec.	Rec.	F1
94.78	93.04	93.90
2022-08-03 15:55:18 INFO: NER tagger score:
2022-08-03 15:55:18 INFO: kk_kazNERD 93.54
2022-08-03 15:55:18 INFO: NER token confusion matrix:
          t\p                     O           ADAGE             ART        CARDINAL         CONTACT            DATE         DISEASE           EVENT        FACILITY             GPE        LANGUAGE             LAW        LOCATION   MISCELLANEOUS           MONEY            NORP         ORDINAL    ORGANISATION      PERCENTAGE          PERSON        POSITION         PRODUCT         PROJECT        QUANTITY            TIME       NON_HUMAN 
                  O          102881               4               4              73               0              77               5              31              46               4               0              18               6               0               4               3               0              46               6               3              20               2              16              11               9               0 
              ADAGE              70              22               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0 
                ART              12               0             426               0               0               0               0              12               2               0               0               0               2               0               0               1               0               5               0               4               0               0               1               0               0               0 
           CARDINAL              39               0               0            4532               0              10               0               0               0               0               0               0               0               0               2               0               2               0               3               1               0               0               0               8              12               0 
            CONTACT               1               0               0               0              35               0               0               0               0               0               0               0               0               2               0               0               0               2               0               0               0               1               0               0               0               0 
               DATE             105               0               0              26               0            5834               0               2               0               0               0               0               0               0               0               0               6               0               0               2               0               0               0               0               3               0 
            DISEASE              19               0               0               0               0               0             162               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0 
              EVENT              29               0              15               0               0               4               0             428               7               6               1               0               1               0               0               0               1               7               0               0               0               0               0               0               0               0 
           FACILITY              72               0               4               8               0               0               0               0             345              23               0               0               3               0               0               2               4              37               0              10               1               0               4               0               0               0 
                GPE              30               0               1               0               0               0               0               2              11            2000               0               0               2               0               0               0               0              14               0              24               8               0               2               0               0               0 
           LANGUAGE               3               0               0               0               0               0               0               0               0               0              65               0               0               0               0               4               0               0               0               0               0               0               0               0               0               0 
                LAW              81               0               0               0               0               0               0               0               0               0               0             186               0               0               0               0               9              10               0               0               0               0               4               0               0               0 
           LOCATION              28               0               0               0               0               0               0               0               7              11               0               0             216               0               0               0               0               0               0               4               0               0               0               0               0               0 
      MISCELLANEOUS               4               0               0               2               0               0               0               0               0               0               0               0               0              28               0               1               0               2               0               0               0               0               2               0               0               0 
              MONEY              15               0               0              11               0               0               0               0               0               0               0               0               0               0            1569               0               0               0               0               0               0               0               0               8               0               0 
               NORP              36               0               0               0               0               0               0               0               0               2               0               0               0               0               0             335               0               0               0               0               0               0               0               0               0               0 
            ORDINAL               1               0               0               0               0               0               0               3              21               0               0               7               0               0               0               0             382               0               0               0               0               0               0               0               0               0 
       ORGANISATION             163               0               7               0               0               0               1              12              27              23               0               1               1               0               0               1               6            1353               0               6              11              12              15               0               0               0 
         PERCENTAGE               6               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0            1189               0               0               0               0               0               0               0 
             PERSON              29               0               1               0               0               0               0               1               1              10               0               0               3               0               0               0               0               1               0            2402               0               0               0               0               0               0 
           POSITION              61               0               0               0               0               0               0               8               1               5               0               0               0               0               0               0               0               9               0               1            1176               0               0               0               0               0 
            PRODUCT               1               0               1               0               0               0               0               9               7               2               0               0               0               2               0               0               0               5               0               0               0              80              12               0               0               0 
            PROJECT              38               0              15               0               1               2               0               0               6               1               0               0               4               0               0               0               0               9               0               2               0               2             462               0               0               0 
           QUANTITY              20               0               0              34               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0            1210               0               0 
               TIME              11               0               0               3               0               4               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0             539               0 
          NON_HUMAN               0               0               1               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0               0 
###############################
end time: 2022-08-03 15:55:23.047657
elapsed time: 1:15:00.024877
